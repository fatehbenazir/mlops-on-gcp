{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import os \n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from google.cloud import bigquery\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://benazirsproject-demo'\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'dataset.csv')\n",
    "VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-38-e4ad1f808a40>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-38-e4ad1f808a40>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    DATA_SOURCE=gs://workshop-datasets/covertype/small/dataset.csv\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## load data into bigquery ... \n",
    "\n",
    "%%bigquery\n",
    "DATASET_LOCATION=US\n",
    "DATASET_ID=covertype_dataset2\n",
    "TABLE_ID=covertype\n",
    "DATA_SOURCE=gs://workshop-datasets/covertype/small/dataset.csv\n",
    "SCHEMA=Elevation:INTEGER,\\\n",
    "Aspect:INTEGER,\\\n",
    "Slope:INTEGER,\\\n",
    "Horizontal_Distance_To_Hydrology:INTEGER,\\\n",
    "Vertical_Distance_To_Hydrology:INTEGER,\\\n",
    "Horizontal_Distance_To_Roadways:INTEGER,\\\n",
    "Hillshade_9am:INTEGER,\\\n",
    "Hillshade_Noon:INTEGER,\\\n",
    "Hillshade_3pm:INTEGER,\\\n",
    "Horizontal_Distance_To_Fire_Points:INTEGER,\\\n",
    "Wilderness_Area:STRING,\\\n",
    "Soil_Type:STRING,\\\n",
    "Cover_Type:INTEGER\n",
    "\n",
    "bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "\n",
    "bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATA_SOURCE \\\n",
    "$SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>Wilderness_Area</th>\n",
       "      <th>Soil_Type</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2085</td>\n",
       "      <td>256</td>\n",
       "      <td>18</td>\n",
       "      <td>150</td>\n",
       "      <td>27</td>\n",
       "      <td>738</td>\n",
       "      <td>176</td>\n",
       "      <td>248</td>\n",
       "      <td>208</td>\n",
       "      <td>914</td>\n",
       "      <td>Cache</td>\n",
       "      <td>C2702</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2125</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>871</td>\n",
       "      <td>169</td>\n",
       "      <td>248</td>\n",
       "      <td>215</td>\n",
       "      <td>300</td>\n",
       "      <td>Cache</td>\n",
       "      <td>C2702</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2146</td>\n",
       "      <td>256</td>\n",
       "      <td>34</td>\n",
       "      <td>150</td>\n",
       "      <td>62</td>\n",
       "      <td>1253</td>\n",
       "      <td>122</td>\n",
       "      <td>237</td>\n",
       "      <td>239</td>\n",
       "      <td>511</td>\n",
       "      <td>Cache</td>\n",
       "      <td>C2702</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2186</td>\n",
       "      <td>256</td>\n",
       "      <td>38</td>\n",
       "      <td>210</td>\n",
       "      <td>102</td>\n",
       "      <td>1294</td>\n",
       "      <td>109</td>\n",
       "      <td>232</td>\n",
       "      <td>244</td>\n",
       "      <td>552</td>\n",
       "      <td>Cache</td>\n",
       "      <td>C2702</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2831</td>\n",
       "      <td>256</td>\n",
       "      <td>25</td>\n",
       "      <td>277</td>\n",
       "      <td>183</td>\n",
       "      <td>1706</td>\n",
       "      <td>153</td>\n",
       "      <td>246</td>\n",
       "      <td>225</td>\n",
       "      <td>1485</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C2705</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>3136</td>\n",
       "      <td>254</td>\n",
       "      <td>12</td>\n",
       "      <td>319</td>\n",
       "      <td>60</td>\n",
       "      <td>5734</td>\n",
       "      <td>193</td>\n",
       "      <td>248</td>\n",
       "      <td>193</td>\n",
       "      <td>2467</td>\n",
       "      <td>Rawah</td>\n",
       "      <td>C7746</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>3242</td>\n",
       "      <td>254</td>\n",
       "      <td>12</td>\n",
       "      <td>636</td>\n",
       "      <td>148</td>\n",
       "      <td>3551</td>\n",
       "      <td>193</td>\n",
       "      <td>248</td>\n",
       "      <td>193</td>\n",
       "      <td>2010</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C7757</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>2071</td>\n",
       "      <td>255</td>\n",
       "      <td>12</td>\n",
       "      <td>234</td>\n",
       "      <td>63</td>\n",
       "      <td>342</td>\n",
       "      <td>192</td>\n",
       "      <td>247</td>\n",
       "      <td>193</td>\n",
       "      <td>247</td>\n",
       "      <td>Cache</td>\n",
       "      <td>C2706</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>3248</td>\n",
       "      <td>255</td>\n",
       "      <td>12</td>\n",
       "      <td>730</td>\n",
       "      <td>113</td>\n",
       "      <td>725</td>\n",
       "      <td>192</td>\n",
       "      <td>247</td>\n",
       "      <td>193</td>\n",
       "      <td>2724</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C7756</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>3153</td>\n",
       "      <td>255</td>\n",
       "      <td>12</td>\n",
       "      <td>404</td>\n",
       "      <td>116</td>\n",
       "      <td>2139</td>\n",
       "      <td>192</td>\n",
       "      <td>247</td>\n",
       "      <td>193</td>\n",
       "      <td>994</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C7756</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0           2085     256     18                               150   \n",
       "1           2125     256     20                                30   \n",
       "2           2146     256     34                               150   \n",
       "3           2186     256     38                               210   \n",
       "4           2831     256     25                               277   \n",
       "...          ...     ...    ...                               ...   \n",
       "99995       3136     254     12                               319   \n",
       "99996       3242     254     12                               636   \n",
       "99997       2071     255     12                               234   \n",
       "99998       3248     255     12                               730   \n",
       "99999       3153     255     12                               404   \n",
       "\n",
       "       Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                                  27                              738   \n",
       "1                                  12                              871   \n",
       "2                                  62                             1253   \n",
       "3                                 102                             1294   \n",
       "4                                 183                             1706   \n",
       "...                               ...                              ...   \n",
       "99995                              60                             5734   \n",
       "99996                             148                             3551   \n",
       "99997                              63                              342   \n",
       "99998                             113                              725   \n",
       "99999                             116                             2139   \n",
       "\n",
       "       Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0                176             248            208   \n",
       "1                169             248            215   \n",
       "2                122             237            239   \n",
       "3                109             232            244   \n",
       "4                153             246            225   \n",
       "...              ...             ...            ...   \n",
       "99995            193             248            193   \n",
       "99996            193             248            193   \n",
       "99997            192             247            193   \n",
       "99998            192             247            193   \n",
       "99999            192             247            193   \n",
       "\n",
       "       Horizontal_Distance_To_Fire_Points Wilderness_Area Soil_Type  \\\n",
       "0                                     914           Cache     C2702   \n",
       "1                                     300           Cache     C2702   \n",
       "2                                     511           Cache     C2702   \n",
       "3                                     552           Cache     C2702   \n",
       "4                                    1485       Commanche     C2705   \n",
       "...                                   ...             ...       ...   \n",
       "99995                                2467           Rawah     C7746   \n",
       "99996                                2010       Commanche     C7757   \n",
       "99997                                 247           Cache     C2706   \n",
       "99998                                2724       Commanche     C7756   \n",
       "99999                                 994       Commanche     C7756   \n",
       "\n",
       "       Cover_Type  \n",
       "0               5  \n",
       "1               2  \n",
       "2               2  \n",
       "3               2  \n",
       "4               1  \n",
       "...           ...  \n",
       "99995           1  \n",
       "99996           0  \n",
       "99997           2  \n",
       "99998           1  \n",
       "99999           1  \n",
       "\n",
       "[100000 rows x 13 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "SELECT *\n",
    "FROM `covertype_dataset.covertype`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r2075c7d5d71d0385_000001742be3a92d_1 ... (1s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "## create training split\n",
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table covertype_dataset.training \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `covertype_dataset.covertype` AS cover \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), 10) IN (1, 2, 3, 4)' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r7b5644a868249e85_000001742be6efe4_1 ... (0s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "## save trainign split at this location\n",
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "covertype_dataset.training \\\n",
    "$TRAINING_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r7cb6be8a932f43f6_000001742bf78b80_1 ... (1s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table covertype_dataset.validation \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `covertype_dataset.covertype` AS cover \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), 10) IN (8)' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_re87973667e9b28c_000001742bf79a1b_1 ... (0s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "covertype_dataset.validation \\\n",
    "$VALIDATION_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40009, 13)\n",
      "(9836, 13)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(TRAINING_FILE_PATH)\n",
    "df_validation = pd.read_csv(VALIDATION_FILE_PATH)\n",
    "print(df_train.shape)\n",
    "print(df_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feature_indexes = slice(0, 10)\n",
    "categorical_feature_indexes = slice(10, 12)\n",
    "num_features_type_map = {feature: 'float64' for feature in df_train.columns[numeric_feature_indexes]}\n",
    "\n",
    "df_train = df_train.astype(num_features_type_map)\n",
    "df_validation = df_validation.astype(num_features_type_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_feature_indexes),\n",
    "        ('cat', OneHotEncoder(), categorical_feature_indexes) \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log', tol=1e-3))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num', StandardScaler(),\n",
       "                                                  slice(0, 10, None)),\n",
       "                                                 ('cat', OneHotEncoder(),\n",
       "                                                  slice(10, 12, None))])),\n",
       "                ('classifier',\n",
       "                 SGDClassifier(alpha=0.001, loss='log', max_iter=200))])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df_train.drop('Cover_Type', axis=1)\n",
    "y_train = df_train['Cover_Type']\n",
    "X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "y_validation = df_validation['Cover_Type']\n",
    "\n",
    "pipeline.set_params(classifier__alpha=0.001, classifier__max_iter=200)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6969296461976413\n"
     ]
    }
   ],
   "source": [
    "accuracy = pipeline.score(X_validation, y_validation)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the mpdel and hyperparameter job on ai platform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_app/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, alpha, max_iter, hptune):\n",
    "    \n",
    "    df_train = pd.read_csv(training_dataset_path)\n",
    "    df_validation = pd.read_csv(validation_dataset_path)\n",
    "\n",
    "    if not hptune:\n",
    "        df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "    numeric_feature_indexes = slice(0, 10)\n",
    "    categorical_feature_indexes = slice(10, 12)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_feature_indexes),\n",
    "        ('cat', OneHotEncoder(), categorical_feature_indexes) \n",
    "    ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', SGDClassifier(loss='log',tol=1e-3))\n",
    "    ])\n",
    "\n",
    "    num_features_type_map = {feature: 'float64' for feature in df_train.columns[numeric_feature_indexes]}\n",
    "    df_train = df_train.astype(num_features_type_map)\n",
    "    df_validation = df_validation.astype(num_features_type_map) \n",
    "\n",
    "    print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "    X_train = df_train.drop('Cover_Type', axis=1)\n",
    "    y_train = df_train['Cover_Type']\n",
    "\n",
    "    pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    if hptune:\n",
    "        X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "        y_validation = df_validation['Cover_Type']\n",
    "        accuracy = pipeline.score(X_validation, y_validation)\n",
    "        print('Model accuracy: {}'.format(accuracy))\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "          hyperparameter_metric_tag='accuracy',\n",
    "          metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        model_filename = 'model.pkl'\n",
    "        with open(model_filename, 'wb') as model_file:\n",
    "            pickle.dump(pipeline, model_file)\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "        subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='trainer_image'\n",
    "IMAGE_TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 3 file(s) totalling 6.2 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://benazirsproject_cloudbuild/source/1598467503.89-7e3f979ba7d9400f91639c7954a031c3.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/benazirsproject/builds/1a765ac6-e896-42e7-a23d-d128b07d5521].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/1a765ac6-e896-42e7-a23d-d128b07d5521?project=981930454113].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"1a765ac6-e896-42e7-a23d-d128b07d5521\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://benazirsproject_cloudbuild/source/1598467503.89-7e3f979ba7d9400f91639c7954a031c3.tgz#1598467504428362\n",
      "Copying gs://benazirsproject_cloudbuild/source/1598467503.89-7e3f979ba7d9400f91639c7954a031c3.tgz#1598467504428362...\n",
      "/ [1 files][  1.6 KiB/  1.6 KiB]                                                \n",
      "Operation completed over 1 objects/1.6 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  9.728kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7c3167c320d: Pulling fs layer\n",
      "131f805ec7fd: Pulling fs layer\n",
      "322ed380e680: Pulling fs layer\n",
      "6ac240b13098: Pulling fs layer\n",
      "9ce3a9266402: Pulling fs layer\n",
      "72c706dfac1d: Pulling fs layer\n",
      "6383427606e5: Pulling fs layer\n",
      "3e8b21666cec: Pulling fs layer\n",
      "358bb5d659ed: Pulling fs layer\n",
      "8ade7556a8f1: Pulling fs layer\n",
      "b2ebb7e1223e: Pulling fs layer\n",
      "8d5d283ad922: Pulling fs layer\n",
      "14c0fd48a5f3: Pulling fs layer\n",
      "ceaad5dc04d2: Pulling fs layer\n",
      "c1074350f761: Pulling fs layer\n",
      "687ad0b9a318: Pulling fs layer\n",
      "d2365d2ee19a: Pulling fs layer\n",
      "5095b04f1d98: Pulling fs layer\n",
      "6ac240b13098: Waiting\n",
      "9ce3a9266402: Waiting\n",
      "72c706dfac1d: Waiting\n",
      "6383427606e5: Waiting\n",
      "3e8b21666cec: Waiting\n",
      "358bb5d659ed: Waiting\n",
      "8ade7556a8f1: Waiting\n",
      "b2ebb7e1223e: Waiting\n",
      "8d5d283ad922: Waiting\n",
      "14c0fd48a5f3: Waiting\n",
      "ceaad5dc04d2: Waiting\n",
      "c1074350f761: Waiting\n",
      "687ad0b9a318: Waiting\n",
      "d2365d2ee19a: Waiting\n",
      "5095b04f1d98: Waiting\n",
      "322ed380e680: Verifying Checksum\n",
      "322ed380e680: Download complete\n",
      "131f805ec7fd: Verifying Checksum\n",
      "131f805ec7fd: Download complete\n",
      "6ac240b13098: Verifying Checksum\n",
      "6ac240b13098: Download complete\n",
      "d7c3167c320d: Verifying Checksum\n",
      "d7c3167c320d: Download complete\n",
      "6383427606e5: Verifying Checksum\n",
      "6383427606e5: Download complete\n",
      "72c706dfac1d: Verifying Checksum\n",
      "72c706dfac1d: Download complete\n",
      "358bb5d659ed: Verifying Checksum\n",
      "358bb5d659ed: Download complete\n",
      "8ade7556a8f1: Verifying Checksum\n",
      "8ade7556a8f1: Download complete\n",
      "b2ebb7e1223e: Verifying Checksum\n",
      "b2ebb7e1223e: Download complete\n",
      "8d5d283ad922: Verifying Checksum\n",
      "8d5d283ad922: Download complete\n",
      "3e8b21666cec: Verifying Checksum\n",
      "3e8b21666cec: Download complete\n",
      "14c0fd48a5f3: Verifying Checksum\n",
      "14c0fd48a5f3: Download complete\n",
      "ceaad5dc04d2: Verifying Checksum\n",
      "ceaad5dc04d2: Download complete\n",
      "c1074350f761: Verifying Checksum\n",
      "c1074350f761: Download complete\n",
      "687ad0b9a318: Verifying Checksum\n",
      "687ad0b9a318: Download complete\n",
      "9ce3a9266402: Verifying Checksum\n",
      "9ce3a9266402: Download complete\n",
      "5095b04f1d98: Verifying Checksum\n",
      "5095b04f1d98: Download complete\n",
      "d7c3167c320d: Pull complete\n",
      "d2365d2ee19a: Verifying Checksum\n",
      "d2365d2ee19a: Download complete\n",
      "131f805ec7fd: Pull complete\n",
      "322ed380e680: Pull complete\n",
      "6ac240b13098: Pull complete\n",
      "9ce3a9266402: Pull complete\n",
      "72c706dfac1d: Pull complete\n",
      "6383427606e5: Pull complete\n",
      "3e8b21666cec: Pull complete\n",
      "358bb5d659ed: Pull complete\n",
      "8ade7556a8f1: Pull complete\n",
      "b2ebb7e1223e: Pull complete\n",
      "8d5d283ad922: Pull complete\n",
      "14c0fd48a5f3: Pull complete\n",
      "ceaad5dc04d2: Pull complete\n",
      "c1074350f761: Pull complete\n",
      "687ad0b9a318: Pull complete\n",
      "d2365d2ee19a: Pull complete\n",
      "5095b04f1d98: Pull complete\n",
      "Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d8706668f140\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in f58b0e38bc12\n",
      "Collecting fire\n",
      "  Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=a6df0b09e83b208a00dea32ba47738dd145890ee099d17d9792c464ba154aae6\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3986 sha256=afb1f640f80a9eaa9eb7c6b9b6e4534b9e56a8fc9983975e60548e9db0b9757e\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=6e509c73e04359e5cb76cc5f9735c0dec84cb02327e3d9d11ccb1f1345bbf5a6\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "\u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: termcolor, fire, cloudml-hypertune, scikit-learn, pandas\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.1\n",
      "    Uninstalling scikit-learn-0.23.1:\n",
      "      Successfully uninstalled scikit-learn-0.23.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.3.1 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "Removing intermediate container f58b0e38bc12\n",
      " ---> dfad85a0081d\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 72b77298c22b\n",
      "Removing intermediate container 72b77298c22b\n",
      " ---> d514e2ca8193\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 24fc9d53cb81\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 4ad753d05f86\n",
      "Removing intermediate container 4ad753d05f86\n",
      " ---> 407d7905aed6\n",
      "Successfully built 407d7905aed6\n",
      "Successfully tagged gcr.io/benazirsproject/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/benazirsproject/trainer_image:latest\n",
      "The push refers to repository [gcr.io/benazirsproject/trainer_image]\n",
      "72478d03b757: Preparing\n",
      "33f70507b0f0: Preparing\n",
      "3dc26bc40592: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "222959643149: Waiting\n",
      "badaf1bc8335: Waiting\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "c51fe61c6231: Layer already exists\n",
      "89212ed9ad75: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "33f70507b0f0: Pushed\n",
      "72478d03b757: Pushed\n",
      "4c4a5579b7a8: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "3dc26bc40592: Pushed\n",
      "latest: digest: sha256:0c1affe9742a2f2a7790109c709d4aa3f6ae50c2b2f26dd1c6737f3acf129fb3 size: 4708\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                     IMAGES                                          STATUS\n",
      "1a765ac6-e896-42e7-a23d-d128b07d5521  2020-08-26T18:45:04+00:00  3M38S     gs://benazirsproject_cloudbuild/source/1598467503.89-7e3f979ba7d9400f91639c7954a031c3.tgz  gcr.io/benazirsproject/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_app/hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/hptuning_config.yaml\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 4\n",
    "    maxParallelTrials: 4\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    enableTrialEarlyStopping: TRUE \n",
    "    params:\n",
    "    - parameterName: max_iter\n",
    "      type: DISCRETE\n",
    "      discreteValues: [\n",
    "          200,\n",
    "          500\n",
    "          ]\n",
    "    - parameterName: alpha\n",
    "      type: DOUBLE\n",
    "      minValue:  0.00001\n",
    "      maxValue:  0.001\n",
    "      scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20200826_185451] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20200826_185451\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20200826_185451\n",
      "jobId: JOB_20200826_185451\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$IMAGE_URI \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "--config $TRAINING_APP_FOLDER/hptuning_config.yaml \\\n",
    "-- \\\n",
    "--training_dataset_path=$TRAINING_FILE_PATH \\\n",
    "--validation_dataset_path=$VALIDATION_FILE_PATH \\\n",
    "--hptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2020-08-26T18:54:53Z'\n",
      "etag: TDFdtRAnFOg=\n",
      "jobId: JOB_20200826_185451\n",
      "startTime: '2020-08-26T18:54:55Z'\n",
      "state: RUNNING\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --training_dataset_path=gs://benazirsproject-demo/data/training/dataset.csv\n",
      "  - --validation_dataset_path=gs://benazirsproject-demo/data/validation/dataset.csv\n",
      "  - --hptune\n",
      "  hyperparameters:\n",
      "    enableTrialEarlyStopping: true\n",
      "    goal: MAXIMIZE\n",
      "    hyperparameterMetricTag: accuracy\n",
      "    maxParallelTrials: 4\n",
      "    maxTrials: 4\n",
      "    params:\n",
      "    - discreteValues:\n",
      "      - 200.0\n",
      "      - 500.0\n",
      "      parameterName: max_iter\n",
      "      type: DISCRETE\n",
      "    - maxValue: 0.001\n",
      "      minValue: 1e-05\n",
      "      parameterName: alpha\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: DOUBLE\n",
      "  jobDir: gs://benazirsproject-demo/jobs/JOB_20200826_185451\n",
      "  masterConfig:\n",
      "    imageUri: gcr.io/benazirsproject/trainer_image:latest\n",
      "  region: us-central1\n",
      "trainingOutput:\n",
      "  hyperparameterMetricTag: accuracy\n",
      "  isHyperparameterTuningJob: true\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/JOB_20200826_185451?project=benazirsproject\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2FJOB_20200826_185451&project=benazirsproject\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\n",
      "\n",
      "Command killed by keyboard interrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs stream-logs $JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobId': 'JOB_20200826_185451',\n",
       " 'trainingInput': {'args': ['--training_dataset_path=gs://benazirsproject-demo/data/training/dataset.csv',\n",
       "   '--validation_dataset_path=gs://benazirsproject-demo/data/validation/dataset.csv',\n",
       "   '--hptune'],\n",
       "  'hyperparameters': {'goal': 'MAXIMIZE',\n",
       "   'params': [{'parameterName': 'max_iter',\n",
       "     'type': 'DISCRETE',\n",
       "     'discreteValues': [200, 500]},\n",
       "    {'parameterName': 'alpha',\n",
       "     'minValue': 1e-05,\n",
       "     'maxValue': 0.001,\n",
       "     'type': 'DOUBLE',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'}],\n",
       "   'maxTrials': 4,\n",
       "   'maxParallelTrials': 4,\n",
       "   'hyperparameterMetricTag': 'accuracy',\n",
       "   'enableTrialEarlyStopping': True},\n",
       "  'region': 'us-central1',\n",
       "  'jobDir': 'gs://benazirsproject-demo/jobs/JOB_20200826_185451',\n",
       "  'masterConfig': {'imageUri': 'gcr.io/benazirsproject/trainer_image:latest'}},\n",
       " 'createTime': '2020-08-26T18:54:53Z',\n",
       " 'startTime': '2020-08-26T18:54:55Z',\n",
       " 'endTime': '2020-08-26T19:05:24Z',\n",
       " 'state': 'SUCCEEDED',\n",
       " 'trainingOutput': {'completedTrialCount': '4',\n",
       "  'trials': [{'trialId': '3',\n",
       "    'hyperparameters': {'alpha': '0.00027200708548161929', 'max_iter': '200'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.7015046766978447},\n",
       "    'startTime': '2020-08-26T18:55:32.497038944Z',\n",
       "    'endTime': '2020-08-26T19:03:55Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '4',\n",
       "    'hyperparameters': {'max_iter': '500', 'alpha': '0.00097031656406342188'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6984546563643758},\n",
       "    'startTime': '2020-08-26T18:55:32.497122137Z',\n",
       "    'endTime': '2020-08-26T19:04:05Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '1',\n",
       "    'hyperparameters': {'max_iter': '500', 'alpha': '0.000505'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6964213094753965},\n",
       "    'startTime': '2020-08-26T18:55:32.496751683Z',\n",
       "    'endTime': '2020-08-26T19:04:16Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '2',\n",
       "    'hyperparameters': {'max_iter': '200', 'alpha': '0.00071876622028743025'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6955063033753558},\n",
       "    'startTime': '2020-08-26T18:55:32.496977724Z',\n",
       "    'endTime': '2020-08-26T19:04:05Z',\n",
       "    'state': 'SUCCEEDED'}],\n",
       "  'consumedMLUnits': 0.26,\n",
       "  'isHyperparameterTuningJob': True,\n",
       "  'hyperparameterMetricTag': 'accuracy'},\n",
       " 'etag': '9NvurrKLXO4='}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = discovery.build('ml', 'v1')\n",
    "\n",
    "job_id = 'projects/{}/jobs/{}'.format(PROJECT_ID, JOB_NAME)\n",
    "request = ml.projects().jobs().get(name=job_id)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "except errors.HttpError as err:\n",
    "    print(err)\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "    \n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trialId': '3',\n",
       " 'hyperparameters': {'alpha': '0.00027200708548161929', 'max_iter': '200'},\n",
       " 'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.7015046766978447},\n",
       " 'startTime': '2020-08-26T18:55:32.497038944Z',\n",
       " 'endTime': '2020-08-26T19:03:55Z',\n",
       " 'state': 'SUCCEEDED'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['trainingOutput']['trials'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrain with best hparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = response['trainingOutput']['trials'][0]['hyperparameters']['alpha']\n",
    "max_iter = response['trainingOutput']['trials'][0]['hyperparameters']['max_iter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20200826_190645] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20200826_190645\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20200826_190645\n",
      "jobId: JOB_20200826_190645\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$IMAGE_URI \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "-- \\\n",
    "--training_dataset_path=$TRAINING_FILE_PATH \\\n",
    "--validation_dataset_path=$VALIDATION_FILE_PATH \\\n",
    "--alpha=$alpha \\\n",
    "--max_iter=$max_iter \\\n",
    "--nohptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs stream-logs $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deploy model to ai platform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Created ml engine model [projects/benazirsproject/models/amyris].\n"
     ]
    }
   ],
   "source": [
    "## create a model resource \n",
    "\n",
    "model_name = 'amyris'\n",
    "labels = \"task=classifier,domain=forestry\"\n",
    "filter = 'name:{}'.format(model_name)\n",
    "# models = !(gcloud ai-platform models list --filter={filter} --format='value(name)')\n",
    "\n",
    "# if not models:\n",
    "!gcloud ai-platform models create  $model_name \\\n",
    "--regions=$REGION \\\n",
    "--labels=$labels\n",
    "# else:\n",
    "#     print(\"Model: {} already exists.\".format(models[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Creating version (this might take a few minutes)......done.                    \n"
     ]
    }
   ],
   "source": [
    "# create a model version\n",
    "model_version = 'v01'\n",
    "filter = 'name:{}'.format(model_version)\n",
    "# versions = !(gcloud ai-platform versions list --model={model_name} --format='value(name)' --filter={filter})\n",
    "\n",
    "# if not versions:\n",
    "!gcloud ai-platform versions create {model_version} \\\n",
    "    --model={model_name} \\\n",
    "    --origin=$JOB_DIR \\\n",
    "    --runtime-version=1.15 \\\n",
    "    --framework=scikit-learn \\\n",
    "    --python-version=3.7\n",
    "# else:\n",
    "#     print(\"Model version: {} already exists.\".format(versions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'serving_instances.json'\n",
    "\n",
    "with open(input_file, 'w') as f:\n",
    "    for index, row in X_validation.head().iterrows():\n",
    "        f.write(json.dumps(list(row.values)))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform predict \\\n",
    "--model $model_name \\\n",
    "--version $model_version \\\n",
    "--json-instances $input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. create a pipeline folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline/covertype_training_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline/covertype_training_pipeline.py\n",
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"KFP pipeline orchestrating BigQuery and Cloud AI Platform services.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from helper_components import evaluate_model\n",
    "from helper_components import retrieve_best_run\n",
    "from jinja2 import Template\n",
    "import kfp\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp.dsl.types import Dict\n",
    "from kfp.dsl.types import GCPProjectID\n",
    "from kfp.dsl.types import GCPRegion\n",
    "from kfp.dsl.types import GCSPath\n",
    "from kfp.dsl.types import String\n",
    "from kfp.gcp import use_gcp_secret\n",
    "\n",
    "# Defaults and environment settings\n",
    "BASE_IMAGE = os.getenv('BASE_IMAGE')\n",
    "TRAINER_IMAGE = os.getenv('TRAINER_IMAGE')\n",
    "RUNTIME_VERSION = os.getenv('RUNTIME_VERSION')\n",
    "PYTHON_VERSION = os.getenv('PYTHON_VERSION')\n",
    "COMPONENT_URL_SEARCH_PREFIX = os.getenv('COMPONENT_URL_SEARCH_PREFIX')\n",
    "USE_KFP_SA = os.getenv('USE_KFP_SA')\n",
    "\n",
    "TRAINING_FILE_PATH = 'datasets/training/data.csv'\n",
    "VALIDATION_FILE_PATH = 'datasets/validation/data.csv'\n",
    "TESTING_FILE_PATH = 'datasets/testing/data.csv'\n",
    "\n",
    "# Parameter defaults\n",
    "SPLITS_DATASET_ID = 'splits'\n",
    "HYPERTUNE_SETTINGS = \"\"\"\n",
    "{\n",
    "    \"hyperparameters\":  {\n",
    "        \"goal\": \"MAXIMIZE\",\n",
    "        \"maxTrials\": 6,\n",
    "        \"maxParallelTrials\": 3,\n",
    "        \"hyperparameterMetricTag\": \"accuracy\",\n",
    "        \"enableTrialEarlyStopping\": True,\n",
    "        \"params\": [\n",
    "            {\n",
    "                \"parameterName\": \"max_iter\",\n",
    "                \"type\": \"DISCRETE\",\n",
    "                \"discreteValues\": [500, 1000]\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"alpha\",\n",
    "                \"type\": \"DOUBLE\",\n",
    "                \"minValue\": 0.0001,\n",
    "                \"maxValue\": 0.001,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def generate_sampling_query(source_table_name, num_lots, lots):\n",
    "    \"\"\"Prepares the data sampling query.\"\"\"\n",
    "\n",
    "    sampling_query_template = \"\"\"\n",
    "         SELECT *\n",
    "         FROM \n",
    "             `{{ source_table }}` AS cover\n",
    "         WHERE \n",
    "         MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) IN ({{ lots }})\n",
    "         \"\"\"\n",
    "    query = Template(sampling_query_template).render(\n",
    "        source_table=source_table_name, num_lots=num_lots, lots=str(lots)[1:-1])\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "bigquery_query_op = component_store.load_component('bigquery/query')\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')\n",
    "retrieve_best_run_op = func_to_container_op(\n",
    "    retrieve_best_run, base_image=BASE_IMAGE)\n",
    "evaluate_model_op = func_to_container_op(evaluate_model, base_image=BASE_IMAGE)\n",
    "\n",
    "\n",
    "@kfp.dsl.pipeline(\n",
    "    name='Covertype Classifier Training',\n",
    "    description='The pipeline training and deploying the Covertype classifierpipeline_yaml'\n",
    ")\n",
    "def covertype_train(project_id,\n",
    "                    region,\n",
    "                    source_table_name,\n",
    "                    gcs_root,\n",
    "                    dataset_id,\n",
    "                    evaluation_metric_name,\n",
    "                    evaluation_metric_threshold,\n",
    "                    model_id,\n",
    "                    version_id,\n",
    "                    replace_existing_version,\n",
    "                    hypertune_settings=HYPERTUNE_SETTINGS,\n",
    "                    dataset_location='US'):\n",
    "    \"\"\"Orchestrates training and deployment of an sklearn model.\"\"\"\n",
    "\n",
    "    # Create the training split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[1, 2, 3, 4])\n",
    "\n",
    "    training_file_path = '{}/{}'.format(gcs_root, TRAINING_FILE_PATH)\n",
    "\n",
    "    create_training_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=training_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Create the validation split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[8])\n",
    "\n",
    "    validation_file_path = '{}/{}'.format(gcs_root, VALIDATION_FILE_PATH)\n",
    "\n",
    "    create_validation_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=validation_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Create the testing split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[9])\n",
    "\n",
    "    testing_file_path = '{}/{}'.format(gcs_root, TESTING_FILE_PATH)\n",
    "\n",
    "    create_testing_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=testing_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Tune hyperparameters\n",
    "    tune_args = [\n",
    "        '--training_dataset_path',\n",
    "        create_training_split.outputs['output_gcs_path'],\n",
    "        '--validation_dataset_path',\n",
    "        create_validation_split.outputs['output_gcs_path'], '--hptune', 'True'\n",
    "    ]\n",
    "\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir/hypertune',\n",
    "                                kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    hypertune = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAINER_IMAGE,\n",
    "        job_dir=job_dir,\n",
    "        args=tune_args,\n",
    "        training_input=hypertune_settings)\n",
    "\n",
    "    # Retrieve the best trial\n",
    "    get_best_trial = retrieve_best_run_op(\n",
    "            project_id, hypertune.outputs['job_id'])\n",
    "\n",
    "    # Train the model on a combined training and validation datasets\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir', kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    train_args = [\n",
    "        '--training_dataset_path',\n",
    "        create_training_split.outputs['output_gcs_path'],\n",
    "        '--validation_dataset_path',\n",
    "        create_validation_split.outputs['output_gcs_path'], '--alpha',\n",
    "        get_best_trial.outputs['alpha'], '--max_iter',\n",
    "        get_best_trial.outputs['max_iter'], '--hptune', 'False'\n",
    "    ]\n",
    "\n",
    "    train_model = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAINER_IMAGE,\n",
    "        job_dir=job_dir,\n",
    "        args=train_args)\n",
    "\n",
    "    # Evaluate the model on the testing split\n",
    "    eval_model = evaluate_model_op(\n",
    "        dataset_path=str(create_testing_split.outputs['output_gcs_path']),\n",
    "        model_path=str(train_model.outputs['job_dir']),\n",
    "        metric_name=evaluation_metric_name)\n",
    "\n",
    "    # Deploy the model if the primary metric is better than threshold\n",
    "    with kfp.dsl.Condition(eval_model.outputs['metric_value'] > evaluation_metric_threshold):\n",
    "        deploy_model = mlengine_deploy_op(\n",
    "        model_uri=train_model.outputs['job_dir'],\n",
    "        project_id=project_id,\n",
    "        model_id=model_id,\n",
    "        version_id=version_id,\n",
    "        runtime_version=RUNTIME_VERSION,\n",
    "        python_version=PYTHON_VERSION,\n",
    "        replace_existing_version=replace_existing_version)\n",
    "\n",
    "    # Configure the pipeline to run using the service account defined\n",
    "    # in the user-gcp-sa k8s secret\n",
    "    if USE_KFP_SA == 'True':\n",
    "        kfp.dsl.get_pipeline_conf().add_op_transformer(\n",
    "              use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./base_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./base_image/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ENDPOINT = '19a5aed0f754a516-dot-us-central2.pipelines.googleusercontent.com'\n",
    "ARTIFACT_STORE_URI = 'gs://benazirsproject-demo'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/mlops-on-gcp/workshops/kfp-caip-sklearn/lab_02_self_test'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./$TRAINING_APP_FOLDER/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. copy /trainer_image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='trainer_image'\n",
    "TAG='latest'\n",
    "TRAINER_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 4 file(s) totalling 7.2 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://benazirsproject_cloudbuild/source/1598483658.08-65876e66f4a64641a6920c1c697d45da.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/benazirsproject/builds/d4a52430-4eab-4df4-a17b-f6939f419eba].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/d4a52430-4eab-4df4-a17b-f6939f419eba?project=981930454113].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"d4a52430-4eab-4df4-a17b-f6939f419eba\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://benazirsproject_cloudbuild/source/1598483658.08-65876e66f4a64641a6920c1c697d45da.tgz#1598483658638891\n",
      "Copying gs://benazirsproject_cloudbuild/source/1598483658.08-65876e66f4a64641a6920c1c697d45da.tgz#1598483658638891...\n",
      "/ [1 files][  1.9 KiB/  1.9 KiB]                                                \n",
      "Operation completed over 1 objects/1.9 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  11.78kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7c3167c320d: Pulling fs layer\n",
      "131f805ec7fd: Pulling fs layer\n",
      "322ed380e680: Pulling fs layer\n",
      "6ac240b13098: Pulling fs layer\n",
      "9ce3a9266402: Pulling fs layer\n",
      "72c706dfac1d: Pulling fs layer\n",
      "6383427606e5: Pulling fs layer\n",
      "3e8b21666cec: Pulling fs layer\n",
      "358bb5d659ed: Pulling fs layer\n",
      "8ade7556a8f1: Pulling fs layer\n",
      "b2ebb7e1223e: Pulling fs layer\n",
      "8d5d283ad922: Pulling fs layer\n",
      "14c0fd48a5f3: Pulling fs layer\n",
      "ceaad5dc04d2: Pulling fs layer\n",
      "c1074350f761: Pulling fs layer\n",
      "687ad0b9a318: Pulling fs layer\n",
      "d2365d2ee19a: Pulling fs layer\n",
      "5095b04f1d98: Pulling fs layer\n",
      "6ac240b13098: Waiting\n",
      "9ce3a9266402: Waiting\n",
      "72c706dfac1d: Waiting\n",
      "6383427606e5: Waiting\n",
      "3e8b21666cec: Waiting\n",
      "358bb5d659ed: Waiting\n",
      "8ade7556a8f1: Waiting\n",
      "b2ebb7e1223e: Waiting\n",
      "8d5d283ad922: Waiting\n",
      "14c0fd48a5f3: Waiting\n",
      "ceaad5dc04d2: Waiting\n",
      "c1074350f761: Waiting\n",
      "687ad0b9a318: Waiting\n",
      "d2365d2ee19a: Waiting\n",
      "5095b04f1d98: Waiting\n",
      "131f805ec7fd: Verifying Checksum\n",
      "131f805ec7fd: Download complete\n",
      "322ed380e680: Verifying Checksum\n",
      "322ed380e680: Download complete\n",
      "6ac240b13098: Verifying Checksum\n",
      "6ac240b13098: Download complete\n",
      "d7c3167c320d: Verifying Checksum\n",
      "d7c3167c320d: Download complete\n",
      "6383427606e5: Verifying Checksum\n",
      "6383427606e5: Download complete\n",
      "72c706dfac1d: Verifying Checksum\n",
      "72c706dfac1d: Download complete\n",
      "358bb5d659ed: Verifying Checksum\n",
      "358bb5d659ed: Download complete\n",
      "8ade7556a8f1: Verifying Checksum\n",
      "8ade7556a8f1: Download complete\n",
      "b2ebb7e1223e: Verifying Checksum\n",
      "b2ebb7e1223e: Download complete\n",
      "8d5d283ad922: Verifying Checksum\n",
      "8d5d283ad922: Download complete\n",
      "14c0fd48a5f3: Verifying Checksum\n",
      "14c0fd48a5f3: Download complete\n",
      "ceaad5dc04d2: Verifying Checksum\n",
      "ceaad5dc04d2: Download complete\n",
      "c1074350f761: Verifying Checksum\n",
      "c1074350f761: Download complete\n",
      "9ce3a9266402: Verifying Checksum\n",
      "9ce3a9266402: Download complete\n",
      "687ad0b9a318: Verifying Checksum\n",
      "687ad0b9a318: Download complete\n",
      "5095b04f1d98: Verifying Checksum\n",
      "5095b04f1d98: Download complete\n",
      "3e8b21666cec: Verifying Checksum\n",
      "3e8b21666cec: Download complete\n",
      "d7c3167c320d: Pull complete\n",
      "131f805ec7fd: Pull complete\n",
      "d2365d2ee19a: Verifying Checksum\n",
      "d2365d2ee19a: Download complete\n",
      "322ed380e680: Pull complete\n",
      "6ac240b13098: Pull complete\n",
      "9ce3a9266402: Pull complete\n",
      "72c706dfac1d: Pull complete\n",
      "6383427606e5: Pull complete\n",
      "3e8b21666cec: Pull complete\n",
      "358bb5d659ed: Pull complete\n",
      "8ade7556a8f1: Pull complete\n",
      "b2ebb7e1223e: Pull complete\n",
      "8d5d283ad922: Pull complete\n",
      "14c0fd48a5f3: Pull complete\n",
      "ceaad5dc04d2: Pull complete\n",
      "c1074350f761: Pull complete\n",
      "687ad0b9a318: Pull complete\n",
      "d2365d2ee19a: Pull complete\n",
      "5095b04f1d98: Pull complete\n",
      "Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d8706668f140\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in 118b5c7df50b\n",
      "Collecting fire\n",
      "  Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=91b8093f05fee77e4ccd0316e1c6f6e259405dbad5d24668c065f78298daeae1\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3986 sha256=eaa9170aa1169e17edb4e96effe1b79044f954e49921a9ae3b5c5572a94cd862\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=71b4837353296e721573ecca442b61ec9da084f8f733c3be3a851c60fef0ea6c\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "\u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: termcolor, fire, cloudml-hypertune, scikit-learn, pandas\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.1\n",
      "    Uninstalling scikit-learn-0.23.1:\n",
      "      Successfully uninstalled scikit-learn-0.23.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.3.1 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "Removing intermediate container 118b5c7df50b\n",
      " ---> 0304c4afbd3f\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in c525ec77b4fa\n",
      "Removing intermediate container c525ec77b4fa\n",
      " ---> 7b3a89aed2b0\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 3bd62a7dedc8\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 62d37dc3b2b5\n",
      "Removing intermediate container 62d37dc3b2b5\n",
      " ---> 7256a542d4f7\n",
      "Successfully built 7256a542d4f7\n",
      "Successfully tagged gcr.io/benazirsproject/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/benazirsproject/trainer_image:latest\n",
      "The push refers to repository [gcr.io/benazirsproject/trainer_image]\n",
      "6fac39a8e5c2: Preparing\n",
      "98b461cdd4c8: Preparing\n",
      "61b1c3448615: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "222959643149: Waiting\n",
      "badaf1bc8335: Waiting\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "89212ed9ad75: Layer already exists\n",
      "c51fe61c6231: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "98b461cdd4c8: Pushed\n",
      "c8cc397a1d54: Layer already exists\n",
      "6fac39a8e5c2: Pushed\n",
      "4c4a5579b7a8: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "61b1c3448615: Pushed\n",
      "latest: digest: sha256:9871ce062de415ff35477dfba4dc8db7d29ec16fb8df29aab3fca78d473afa3a size: 4708\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                     IMAGES                                          STATUS\n",
      "d4a52430-4eab-4df4-a17b-f6939f419eba  2020-08-26T23:14:18+00:00  3M51S     gs://benazirsproject_cloudbuild/source/1598483658.08-65876e66f4a64641a6920c1c697d45da.tgz  gcr.io/benazirsproject/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $TRAINER_IMAGE training_app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copy base image folder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='base_image'\n",
    "TAG='latest'\n",
    "BASE_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 244 bytes before compression.\n",
      "Uploading tarball of [base_image] to [gs://benazirsproject_cloudbuild/source/1598483933.72-711e0acfe0cd4f17b1b8d0961b8e896e.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/benazirsproject/builds/983bd107-1259-44a5-9b9f-a0c03a49a2d1].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/983bd107-1259-44a5-9b9f-a0c03a49a2d1?project=981930454113].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"983bd107-1259-44a5-9b9f-a0c03a49a2d1\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://benazirsproject_cloudbuild/source/1598483933.72-711e0acfe0cd4f17b1b8d0961b8e896e.tgz#1598483934175368\n",
      "Copying gs://benazirsproject_cloudbuild/source/1598483933.72-711e0acfe0cd4f17b1b8d0961b8e896e.tgz#1598483934175368...\n",
      "/ [1 files][  290.0 B/  290.0 B]                                                \n",
      "Operation completed over 1 objects/290.0 B.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  3.584kB\n",
      "Step 1/2 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7c3167c320d: Pulling fs layer\n",
      "131f805ec7fd: Pulling fs layer\n",
      "322ed380e680: Pulling fs layer\n",
      "6ac240b13098: Pulling fs layer\n",
      "9ce3a9266402: Pulling fs layer\n",
      "72c706dfac1d: Pulling fs layer\n",
      "6383427606e5: Pulling fs layer\n",
      "3e8b21666cec: Pulling fs layer\n",
      "358bb5d659ed: Pulling fs layer\n",
      "8ade7556a8f1: Pulling fs layer\n",
      "b2ebb7e1223e: Pulling fs layer\n",
      "8d5d283ad922: Pulling fs layer\n",
      "14c0fd48a5f3: Pulling fs layer\n",
      "ceaad5dc04d2: Pulling fs layer\n",
      "c1074350f761: Pulling fs layer\n",
      "687ad0b9a318: Pulling fs layer\n",
      "d2365d2ee19a: Pulling fs layer\n",
      "5095b04f1d98: Pulling fs layer\n",
      "6ac240b13098: Waiting\n",
      "9ce3a9266402: Waiting\n",
      "6383427606e5: Waiting\n",
      "3e8b21666cec: Waiting\n",
      "358bb5d659ed: Waiting\n",
      "8ade7556a8f1: Waiting\n",
      "b2ebb7e1223e: Waiting\n",
      "8d5d283ad922: Waiting\n",
      "14c0fd48a5f3: Waiting\n",
      "ceaad5dc04d2: Waiting\n",
      "c1074350f761: Waiting\n",
      "687ad0b9a318: Waiting\n",
      "d2365d2ee19a: Waiting\n",
      "5095b04f1d98: Waiting\n",
      "72c706dfac1d: Waiting\n",
      "322ed380e680: Verifying Checksum\n",
      "322ed380e680: Download complete\n",
      "131f805ec7fd: Verifying Checksum\n",
      "131f805ec7fd: Download complete\n",
      "6ac240b13098: Verifying Checksum\n",
      "6ac240b13098: Download complete\n",
      "d7c3167c320d: Verifying Checksum\n",
      "d7c3167c320d: Download complete\n",
      "6383427606e5: Verifying Checksum\n",
      "6383427606e5: Download complete\n",
      "72c706dfac1d: Verifying Checksum\n",
      "72c706dfac1d: Download complete\n",
      "3e8b21666cec: Verifying Checksum\n",
      "3e8b21666cec: Download complete\n",
      "358bb5d659ed: Verifying Checksum\n",
      "358bb5d659ed: Download complete\n",
      "8ade7556a8f1: Verifying Checksum\n",
      "8ade7556a8f1: Download complete\n",
      "b2ebb7e1223e: Verifying Checksum\n",
      "b2ebb7e1223e: Download complete\n",
      "14c0fd48a5f3: Verifying Checksum\n",
      "14c0fd48a5f3: Download complete\n",
      "8d5d283ad922: Verifying Checksum\n",
      "8d5d283ad922: Download complete\n",
      "c1074350f761: Download complete\n",
      "ceaad5dc04d2: Verifying Checksum\n",
      "ceaad5dc04d2: Download complete\n",
      "687ad0b9a318: Verifying Checksum\n",
      "687ad0b9a318: Download complete\n",
      "5095b04f1d98: Verifying Checksum\n",
      "5095b04f1d98: Download complete\n",
      "9ce3a9266402: Verifying Checksum\n",
      "9ce3a9266402: Download complete\n",
      "d2365d2ee19a: Verifying Checksum\n",
      "d2365d2ee19a: Download complete\n",
      "d7c3167c320d: Pull complete\n",
      "131f805ec7fd: Pull complete\n",
      "322ed380e680: Pull complete\n",
      "6ac240b13098: Pull complete\n",
      "9ce3a9266402: Pull complete\n",
      "72c706dfac1d: Pull complete\n",
      "6383427606e5: Pull complete\n",
      "3e8b21666cec: Pull complete\n",
      "358bb5d659ed: Pull complete\n",
      "8ade7556a8f1: Pull complete\n",
      "b2ebb7e1223e: Pull complete\n",
      "8d5d283ad922: Pull complete\n",
      "14c0fd48a5f3: Pull complete\n",
      "ceaad5dc04d2: Pull complete\n",
      "c1074350f761: Pull complete\n",
      "687ad0b9a318: Pull complete\n",
      "d2365d2ee19a: Pull complete\n",
      "5095b04f1d98: Pull complete\n",
      "Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d8706668f140\n",
      "Step 2/2 : RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5\n",
      " ---> Running in 6555e0a92569\n",
      "Collecting fire\n",
      "  Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Collecting kfp==0.2.5\n",
      "  Downloading kfp-0.2.5.tar.gz (116 kB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Collecting urllib3<1.25,>=1.15\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.29.0)\n",
      "Collecting kubernetes<=10.0.0,>=8.0.0\n",
      "  Downloading kubernetes-10.0.0-py2.py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied, skipping upgrade: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.9.2)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.17.2)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting cloudpickle==1.1.1\n",
      "  Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting kfp-server-api<=0.1.40,>=0.1.18\n",
      "  Downloading kfp-server-api-0.1.40.tar.gz (38 kB)\n",
      "Collecting argo-models==2.2.1a\n",
      "  Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (3.2.0)\n",
      "Collecting tabulate==0.8.3\n",
      "  Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "Collecting click==7.0\n",
      "  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.10-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.9.tar.gz (30 kB)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (0.5.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (47.3.1.post20200616)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (0.57.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.2.5) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (0.2.7)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==0.2.5) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp==0.2.5) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.4.2->kfp==0.2.5) (2.20)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==0.2.5) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp==0.2.5) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (3.12.3)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.51.0)\n",
      "Building wheels for collected packages: fire, kfp, termcolor, kfp-server-api, argo-models, tabulate, strip-hints\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=0e816db0afcebaaa8afa229744bf5809ca45453475365dd862497a309572dc01\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-0.2.5-py3-none-any.whl size=159978 sha256=6273dc25955a595058fdc1d74926a68f3484959039e046ab5461eab40d2502fc\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/74/7e/0a882d654bdf82d039460ab5c6adf8724ae56e277de7c0eaea\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=4a60a5ca0d4a132d02522465d19f4913c60e8c63f77b6c5967a10c79c345829a\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-0.1.40-py3-none-any.whl size=102468 sha256=3186527a51a3b34b00ab97741bb77d7705b1631761b0a4c8bdc9a99464e405f3\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/e3/43/3972dea76ee89e35f090b313817089043f2609236cf560069d\n",
      "  Building wheel for argo-models (setup.py): started\n",
      "  Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "  Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57307 sha256=eb7b2893510fd64086196e8fb3bd9f2d9168754f748d8b55a894c907786fdd2c\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/4b/fd/cdd013bd2ad1a7162ecfaf954e9f1bb605174a20e3c02016b7\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23378 sha256=4cd4e11ece5e48df7151e52a1df8e595d8ee6b351579ed5df345547e6d9dbf8c\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.9-py2.py3-none-any.whl size=20993 sha256=ee6c6e579f758cd4e5f9a04a74de4a5d6ba9c69961863377039fc47482aa8948\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/b8/4e/a3ec111d2db63cec88121bd7c0ab1a123bce3b55dd19dda5c1\n",
      "Successfully built fire kfp termcolor kfp-server-api argo-models tabulate strip-hints\n",
      "\u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: jupyterlab-git 0.10.1 has requirement nbdime<2.0.0,>=1.1.0, but you'll have nbdime 2.0.0 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: distributed 2.19.0 has requirement cloudpickle>=1.3.0, but you'll have cloudpickle 1.1.1 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: termcolor, fire, scikit-learn, pandas, urllib3, kubernetes, requests-toolbelt, cloudpickle, kfp-server-api, argo-models, tabulate, click, Deprecated, strip-hints, kfp\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.1\n",
      "    Uninstalling scikit-learn-0.23.1:\n",
      "      Successfully uninstalled scikit-learn-0.23.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.9\n",
      "    Uninstalling urllib3-1.25.9:\n",
      "      Successfully uninstalled urllib3-1.25.9\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 11.0.0\n",
      "    Uninstalling kubernetes-11.0.0:\n",
      "      Successfully uninstalled kubernetes-11.0.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.4.1\n",
      "    Uninstalling cloudpickle-1.4.1:\n",
      "      Successfully uninstalled cloudpickle-1.4.1\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 7.1.2\n",
      "    Uninstalling click-7.1.2:\n",
      "      Successfully uninstalled click-7.1.2\n",
      "Successfully installed Deprecated-1.2.10 argo-models-2.2.1a0 click-7.0 cloudpickle-1.1.1 fire-0.3.1 kfp-0.2.5 kfp-server-api-0.1.40 kubernetes-10.0.0 pandas-0.24.2 requests-toolbelt-0.9.1 scikit-learn-0.20.4 strip-hints-0.1.9 tabulate-0.8.3 termcolor-1.1.0 urllib3-1.24.3\n",
      "Removing intermediate container 6555e0a92569\n",
      " ---> daeeb498c4b1\n",
      "Successfully built daeeb498c4b1\n",
      "Successfully tagged gcr.io/benazirsproject/base_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/benazirsproject/base_image:latest\n",
      "The push refers to repository [gcr.io/benazirsproject/base_image]\n",
      "f64ae27b105d: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "c51fe61c6231: Layer already exists\n",
      "89212ed9ad75: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "4c4a5579b7a8: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "f64ae27b105d: Pushed\n",
      "latest: digest: sha256:7cc1f5abfc5cb84e0dd1f18bb22ef1e752b243e6f3dc52b03028a1e27708ee0e size: 4293\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                     IMAGES                                       STATUS\n",
      "983bd107-1259-44a5-9b9f-a0c03a49a2d1  2020-08-26T23:18:54+00:00  4M3S      gs://benazirsproject_cloudbuild/source/1598483933.72-711e0acfe0cd4f17b1b8d0961b8e896e.tgz  gcr.io/benazirsproject/base_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $BASE_IMAGE base_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compile pipeline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: USE_KFP_SA=False\n",
      "env: BASE_IMAGE=gcr.io/benazirsproject/base_image:latest\n",
      "env: TRAINER_IMAGE=gcr.io/benazirsproject/trainer_image:latest\n",
      "env: COMPONENT_URL_SEARCH_PREFIX=https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/\n",
      "env: RUNTIME_VERSION=1.15\n",
      "env: PYTHON_VERSION=3.7\n"
     ]
    }
   ],
   "source": [
    "USE_KFP_SA = False\n",
    "\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/'\n",
    "RUNTIME_VERSION = '1.15'\n",
    "PYTHON_VERSION = '3.7'\n",
    "\n",
    "%env USE_KFP_SA={USE_KFP_SA}\n",
    "%env BASE_IMAGE={BASE_IMAGE}\n",
    "%env TRAINER_IMAGE={TRAINER_IMAGE}\n",
    "%env COMPONENT_URL_SEARCH_PREFIX={COMPONENT_URL_SEARCH_PREFIX}\n",
    "%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "%env PYTHON_VERSION={PYTHON_VERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy helper_componenets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dsl-compile --py pipeline/covertype_training_pipeline.py --output covertype_training_pipeline.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: argoproj.io/v1alpha1\n",
      "kind: Workflow\n",
      "metadata:\n",
      "  generateName: covertype-classifier-training-\n",
      "  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.0.0, pipelines.kubeflow.org/pipeline_compilation_time: '2020-08-26T23:24:58.156040',\n",
      "    pipelines.kubeflow.org/pipeline_spec: '{\"description\": \"The pipeline training\n",
      "      and deploying the Covertype classifierpipeline_yaml\", \"inputs\": [{\"name\": \"project_id\"},\n",
      "      {\"name\": \"region\"}, {\"name\": \"source_table_name\"}, {\"name\": \"gcs_root\"}, {\"name\":\n",
      "      \"dataset_id\"}, {\"name\": \"evaluation_metric_name\"}, {\"name\": \"evaluation_metric_threshold\"},\n",
      "      {\"name\": \"model_id\"}, {\"name\": \"version_id\"}, {\"name\": \"replace_existing_version\"},\n"
     ]
    }
   ],
   "source": [
    "!head covertype_training_pipeline.yaml ## where is this file . ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deploy the pipeline package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline b6b2b054-e524-4acf-b3fc-0e7e3ba271bd has been submitted\n",
      "\n",
      "Pipeline Details\n",
      "------------------\n",
      "ID           b6b2b054-e524-4acf-b3fc-0e7e3ba271bd\n",
      "Name         amyris_pipeline\n",
      "Description\n",
      "Uploaded at  2020-08-26T23:27:54+00:00\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| Parameter Name              | Default Value                                    |\n",
      "+=============================+==================================================+\n",
      "| project_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| region                      |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| source_table_name           |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| gcs_root                    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| dataset_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| evaluation_metric_name      |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| evaluation_metric_threshold |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| model_id                    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| version_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| replace_existing_version    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| hypertune_settings          | {                                                |\n",
      "|                             |     \"hyperparameters\":  {                        |\n",
      "|                             |         \"goal\": \"MAXIMIZE\",                      |\n",
      "|                             |         \"maxTrials\": 6,                          |\n",
      "|                             |         \"maxParallelTrials\": 3,                  |\n",
      "|                             |         \"hyperparameterMetricTag\": \"accuracy\",   |\n",
      "|                             |         \"enableTrialEarlyStopping\": True,        |\n",
      "|                             |         \"params\": [                              |\n",
      "|                             |             {                                    |\n",
      "|                             |                 \"parameterName\": \"max_iter\",     |\n",
      "|                             |                 \"type\": \"DISCRETE\",              |\n",
      "|                             |                 \"discreteValues\": [500, 1000]    |\n",
      "|                             |             },                                   |\n",
      "|                             |             {                                    |\n",
      "|                             |                 \"parameterName\": \"alpha\",        |\n",
      "|                             |                 \"type\": \"DOUBLE\",                |\n",
      "|                             |                 \"minValue\": 0.0001,              |\n",
      "|                             |                 \"maxValue\": 0.001,               |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\" |\n",
      "|                             |             }                                    |\n",
      "|                             |         ]                                        |\n",
      "|                             |     }                                            |\n",
      "|                             | }                                                |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| dataset_location            | US                                               |\n",
      "+-----------------------------+--------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME='amyris_pipeline'\n",
    "\n",
    "!kfp --endpoint $ENDPOINT pipeline upload \\\n",
    "-p $PIPELINE_NAME \\\n",
    "covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| Pipeline ID                          | Name                                            | Uploaded at               |\n",
      "+======================================+=================================================+===========================+\n",
      "| b6b2b054-e524-4acf-b3fc-0e7e3ba271bd | amyris_pipeline                                 | 2020-08-26T23:27:54+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 4ffefa9f-c1aa-4b11-9b4b-ad70c223226e | covertype_continuous_training_self              | 2020-08-26T23:25:58+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 1b5be90d-6020-43fc-8126-b6d979da2e39 | [Tutorial] DSL - Control structures             | 2020-08-26T22:17:35+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| b162aff5-6673-41eb-8619-221ccfaaa71c | [Tutorial] Data passing in python components    | 2020-08-26T22:17:34+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 81d20eef-4bb7-4bed-8d87-569c56f824f5 | [Demo] TFX - Iris classification pipeline       | 2020-08-26T22:17:33+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 5707fa44-e6d1-403f-88fb-88ee299d2f33 | [Demo] TFX - Taxi tip prediction model trainer  | 2020-08-26T22:17:32+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 6f868f09-82bc-49b5-b44f-ed260eb653e0 | [Demo] XGBoost - Training with confusion matrix | 2020-08-26T22:17:31+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT pipeline list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID='b6b2b054-e524-4acf-b3fc-0e7e3ba271bd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'Amyris TRaining'\n",
    "RUN_ID = 'Run_001'\n",
    "SOURCE_TABLE = 'covertype_dataset.covertype'\n",
    "DATASET_ID = 'splits'\n",
    "EVALUATION_METRIC = 'accuracy'\n",
    "EVALUATION_METRIC_THRESHOLD = '0.69'\n",
    "MODEL_ID = 'covertype_classifier'\n",
    "VERSION_ID = 'v01'\n",
    "REPLACE_EXISTING_VERSION = 'True'\n",
    "\n",
    "GCS_STAGING_PATH = '{}/staging'.format(ARTIFACT_STORE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary update sequence element #0 has length 1; 2 is required\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT run submit \\\n",
    "-e $EXPERIMENT_NAME \\\n",
    "-r $RUN_ID \\\n",
    "-p $PIPELINE_ID \\\n",
    "project_id=$PROJECT_ID \\\n",
    "gcs_root=$GCS_STAGING_PATH \\\n",
    "region=$REGION \\\n",
    "source_table_name=$SOURCE_TABLE \\\n",
    "dataset_id=$DATASET_ID \\\n",
    "evaluation_metric_name=$EVALUATION_METRIC \\\n",
    "evaluation_metric_threshold=$EVALUATION_METRIC_THRESHOLD \\\n",
    "model_id=$MODEL_ID \\\n",
    "version_id=$VERSION_ID \\\n",
    "replace_existing_version=$REPLACE_EXISTING_VERSION"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
