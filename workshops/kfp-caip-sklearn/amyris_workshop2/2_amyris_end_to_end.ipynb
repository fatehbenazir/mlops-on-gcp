{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lab 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "import time \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upload data in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://benazirsproject-demo'\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'Anonymized_Fermentation_Data_final.xlsx')\n",
    "# VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'dataset.csv')\n",
    "TESTING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'testing', 'test_amyris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://benazirsproject-demo/data/testing/test_amyris.csv'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESTING_FILE_PATH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(TRAINING_FILE_PATH,sheet_name='data')\n",
    "meta_data = pd.read_excel(TRAINING_FILE_PATH, sheet_name='meta data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## they need to upload data in bigquery - and then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Purpose</th>\n",
       "      <th>experiment</th>\n",
       "      <th>run</th>\n",
       "      <th>Project_Name</th>\n",
       "      <th>run_label</th>\n",
       "      <th>Strain</th>\n",
       "      <th>strain_key</th>\n",
       "      <th>Feedstock_Parent1</th>\n",
       "      <th>Start_Time</th>\n",
       "      <th>...</th>\n",
       "      <th>Cap_Oil_Em_End__percent</th>\n",
       "      <th>Cap_PCV_End__percent</th>\n",
       "      <th>Cap_Dead_Cell_Layer_End__percent</th>\n",
       "      <th>Zeex9ieJAlt_end__g_L</th>\n",
       "      <th>Zeex9ieJ_mAU_sec_end__area</th>\n",
       "      <th>Zeex9ieJ_Screening_end__g_L</th>\n",
       "      <th>Zeex9ieJ_end__g_L</th>\n",
       "      <th>Zeex9ieJ_end__g_L_121</th>\n",
       "      <th>Zeex9ieJ_end__g_L_122</th>\n",
       "      <th>interval_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>15</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>11408-15</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>9215897</td>\n",
       "      <td>m1098919</td>\n",
       "      <td>1/20/21 2:25 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>16.32145</td>\n",
       "      <td>21.04034</td>\n",
       "      <td>NULL</td>\n",
       "      <td>7.455144</td>\n",
       "      <td>1.3341</td>\n",
       "      <td>9.923562</td>\n",
       "      <td>0.606429</td>\n",
       "      <td>0.675183</td>\n",
       "      <td>0</td>\n",
       "      <td>Cumulative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>15</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>11408-15</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>9215897</td>\n",
       "      <td>m1098919</td>\n",
       "      <td>1/20/21 2:25 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>16.32145</td>\n",
       "      <td>21.04034</td>\n",
       "      <td>NULL</td>\n",
       "      <td>7.455144</td>\n",
       "      <td>1.3341</td>\n",
       "      <td>9.923562</td>\n",
       "      <td>0.606429</td>\n",
       "      <td>0.675183</td>\n",
       "      <td>0</td>\n",
       "      <td>Curated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>15</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>11408-15</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>9215897</td>\n",
       "      <td>m1098919</td>\n",
       "      <td>1/20/21 2:25 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>16.32145</td>\n",
       "      <td>21.04034</td>\n",
       "      <td>NULL</td>\n",
       "      <td>7.455144</td>\n",
       "      <td>1.3341</td>\n",
       "      <td>9.923562</td>\n",
       "      <td>0.606429</td>\n",
       "      <td>0.675183</td>\n",
       "      <td>0</td>\n",
       "      <td>Cumulative (Day 3-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>15</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>11408-15</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>9215897</td>\n",
       "      <td>m1098919</td>\n",
       "      <td>1/20/21 2:25 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>16.32145</td>\n",
       "      <td>21.04034</td>\n",
       "      <td>NULL</td>\n",
       "      <td>7.455144</td>\n",
       "      <td>1.3341</td>\n",
       "      <td>9.923562</td>\n",
       "      <td>0.606429</td>\n",
       "      <td>0.675183</td>\n",
       "      <td>0</td>\n",
       "      <td>Single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>16</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>11409-16</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>9215897</td>\n",
       "      <td>m1098919</td>\n",
       "      <td>1/20/21 2:24 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>15.21805</td>\n",
       "      <td>20.34157</td>\n",
       "      <td>NULL</td>\n",
       "      <td>4.546885</td>\n",
       "      <td>2.179946</td>\n",
       "      <td>7.52725</td>\n",
       "      <td>0.342935</td>\n",
       "      <td>0.455659</td>\n",
       "      <td>1.684056</td>\n",
       "      <td>Cumulative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>16209-8</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8770794</td>\n",
       "      <td>m1252714</td>\n",
       "      <td>9/30/21 2:32 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>19.50613</td>\n",
       "      <td>22.02205</td>\n",
       "      <td>NULL</td>\n",
       "      <td>17.845346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>16209-8</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8770794</td>\n",
       "      <td>m1252714</td>\n",
       "      <td>9/30/21 2:32 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0144</td>\n",
       "      <td>18.78522</td>\n",
       "      <td>NULL</td>\n",
       "      <td>31.702724</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Cumulative (Day 3-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>16209-8</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8770794</td>\n",
       "      <td>m1252714</td>\n",
       "      <td>9/30/21 2:32 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>16.5879</td>\n",
       "      <td>18.74606</td>\n",
       "      <td>NULL</td>\n",
       "      <td>38.355477</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Cumulative (Day 3-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>16209-8</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8770794</td>\n",
       "      <td>m1252714</td>\n",
       "      <td>9/30/21 2:32 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0144</td>\n",
       "      <td>18.78522</td>\n",
       "      <td>NULL</td>\n",
       "      <td>31.702724</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>16209-8</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8770794</td>\n",
       "      <td>m1252714</td>\n",
       "      <td>9/30/21 2:32 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>16.5879</td>\n",
       "      <td>18.74606</td>\n",
       "      <td>NULL</td>\n",
       "      <td>38.355477</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Single</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1568 rows × 124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Product Purpose  experiment  run      Project_Name run_label  \\\n",
       "0     Zeex9ieJ      MF  F9FD5EC4C1   15  Zeex9ieJ for All  11408-15   \n",
       "1     Zeex9ieJ      MF  F9FD5EC4C1   15  Zeex9ieJ for All  11408-15   \n",
       "2     Zeex9ieJ      MF  F9FD5EC4C1   15  Zeex9ieJ for All  11408-15   \n",
       "3     Zeex9ieJ      MF  F9FD5EC4C1   15  Zeex9ieJ for All  11408-15   \n",
       "4     Zeex9ieJ      MF  F9FD5EC4C1   16  Zeex9ieJ for All  11409-16   \n",
       "...        ...     ...         ...  ...               ...       ...   \n",
       "1563  Zeex9ieJ      MF  B4EDDA67F0    8  Zeex9ieJ for All   16209-8   \n",
       "1564  Zeex9ieJ      MF  B4EDDA67F0    8  Zeex9ieJ for All   16209-8   \n",
       "1565  Zeex9ieJ      MF  B4EDDA67F0    8  Zeex9ieJ for All   16209-8   \n",
       "1566  Zeex9ieJ      MF  B4EDDA67F0    8  Zeex9ieJ for All   16209-8   \n",
       "1567  Zeex9ieJ      MF  B4EDDA67F0    8  Zeex9ieJ for All   16209-8   \n",
       "\n",
       "          Strain  strain_key Feedstock_Parent1       Start_Time  ...  \\\n",
       "0     F9FD5EC4C1     9215897          m1098919  1/20/21 2:25 PM  ...   \n",
       "1     F9FD5EC4C1     9215897          m1098919  1/20/21 2:25 PM  ...   \n",
       "2     F9FD5EC4C1     9215897          m1098919  1/20/21 2:25 PM  ...   \n",
       "3     F9FD5EC4C1     9215897          m1098919  1/20/21 2:25 PM  ...   \n",
       "4     F9FD5EC4C1     9215897          m1098919  1/20/21 2:24 PM  ...   \n",
       "...          ...         ...               ...              ...  ...   \n",
       "1563  B4EDDA67F0     8770794          m1252714  9/30/21 2:32 PM  ...   \n",
       "1564  B4EDDA67F0     8770794          m1252714  9/30/21 2:32 PM  ...   \n",
       "1565  B4EDDA67F0     8770794          m1252714  9/30/21 2:32 PM  ...   \n",
       "1566  B4EDDA67F0     8770794          m1252714  9/30/21 2:32 PM  ...   \n",
       "1567  B4EDDA67F0     8770794          m1252714  9/30/21 2:32 PM  ...   \n",
       "\n",
       "      Cap_Oil_Em_End__percent  Cap_PCV_End__percent  \\\n",
       "0                    16.32145              21.04034   \n",
       "1                    16.32145              21.04034   \n",
       "2                    16.32145              21.04034   \n",
       "3                    16.32145              21.04034   \n",
       "4                    15.21805              20.34157   \n",
       "...                       ...                   ...   \n",
       "1563                 19.50613              22.02205   \n",
       "1564                  16.0144              18.78522   \n",
       "1565                  16.5879              18.74606   \n",
       "1566                  16.0144              18.78522   \n",
       "1567                  16.5879              18.74606   \n",
       "\n",
       "      Cap_Dead_Cell_Layer_End__percent Zeex9ieJAlt_end__g_L  \\\n",
       "0                                 NULL             7.455144   \n",
       "1                                 NULL             7.455144   \n",
       "2                                 NULL             7.455144   \n",
       "3                                 NULL             7.455144   \n",
       "4                                 NULL             4.546885   \n",
       "...                                ...                  ...   \n",
       "1563                              NULL            17.845346   \n",
       "1564                              NULL            31.702724   \n",
       "1565                              NULL            38.355477   \n",
       "1566                              NULL            31.702724   \n",
       "1567                              NULL            38.355477   \n",
       "\n",
       "     Zeex9ieJ_mAU_sec_end__area Zeex9ieJ_Screening_end__g_L Zeex9ieJ_end__g_L  \\\n",
       "0                        1.3341                    9.923562          0.606429   \n",
       "1                        1.3341                    9.923562          0.606429   \n",
       "2                        1.3341                    9.923562          0.606429   \n",
       "3                        1.3341                    9.923562          0.606429   \n",
       "4                      2.179946                     7.52725          0.342935   \n",
       "...                         ...                         ...               ...   \n",
       "1563                          0                           0                 0   \n",
       "1564                          0                           0                 0   \n",
       "1565                          0                           0                 0   \n",
       "1566                          0                           0                 0   \n",
       "1567                          0                           0                 0   \n",
       "\n",
       "     Zeex9ieJ_end__g_L_121 Zeex9ieJ_end__g_L_122        interval_type  \n",
       "0                 0.675183                     0           Cumulative  \n",
       "1                 0.675183                     0              Curated  \n",
       "2                 0.675183                     0  Cumulative (Day 3-)  \n",
       "3                 0.675183                     0               Single  \n",
       "4                 0.455659              1.684056           Cumulative  \n",
       "...                    ...                   ...                  ...  \n",
       "1563                     0                  NULL               Single  \n",
       "1564                     0                  NULL  Cumulative (Day 3-)  \n",
       "1565                     0                  NULL  Cumulative (Day 3-)  \n",
       "1566                     0                  NULL               Single  \n",
       "1567                     0                  NULL               Single  \n",
       "\n",
       "[1568 rows x 124 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "SELECT *\n",
    "FROM `amyris.amyris_fermentation_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r7286b0f3cf393f52_000001744bd1a546_1 ... (1s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table amyris.training \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `amyris.amyris_fermentation_data` AS training \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(training))), 10) IN (1, 2, 3, 4)' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contains 1568 samples and 124 variables\n",
      "There are 20 unique strains which are replicated or measured under different fermentation conditions.\n",
      "The variables are comprised of a variety of fermentation process (meta data), physiological and biochemical parameters (independent):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "independent    63\n",
       "metadata       53\n",
       "dependent       5\n",
       "category        3\n",
       "Name: variable type, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "include: ['Run_Execution' 'Run_Performance' 'Product_Produced__g'\n",
      " 'Titer_End__g_over_kg']\n"
     ]
    }
   ],
   "source": [
    "meta_data.head()\n",
    "\n",
    "print('The data contains {} samples and {} variables'.format(data.shape[0],data.shape[1]))\n",
    "print('There are {} unique strains which are replicated or measured under different fermentation conditions.'.format(data['Strain'].nunique()))\n",
    "print('The variables are comprised of a variety of fermentation process (meta data), physiological and biochemical parameters (independent):')\n",
    "display(meta_data['variable type'].value_counts())\n",
    "print('include: {}'.format(meta_data.query('target == 1').name.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run_Performance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Run_Performance\n",
       "0           delta\n",
       "1           delta\n",
       "2           delta\n",
       "3           delta\n",
       "4           delta"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare data for analysis\n",
    "#split out numeric from categorical varibles\n",
    "numeric_vars = ((data.dtypes == 'float64') | (data.dtypes == 'int64')) & (meta_data['variable type'] == 'independent').values\n",
    "numeric_x_data = data[data.columns[numeric_vars]]\n",
    "\n",
    "#things to try to predict\n",
    "model_target = 'Run_Performance'\n",
    "y_data = data[[model_target]] \n",
    "#data[data.columns[(meta_data['target'] == 1).values]]\n",
    "y_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta data about variables\n",
    "meta_data = meta_data.set_index('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = numeric_x_data[:1400]\n",
    "# y_train = y_data[:1400]\n",
    "# X_validate = numeric_x_data[1400:]\n",
    "# y_validate = y_data[1400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(numeric_x_data, y_data, test_size=0.25, stratify = y_data[model_target], random_state=42)\n",
    "\n",
    "#split train set to create a pseudo test or validation dataset\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=0.33, stratify= y_train[model_target], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Viable_Cell_Density__g_over_L</th>\n",
       "      <th>Product_Flux_Viable_Cells__mmol_over_gDCW_over_h</th>\n",
       "      <th>Growth_Rate_Viable_Cells__1_over_h</th>\n",
       "      <th>Death_Rate_Avg__1_over_h</th>\n",
       "      <th>Death_Rate_Avg_Viable_Cells__1_over_h</th>\n",
       "      <th>TRS_Flux_Viable_Cells__mmol_over_gDCW_over_h</th>\n",
       "      <th>TRS_Flux__mmol_over_gDCW_over_h</th>\n",
       "      <th>TRS_for_Other_Viable_Cells__mmol_over_gDCW_over_h</th>\n",
       "      <th>sOUR_Viable_Cells__mmol_over_gDCW_over_h</th>\n",
       "      <th>TRS_to_Maintenance_Flux__mmol_over_gDCW_over_h</th>\n",
       "      <th>...</th>\n",
       "      <th>Cap_Feedstock_Solids_End__percent</th>\n",
       "      <th>Cap_Heavy_Em_End__percent</th>\n",
       "      <th>Cap_Light_Em_End__percent</th>\n",
       "      <th>Cap_Oil_Em_End__percent</th>\n",
       "      <th>Cap_PCV_End__percent</th>\n",
       "      <th>Cap_Dead_Cell_Layer_End__percent</th>\n",
       "      <th>Zeex9ieJAlt_end__g_L</th>\n",
       "      <th>Zeex9ieJ_mAU_sec_end__area</th>\n",
       "      <th>Zeex9ieJ_Screening_end__g_L</th>\n",
       "      <th>Run_Performance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.022618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.580472</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.59890</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>31.23719</td>\n",
       "      <td>14.02679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.279656</td>\n",
       "      <td>711.209220</td>\n",
       "      <td>10.704900</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>42.640952</td>\n",
       "      <td>0.019322</td>\n",
       "      <td>0.011842</td>\n",
       "      <td>0.002537</td>\n",
       "      <td>0.003682</td>\n",
       "      <td>0.721315</td>\n",
       "      <td>0.497036</td>\n",
       "      <td>0.486782</td>\n",
       "      <td>2.087218</td>\n",
       "      <td>0.335427</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.21529</td>\n",
       "      <td>1.63002</td>\n",
       "      <td>17.48424</td>\n",
       "      <td>21.26711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.811126</td>\n",
       "      <td>1.287626</td>\n",
       "      <td>15.784800</td>\n",
       "      <td>gamma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.693218</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.955499</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.64506</td>\n",
       "      <td>12.48656</td>\n",
       "      <td>16.15727</td>\n",
       "      <td>19.72047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.304696</td>\n",
       "      <td>1430.264616</td>\n",
       "      <td>20.768533</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.980806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.690178</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.48696</td>\n",
       "      <td>3.48728</td>\n",
       "      <td>27.08137</td>\n",
       "      <td>11.37292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.430124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.538637</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.861936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.072883</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.98190</td>\n",
       "      <td>2.04432</td>\n",
       "      <td>19.19476</td>\n",
       "      <td>20.20911</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.018522</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.216795</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Viable_Cell_Density__g_over_L  \\\n",
       "1344                            NaN   \n",
       "100                       42.640952   \n",
       "1206                            NaN   \n",
       "1004                            NaN   \n",
       "847                             NaN   \n",
       "\n",
       "      Product_Flux_Viable_Cells__mmol_over_gDCW_over_h  \\\n",
       "1344                                               NaN   \n",
       "100                                           0.019322   \n",
       "1206                                               NaN   \n",
       "1004                                               NaN   \n",
       "847                                                NaN   \n",
       "\n",
       "      Growth_Rate_Viable_Cells__1_over_h  Death_Rate_Avg__1_over_h  \\\n",
       "1344                                 NaN                       NaN   \n",
       "100                             0.011842                  0.002537   \n",
       "1206                                 NaN                       NaN   \n",
       "1004                                 NaN                       NaN   \n",
       "847                                  NaN                       NaN   \n",
       "\n",
       "      Death_Rate_Avg_Viable_Cells__1_over_h  \\\n",
       "1344                                    NaN   \n",
       "100                                0.003682   \n",
       "1206                                    NaN   \n",
       "1004                                    NaN   \n",
       "847                                     NaN   \n",
       "\n",
       "      TRS_Flux_Viable_Cells__mmol_over_gDCW_over_h  \\\n",
       "1344                                           NaN   \n",
       "100                                       0.721315   \n",
       "1206                                           NaN   \n",
       "1004                                           NaN   \n",
       "847                                            NaN   \n",
       "\n",
       "      TRS_Flux__mmol_over_gDCW_over_h  \\\n",
       "1344                         1.022618   \n",
       "100                          0.497036   \n",
       "1206                         1.693218   \n",
       "1004                         0.980806   \n",
       "847                          1.861936   \n",
       "\n",
       "      TRS_for_Other_Viable_Cells__mmol_over_gDCW_over_h  \\\n",
       "1344                                                NaN   \n",
       "100                                            0.486782   \n",
       "1206                                                NaN   \n",
       "1004                                                NaN   \n",
       "847                                                 NaN   \n",
       "\n",
       "      sOUR_Viable_Cells__mmol_over_gDCW_over_h  \\\n",
       "1344                                       NaN   \n",
       "100                                   2.087218   \n",
       "1206                                       NaN   \n",
       "1004                                       NaN   \n",
       "847                                        NaN   \n",
       "\n",
       "      TRS_to_Maintenance_Flux__mmol_over_gDCW_over_h  ...  \\\n",
       "1344                                        0.580472  ...   \n",
       "100                                         0.335427  ...   \n",
       "1206                                        0.955499  ...   \n",
       "1004                                        0.690178  ...   \n",
       "847                                         1.072883  ...   \n",
       "\n",
       "      Cap_Feedstock_Solids_End__percent  Cap_Heavy_Em_End__percent  \\\n",
       "1344                                NaN                    0.59890   \n",
       "100                                 NaN                    0.21529   \n",
       "1206                                NaN                    0.64506   \n",
       "1004                                NaN                    0.48696   \n",
       "847                                 NaN                    1.98190   \n",
       "\n",
       "      Cap_Light_Em_End__percent  Cap_Oil_Em_End__percent  \\\n",
       "1344                    0.00000                 31.23719   \n",
       "100                     1.63002                 17.48424   \n",
       "1206                   12.48656                 16.15727   \n",
       "1004                    3.48728                 27.08137   \n",
       "847                     2.04432                 19.19476   \n",
       "\n",
       "      Cap_PCV_End__percent  Cap_Dead_Cell_Layer_End__percent  \\\n",
       "1344              14.02679                               NaN   \n",
       "100               21.26711                               NaN   \n",
       "1206              19.72047                               NaN   \n",
       "1004              11.37292                               NaN   \n",
       "847               20.20911                               NaN   \n",
       "\n",
       "      Zeex9ieJAlt_end__g_L  Zeex9ieJ_mAU_sec_end__area  \\\n",
       "1344              4.279656                  711.209220   \n",
       "100              12.811126                    1.287626   \n",
       "1206             12.304696                 1430.264616   \n",
       "1004             15.430124                         NaN   \n",
       "847              19.018522                         NaN   \n",
       "\n",
       "      Zeex9ieJ_Screening_end__g_L  Run_Performance  \n",
       "1344                    10.704900            delta  \n",
       "100                     15.784800            gamma  \n",
       "1206                    20.768533            delta  \n",
       "1004                     9.538637            delta  \n",
       "847                     24.216795            delta  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xy_test = pd.concat([X_test, y_test])\n",
    "Xy_test =X_test.join(y_test)\n",
    "Xy_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Viable_Cell_Density__g_over_L</th>\n",
       "      <th>Product_Flux_Viable_Cells__mmol_over_gDCW_over_h</th>\n",
       "      <th>Growth_Rate_Viable_Cells__1_over_h</th>\n",
       "      <th>Death_Rate_Avg__1_over_h</th>\n",
       "      <th>Death_Rate_Avg_Viable_Cells__1_over_h</th>\n",
       "      <th>TRS_Flux_Viable_Cells__mmol_over_gDCW_over_h</th>\n",
       "      <th>TRS_Flux__mmol_over_gDCW_over_h</th>\n",
       "      <th>TRS_for_Other_Viable_Cells__mmol_over_gDCW_over_h</th>\n",
       "      <th>sOUR_Viable_Cells__mmol_over_gDCW_over_h</th>\n",
       "      <th>TRS_to_Maintenance_Flux__mmol_over_gDCW_over_h</th>\n",
       "      <th>...</th>\n",
       "      <th>Cap_Feedstock_Solids_End__percent</th>\n",
       "      <th>Cap_Heavy_Em_End__percent</th>\n",
       "      <th>Cap_Light_Em_End__percent</th>\n",
       "      <th>Cap_Oil_Em_End__percent</th>\n",
       "      <th>Cap_PCV_End__percent</th>\n",
       "      <th>Cap_Dead_Cell_Layer_End__percent</th>\n",
       "      <th>Zeex9ieJAlt_end__g_L</th>\n",
       "      <th>Zeex9ieJ_mAU_sec_end__area</th>\n",
       "      <th>Zeex9ieJ_Screening_end__g_L</th>\n",
       "      <th>Run_Performance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.022618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.580472</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.59890</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>31.23719</td>\n",
       "      <td>14.02679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.279656</td>\n",
       "      <td>711.209220</td>\n",
       "      <td>10.704900</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42.640952</td>\n",
       "      <td>0.019322</td>\n",
       "      <td>0.011842</td>\n",
       "      <td>0.002537</td>\n",
       "      <td>0.003682</td>\n",
       "      <td>0.721315</td>\n",
       "      <td>0.497036</td>\n",
       "      <td>0.486782</td>\n",
       "      <td>2.087218</td>\n",
       "      <td>0.335427</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.21529</td>\n",
       "      <td>1.63002</td>\n",
       "      <td>17.48424</td>\n",
       "      <td>21.26711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.811126</td>\n",
       "      <td>1.287626</td>\n",
       "      <td>15.784800</td>\n",
       "      <td>gamma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.693218</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.955499</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.64506</td>\n",
       "      <td>12.48656</td>\n",
       "      <td>16.15727</td>\n",
       "      <td>19.72047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.304696</td>\n",
       "      <td>1430.264616</td>\n",
       "      <td>20.768533</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.980806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.690178</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.48696</td>\n",
       "      <td>3.48728</td>\n",
       "      <td>27.08137</td>\n",
       "      <td>11.37292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.430124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.538637</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.861936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.072883</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.98190</td>\n",
       "      <td>2.04432</td>\n",
       "      <td>19.19476</td>\n",
       "      <td>20.20911</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.018522</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.216795</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Viable_Cell_Density__g_over_L  \\\n",
       "0                            NaN   \n",
       "1                      42.640952   \n",
       "2                            NaN   \n",
       "3                            NaN   \n",
       "4                            NaN   \n",
       "\n",
       "   Product_Flux_Viable_Cells__mmol_over_gDCW_over_h  \\\n",
       "0                                               NaN   \n",
       "1                                          0.019322   \n",
       "2                                               NaN   \n",
       "3                                               NaN   \n",
       "4                                               NaN   \n",
       "\n",
       "   Growth_Rate_Viable_Cells__1_over_h  Death_Rate_Avg__1_over_h  \\\n",
       "0                                 NaN                       NaN   \n",
       "1                            0.011842                  0.002537   \n",
       "2                                 NaN                       NaN   \n",
       "3                                 NaN                       NaN   \n",
       "4                                 NaN                       NaN   \n",
       "\n",
       "   Death_Rate_Avg_Viable_Cells__1_over_h  \\\n",
       "0                                    NaN   \n",
       "1                               0.003682   \n",
       "2                                    NaN   \n",
       "3                                    NaN   \n",
       "4                                    NaN   \n",
       "\n",
       "   TRS_Flux_Viable_Cells__mmol_over_gDCW_over_h  \\\n",
       "0                                           NaN   \n",
       "1                                      0.721315   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "\n",
       "   TRS_Flux__mmol_over_gDCW_over_h  \\\n",
       "0                         1.022618   \n",
       "1                         0.497036   \n",
       "2                         1.693218   \n",
       "3                         0.980806   \n",
       "4                         1.861936   \n",
       "\n",
       "   TRS_for_Other_Viable_Cells__mmol_over_gDCW_over_h  \\\n",
       "0                                                NaN   \n",
       "1                                           0.486782   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "   sOUR_Viable_Cells__mmol_over_gDCW_over_h  \\\n",
       "0                                       NaN   \n",
       "1                                  2.087218   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                       NaN   \n",
       "\n",
       "   TRS_to_Maintenance_Flux__mmol_over_gDCW_over_h  ...  \\\n",
       "0                                        0.580472  ...   \n",
       "1                                        0.335427  ...   \n",
       "2                                        0.955499  ...   \n",
       "3                                        0.690178  ...   \n",
       "4                                        1.072883  ...   \n",
       "\n",
       "   Cap_Feedstock_Solids_End__percent  Cap_Heavy_Em_End__percent  \\\n",
       "0                                NaN                    0.59890   \n",
       "1                                NaN                    0.21529   \n",
       "2                                NaN                    0.64506   \n",
       "3                                NaN                    0.48696   \n",
       "4                                NaN                    1.98190   \n",
       "\n",
       "   Cap_Light_Em_End__percent  Cap_Oil_Em_End__percent  Cap_PCV_End__percent  \\\n",
       "0                    0.00000                 31.23719              14.02679   \n",
       "1                    1.63002                 17.48424              21.26711   \n",
       "2                   12.48656                 16.15727              19.72047   \n",
       "3                    3.48728                 27.08137              11.37292   \n",
       "4                    2.04432                 19.19476              20.20911   \n",
       "\n",
       "   Cap_Dead_Cell_Layer_End__percent  Zeex9ieJAlt_end__g_L  \\\n",
       "0                               NaN              4.279656   \n",
       "1                               NaN             12.811126   \n",
       "2                               NaN             12.304696   \n",
       "3                               NaN             15.430124   \n",
       "4                               NaN             19.018522   \n",
       "\n",
       "   Zeex9ieJ_mAU_sec_end__area  Zeex9ieJ_Screening_end__g_L  Run_Performance  \n",
       "0                  711.209220                    10.704900            delta  \n",
       "1                    1.287626                    15.784800            gamma  \n",
       "2                 1430.264616                    20.768533            delta  \n",
       "3                         NaN                     9.538637            delta  \n",
       "4                         NaN                    24.216795            delta  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_test.to_csv('test_amyris.csv', index=False)\n",
    "new_df = pd.read_csv('test_amyris.csv')\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute missing with median\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "#auto scale\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pipe = Pipeline([('imputer',imputer),\n",
    "                 ('scaler', scaler)\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for modeling\n",
    "#use the pipeline created above\n",
    "\n",
    "_X_train = pipe.fit_transform(X_train)\n",
    "_y_train = y_train[model_target]\n",
    "_X_test = pipe.fit_transform(X_validate)\n",
    "_y_test = y_validate[model_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weights (1,0): [0.74105461 1.53710938]\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=['delta' 'gamma'], y=957     delta\n",
      "1203    delta\n",
      "1218    delta\n",
      "574     gamma\n",
      "232     delta\n",
      "        ...  \n",
      "492     gamma\n",
      "1491    delta\n",
      "1501    delta\n",
      "1180    delta\n",
      "1336    delta\n",
      "Name: Run_Performance, Length: 787, dtype: object as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   14.2s finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Simple hyperparameter tunnig for random forest model\n",
    "estimator = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "# tune grid\n",
    "param_grid = {\n",
    "    'n_estimators': np.linspace(10, 200).astype(int),\n",
    "    'max_depth': [None] + list(np.linspace(3, 20).astype(int)),\n",
    "    'max_features': ['auto', 'sqrt', None] + list(np.arange(0.5, 1, 0.1)),\n",
    "    'max_leaf_nodes': [None] + list(np.linspace(10, 50, 500).astype(int)),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'bootstrap': [True, False],\n",
    "    'class_weight' : [\"balanced\", \"balanced_subsample\"] # RF classifier tends to be biased towards the majority class, place a heavier penalty on misclassifying the minority class\n",
    "}\n",
    "\n",
    "print('class weights (1,0): {}'.format(compute_class_weight('balanced', np.unique(_y_train), _y_train)) )\n",
    "\n",
    "\n",
    "\n",
    "# Create the random search model\n",
    "rs = RandomizedSearchCV(estimator, param_grid, n_jobs = -1, \n",
    "                        scoring = 'roc_auc', cv = 3, \n",
    "                        n_iter = 10, verbose = 1, random_state=42)\n",
    "\n",
    "# Fit \n",
    "rs.fit(_X_train, _y_train)\n",
    "pca_result = pipe.fit(X_train, y_train.values.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of classifier on test set: 0.95\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       delta       0.97      0.95      0.96       262\n",
      "       gamma       0.90      0.94      0.92       127\n",
      "\n",
      "    accuracy                           0.95       389\n",
      "   macro avg       0.94      0.95      0.94       389\n",
      "weighted avg       0.95      0.95      0.95       389\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_X_train = pipe.fit_transform(X_train)\n",
    "_y_train = y_train[model_target]\n",
    "_X_test = pipe.fit_transform(X_validate)\n",
    "_y_test = y_validate[model_target]\n",
    "\n",
    "# Simple hyperparameter tunnig for random forest model\n",
    "estimator = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "estimator.fit(_X_train, _y_train)\n",
    "\n",
    "rf_predictions = estimator.predict(_X_test)\n",
    "print('Accuracy of classifier on test set: {:.2f}'.format(estimator.score(_X_test, _y_test).round(2)))\n",
    "print(classification_report(_y_test, rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select best model\n",
    "best_model = rs.best_estimator_\n",
    "\n",
    "train_rf_predictions = best_model.predict(_X_train)\n",
    "train_rf_probs = best_model.predict_proba(_X_train)[:, 1]\n",
    "\n",
    "rf_predictions = best_model.predict(_X_test)\n",
    "rf_probs = best_model.predict_proba(_X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of classifier on validation set: 0.86\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       delta       0.97      0.81      0.88       262\n",
      "       gamma       0.71      0.94      0.81       127\n",
      "\n",
      "    accuracy                           0.86       389\n",
      "   macro avg       0.84      0.88      0.85       389\n",
      "weighted avg       0.88      0.86      0.86       389\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f0c87b7a050>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#evaluate performance on validation data\n",
    "\n",
    "\n",
    "print('Accuracy of classifier on validation set: {:.2f}'.format(best_model.score(_X_test, _y_test).round(2)))\n",
    "print(classification_report(_y_test, rf_predictions))\n",
    "plot_confusion_matrix(best_model, _X_test, _y_test,\n",
    "                                 display_labels=['no','yes'],\n",
    "                                 cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## steps to create an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE_FOLDER = 'train_image'\n",
    "os.makedirs(TRAIN_IMAGE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_image/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAIN_IMAGE_FOLDER}/train.py\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, n_estimators, max_leaf_nodes, max_depth, min_samples_split, max_features, \n",
    "                   class_weight, bootstrap, hptune):\n",
    "    data = pd.read_excel(trainin g_dataset_path,sheet_name='data')\n",
    "    meta_data = pd.read_excel(training_dataset_path, sheet_name='meta data')\n",
    "    \n",
    "    numeric_vars = ((data.dtypes == 'float64') | (data.dtypes == 'int64')) & (meta_data['variable type'] == 'independent').values\n",
    "    numeric_x_data = data[data.columns[numeric_vars]]\n",
    "\n",
    "    model_target = 'Run_Performance'\n",
    "    y_data = data[[model_target]] \n",
    "    meta_data = meta_data.set_index('name')\n",
    " \n",
    "\n",
    "    #maintain class balance\n",
    "    X_train, X_test, y_train, y_test = train_test_split(numeric_x_data, y_data, test_size=0.25, stratify = y_data[model_target], random_state=42)\n",
    "\n",
    "    #split train set to create a pseudo test or validation dataset\n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=0.33, stratify= y_train[model_target], random_state=42)\n",
    "    \n",
    "    if not hptune:\n",
    "        X_train = pd.concat([X_train, X_validate])\n",
    "        y_train = pd.concat([y_train, y_validate])\n",
    "\n",
    "    #impute missing with median\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    #auto scale\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "     # Simple hyperparameter tunnig for random forest model\n",
    "    estimator = RandomForestClassifier(random_state = 42)\n",
    "    \n",
    "     \n",
    "    pipe = Pipeline([('imputer',imputer),\n",
    "                     ('scaler', scaler),\n",
    "                     ('rfclassifier', estimator)\n",
    "                    ])\n",
    "    \n",
    "    \n",
    "    pipe.set_params(rfclassifier__n_estimators=n_estimators, rfclassifier__max_leaf_nodes=max_leaf_nodes, rfclassifier__max_depth=max_depth,\n",
    "                       rfclassifier__min_samples_split=min_samples_split, rfclassifier__max_features=max_features, \n",
    "                       rfclassifier__class_weight=class_weight, rfclassifier__bootstrap=bootstrap )\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "#     _X_train = pipe.fit_transform(X_train)\n",
    "#     _y_train = y_train[model_target]\n",
    "#     _X_test = pipe.fit_transform(X_test)\n",
    "#     _y_test = y_test[model_target]\n",
    "    \n",
    "      \n",
    "#     estimator.fit(_X_train, _y_train) \n",
    "#     rf_predictions = estimator.predict(_X_test)\n",
    "#     print('Accuracy of classifier on test set: {:.2f}'.format(estimator.score(_X_test, _y_test).round(2)))\n",
    "#     print(classification_report(_y_test, rf_predictions))\n",
    "       \n",
    "        \n",
    "####################################3 \n",
    "#         # tune grid\n",
    "#     param_grid = {\n",
    "#         'n_estimators': np.linspace(10, 200).astype(int),\n",
    "#         'max_depth': [None] + list(np.linspace(3, 20).astype(int)),\n",
    "#         'max_features': ['auto', 'sqrt', None] + list(np.arange(0.5, 1, 0.1)),\n",
    "#         'max_leaf_nodes': [None] + list(np.linspace(10, 50, 500).astype(int)),\n",
    "#         'min_samples_split': [2, 5, 10],\n",
    "#         'bootstrap': [True, False],\n",
    "#         'class_weight' : [\"balanced\", \"balanced_subsample\"] # RF classifier tends to be biased towards the majority class, place a heavier penalty on misclassifying the minority class\n",
    "#     }\n",
    "\n",
    "#     print('class weights (1,0): {}'.format(compute_class_weight('balanced', np.unique(_y_train), _y_train)) )\n",
    "\n",
    "#     # Create the random search model\n",
    "#     rs = RandomizedSearchCV(estimator, param_grid, n_jobs = -1, \n",
    "#                             scoring = 'roc_auc', cv = 3, \n",
    "#                             n_iter = 10, verbose = 1, random_state=42)\n",
    "\n",
    "#     # Fit \n",
    "#     rs.fit(_X_train, _y_train)\n",
    "\n",
    "#     #select best model\n",
    "#     best_model = rs.best_estimator_\n",
    "\n",
    "#     train_rf_predictions = best_model.predict(_X_train)\n",
    "#     train_rf_probs = best_model.predict_proba(_X_train)[:, 1]\n",
    "\n",
    "#     rf_predictions = best_model.predict(_X_test)\n",
    "#     rf_probs = best_model.predict_proba(_X_test)[:, 1]\n",
    "# #     evaluate performance on validation data\n",
    "#     print('Accuracy of classifier on validation set: {:.2f}'.format(best_model.score(_X_test, _y_test).round(2)))\n",
    "#     print(classification_report(_y_test, rf_predictions))\n",
    "\n",
    "#########################################################3\n",
    "\n",
    "    if hptune:\n",
    "#         _X_validate = pipe.fit_transform(X_validate)\n",
    "#         _y_validate = y_validate[model_target]\n",
    "        accuracy = pipe.score(X_validate, y_validate)\n",
    "        print('Model accuracy: {}'.format(accuracy))\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "          hyperparameter_metric_tag='accuracy',\n",
    "          metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        model_filename = 'model.pkl'\n",
    "        with open(model_filename, 'wb') as model_file:\n",
    "            pickle.dump(pipe, model_file)\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "        subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_image/hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAIN_IMAGE_FOLDER}/hptuning_config.yaml\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 4\n",
    "    maxParallelTrials: 4\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    enableTrialEarlyStopping: TRUE \n",
    "    algorithm: RANDOM_SEARCH\n",
    "    params:\n",
    "    - parameterName: n_estimators\n",
    "      type: INTEGER\n",
    "      minValue:  10\n",
    "      maxValue:  200\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: max_leaf_nodes\n",
    "      type: INTEGER\n",
    "      minValue:  10\n",
    "      maxValue:  500\n",
    "      scaleType: UNIT_LINEAR_SCALE    \n",
    "    - parameterName: max_depth\n",
    "      type: INTEGER\n",
    "      minValue:  3\n",
    "      maxValue:  20\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: min_samples_split\n",
    "      type: DISCRETE\n",
    "      discreteValues: [\n",
    "          2,\n",
    "          5,\n",
    "          10\n",
    "          ]\n",
    "    - parameterName: max_features\n",
    "      type: DOUBLE\n",
    "      minValue: 0.5\n",
    "      maxValue: 1.0\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: class_weight\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\n",
    "          \"balanced\",\n",
    "          \"balanced_subsample\"\n",
    "      ]\n",
    "    - parameterName: bootstrap\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\n",
    "          \"TRUE\",\n",
    "          \"FALSE\"\n",
    "      ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## package script into a docker image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAIN_IMAGE_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# buidl the docker image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='trainer_image'\n",
    "IMAGE_TAG='latest'\n",
    "TRAINER_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 3 file(s) totalling 7.6 KiB before compression.\n",
      "Uploading tarball of [train_image] to [gs://benazirsproject_cloudbuild/source/1599349075.61-ff58cc9bfa0a4e57a151c321486e9e7f.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/benazirsproject/builds/009e8d35-05cc-4ad2-bd30-c7b2d28399ab].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/009e8d35-05cc-4ad2-bd30-c7b2d28399ab?project=981930454113].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"009e8d35-05cc-4ad2-bd30-c7b2d28399ab\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://benazirsproject_cloudbuild/source/1599349075.61-ff58cc9bfa0a4e57a151c321486e9e7f.tgz#1599349076035661\n",
      "Copying gs://benazirsproject_cloudbuild/source/1599349075.61-ff58cc9bfa0a4e57a151c321486e9e7f.tgz#1599349076035661...\n",
      "/ [1 files][  2.8 KiB/  2.8 KiB]                                                \n",
      "Operation completed over 1 objects/2.8 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  11.26kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7c3167c320d: Pulling fs layer\n",
      "131f805ec7fd: Pulling fs layer\n",
      "322ed380e680: Pulling fs layer\n",
      "6ac240b13098: Pulling fs layer\n",
      "9ce3a9266402: Pulling fs layer\n",
      "72c706dfac1d: Pulling fs layer\n",
      "6383427606e5: Pulling fs layer\n",
      "3e8b21666cec: Pulling fs layer\n",
      "358bb5d659ed: Pulling fs layer\n",
      "8ade7556a8f1: Pulling fs layer\n",
      "b2ebb7e1223e: Pulling fs layer\n",
      "8d5d283ad922: Pulling fs layer\n",
      "14c0fd48a5f3: Pulling fs layer\n",
      "ceaad5dc04d2: Pulling fs layer\n",
      "c1074350f761: Pulling fs layer\n",
      "687ad0b9a318: Pulling fs layer\n",
      "d2365d2ee19a: Pulling fs layer\n",
      "5095b04f1d98: Pulling fs layer\n",
      "6ac240b13098: Waiting\n",
      "9ce3a9266402: Waiting\n",
      "72c706dfac1d: Waiting\n",
      "6383427606e5: Waiting\n",
      "3e8b21666cec: Waiting\n",
      "358bb5d659ed: Waiting\n",
      "8ade7556a8f1: Waiting\n",
      "b2ebb7e1223e: Waiting\n",
      "8d5d283ad922: Waiting\n",
      "14c0fd48a5f3: Waiting\n",
      "ceaad5dc04d2: Waiting\n",
      "c1074350f761: Waiting\n",
      "687ad0b9a318: Waiting\n",
      "d2365d2ee19a: Waiting\n",
      "5095b04f1d98: Waiting\n",
      "131f805ec7fd: Verifying Checksum\n",
      "131f805ec7fd: Download complete\n",
      "322ed380e680: Verifying Checksum\n",
      "322ed380e680: Download complete\n",
      "6ac240b13098: Verifying Checksum\n",
      "6ac240b13098: Download complete\n",
      "d7c3167c320d: Verifying Checksum\n",
      "d7c3167c320d: Download complete\n",
      "6383427606e5: Verifying Checksum\n",
      "6383427606e5: Download complete\n",
      "72c706dfac1d: Verifying Checksum\n",
      "72c706dfac1d: Download complete\n",
      "358bb5d659ed: Verifying Checksum\n",
      "358bb5d659ed: Download complete\n",
      "8ade7556a8f1: Verifying Checksum\n",
      "8ade7556a8f1: Download complete\n",
      "3e8b21666cec: Verifying Checksum\n",
      "3e8b21666cec: Download complete\n",
      "b2ebb7e1223e: Verifying Checksum\n",
      "b2ebb7e1223e: Download complete\n",
      "8d5d283ad922: Verifying Checksum\n",
      "8d5d283ad922: Download complete\n",
      "14c0fd48a5f3: Verifying Checksum\n",
      "14c0fd48a5f3: Download complete\n",
      "ceaad5dc04d2: Verifying Checksum\n",
      "ceaad5dc04d2: Download complete\n",
      "c1074350f761: Verifying Checksum\n",
      "c1074350f761: Download complete\n",
      "687ad0b9a318: Verifying Checksum\n",
      "687ad0b9a318: Download complete\n",
      "5095b04f1d98: Verifying Checksum\n",
      "5095b04f1d98: Download complete\n",
      "9ce3a9266402: Verifying Checksum\n",
      "9ce3a9266402: Download complete\n",
      "d7c3167c320d: Pull complete\n",
      "131f805ec7fd: Pull complete\n",
      "322ed380e680: Pull complete\n",
      "6ac240b13098: Pull complete\n",
      "d2365d2ee19a: Verifying Checksum\n",
      "d2365d2ee19a: Download complete\n",
      "9ce3a9266402: Pull complete\n",
      "72c706dfac1d: Pull complete\n",
      "6383427606e5: Pull complete\n",
      "3e8b21666cec: Pull complete\n",
      "358bb5d659ed: Pull complete\n",
      "8ade7556a8f1: Pull complete\n",
      "b2ebb7e1223e: Pull complete\n",
      "8d5d283ad922: Pull complete\n",
      "14c0fd48a5f3: Pull complete\n",
      "ceaad5dc04d2: Pull complete\n",
      "c1074350f761: Pull complete\n",
      "687ad0b9a318: Pull complete\n",
      "d2365d2ee19a: Pull complete\n",
      "5095b04f1d98: Pull complete\n",
      "Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d8706668f140\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in 8806a89ba9c4\n",
      "Collecting fire\n",
      "  Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=1de40a88505c45faffb3f3bffae6867ee2ba545c63a3f8ae11f5c8b1583baedb\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3986 sha256=9b45f63ef3de6f1920120736155e5df53b09d9d3385d21fd1f929a27b8329bb0\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=0cd53f679705fe37aa62a1d23128c9e4b2073bb367b8e788660c8b71dcc1d741\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "\u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: termcolor, fire, cloudml-hypertune, scikit-learn, pandas\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.1\n",
      "    Uninstalling scikit-learn-0.23.1:\n",
      "      Successfully uninstalled scikit-learn-0.23.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.3.1 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "Removing intermediate container 8806a89ba9c4\n",
      " ---> 9ee561962bef\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 078da950b620\n",
      "Removing intermediate container 078da950b620\n",
      " ---> 8e6a6b72a43f\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> e0c76c565e64\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 44c69f40938a\n",
      "Removing intermediate container 44c69f40938a\n",
      " ---> 737f320ef340\n",
      "Successfully built 737f320ef340\n",
      "Successfully tagged gcr.io/benazirsproject/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/benazirsproject/trainer_image:latest\n",
      "The push refers to repository [gcr.io/benazirsproject/trainer_image]\n",
      "f8dd36d06b9f: Preparing\n",
      "c2e8dbc75cba: Preparing\n",
      "54636d1e50bf: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "222959643149: Waiting\n",
      "badaf1bc8335: Waiting\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "89212ed9ad75: Layer already exists\n",
      "c51fe61c6231: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "4c4a5579b7a8: Layer already exists\n",
      "c2e8dbc75cba: Pushed\n",
      "f8dd36d06b9f: Pushed\n",
      "5b5017461bc6: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "54636d1e50bf: Pushed\n",
      "latest: digest: sha256:e385309a8612b642004fc530145a152387369fd879590fb78a8da98b08dfc239 size: 4708\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                     IMAGES                                          STATUS\n",
      "009e8d35-05cc-4ad2-bd30-c7b2d28399ab  2020-09-05T23:37:56+00:00  3M9S      gs://benazirsproject_cloudbuild/source/1599349075.61-ff58cc9bfa0a4e57a151c321486e9e7f.tgz  gcr.io/benazirsproject/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $TRAINER_IMAGE $TRAIN_IMAGE_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create hyperparameter file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start hyper parameter tuning job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20200904_020441] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20200904_020441\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20200904_020441\n",
      "jobId: JOB_20200904_020441\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$TRAINER_IMAGE \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "--config $TRAIN_IMAGE_FOLDER/hptuning_config.yaml \\\n",
    "-- \\\n",
    "--training_dataset_path=$TRAINING_FILE_PATH \\\n",
    "--hptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobId': 'JOB_20200904_020441',\n",
       " 'trainingInput': {'args': ['--training_dataset_path=gs://benazirsproject-demo/data/training/Anonymized_Fermentation_Data_final.xlsx',\n",
       "   '--hptune'],\n",
       "  'hyperparameters': {'goal': 'MAXIMIZE',\n",
       "   'params': [{'parameterName': 'n_estimators',\n",
       "     'minValue': 10,\n",
       "     'maxValue': 200,\n",
       "     'type': 'INTEGER',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'max_depth',\n",
       "     'minValue': 3,\n",
       "     'maxValue': 20,\n",
       "     'type': 'INTEGER',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'min_samples_split',\n",
       "     'type': 'DISCRETE',\n",
       "     'discreteValues': [2, 5, 10]}],\n",
       "   'maxTrials': 4,\n",
       "   'maxParallelTrials': 4,\n",
       "   'hyperparameterMetricTag': 'accuracy',\n",
       "   'enableTrialEarlyStopping': True,\n",
       "   'algorithm': 'RANDOM_SEARCH'},\n",
       "  'region': 'us-central1',\n",
       "  'jobDir': 'gs://benazirsproject-demo/jobs/JOB_20200904_020441',\n",
       "  'masterConfig': {'imageUri': 'gcr.io/benazirsproject/amyris_trainer_image2:latest'}},\n",
       " 'createTime': '2020-09-04T02:04:43Z',\n",
       " 'startTime': '2020-09-04T02:04:44Z',\n",
       " 'endTime': '2020-09-04T02:14:22Z',\n",
       " 'state': 'SUCCEEDED',\n",
       " 'trainingOutput': {'completedTrialCount': '4',\n",
       "  'trials': [{'trialId': '2',\n",
       "    'hyperparameters': {'n_estimators': '131',\n",
       "     'min_samples_split': '2',\n",
       "     'max_depth': '20'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.9897172236503856},\n",
       "    'startTime': '2020-09-04T02:05:21.684116167Z',\n",
       "    'endTime': '2020-09-04T02:13:55Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '3',\n",
       "    'hyperparameters': {'min_samples_split': '2',\n",
       "     'max_depth': '13',\n",
       "     'n_estimators': '69'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.9845758354755784},\n",
       "    'startTime': '2020-09-04T02:05:21.684152763Z',\n",
       "    'endTime': '2020-09-04T02:14:03Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '1',\n",
       "    'hyperparameters': {'n_estimators': '94',\n",
       "     'max_depth': '15',\n",
       "     'min_samples_split': '2'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.9820051413881749},\n",
       "    'startTime': '2020-09-04T02:05:21.684004846Z',\n",
       "    'endTime': '2020-09-04T02:13:44Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '4',\n",
       "    'hyperparameters': {'min_samples_split': '10',\n",
       "     'n_estimators': '92',\n",
       "     'max_depth': '11'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.9717223650385605},\n",
       "    'startTime': '2020-09-04T02:05:21.684181394Z',\n",
       "    'endTime': '2020-09-04T02:13:53Z',\n",
       "    'state': 'SUCCEEDED'}],\n",
       "  'consumedMLUnits': 0.26,\n",
       "  'isHyperparameterTuningJob': True,\n",
       "  'hyperparameterMetricTag': 'accuracy'},\n",
       " 'etag': 'd9aitxH+Fuw='}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = discovery.build('ml', 'v1')\n",
    "\n",
    "job_id = 'projects/{}/jobs/{}'.format(PROJECT_ID, JOB_NAME)\n",
    "request = ml.projects().jobs().get(name=job_id)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "except errors.HttpError as err:\n",
    "    print(err)\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "    \n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trialId': '2',\n",
       " 'hyperparameters': {'n_estimators': '131',\n",
       "  'min_samples_split': '2',\n",
       "  'max_depth': '20'},\n",
       " 'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.9897172236503856},\n",
       " 'startTime': '2020-09-04T02:05:21.684116167Z',\n",
       " 'endTime': '2020-09-04T02:13:55Z',\n",
       " 'state': 'SUCCEEDED'}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['trainingOutput']['trials'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = response['trainingOutput']['trials'][0]['hyperparameters']['max_depth']\n",
    "n_estimators = response['trainingOutput']['trials'][0]['hyperparameters']['n_estimators']\n",
    "max_leaf_nodes = response['trainingOutput']['trials'][0]['hyperparameters']['max_leaf_nodes']\n",
    "min_samples_split = response['trainingOutput']['trials'][0]['hyperparameters']['min_samples_split']\n",
    "max_features = response['trainingOutput']['trials'][0]['hyperparameters']['max_features']\n",
    "class_weight = response['trainingOutput']['trials'][0]['hyperparameters']['class_weight']\n",
    "bootstrap = response['trainingOutput']['trials'][0]['hyperparameters']['bootstrap']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20200904_021933] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20200904_021933\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20200904_021933\n",
      "jobId: JOB_20200904_021933\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$TRAINER_IMAGE \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "-- \\\n",
    "--training_dataset_path=$TRAINING_FILE_PATH \\\n",
    "--max_depth=$max_depth \\\n",
    "--n_estimators=$n_estimators \\\n",
    "--max_leaf_nodes=$max_leaf_nodes \\\n",
    "--min_samples_split=$min_samples_split \\\n",
    "--max_features=$max_features \\\n",
    "--class_weight=$class_weight \\\n",
    "--bootstrap=$bootstrap \\\n",
    "--nohptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: amyris_endtoendRF2mp already exists.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'amyris_endtoendRF2mp'\n",
    "labels = \"task=classifier,domain=healthcare\"\n",
    "filter = 'name:{}'.format(model_name)\n",
    "models = !(gcloud ai-platform models list --filter={filter} --format='value(name)')\n",
    "\n",
    "if not models:\n",
    "    !gcloud ai-platform models create  $model_name \\\n",
    "        --regions=$REGION \\\n",
    "        --labels=$labels\n",
    "else:\n",
    "    print(\"Model: {} already exists.\".format(models[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model version: v01 already exists.\n"
     ]
    }
   ],
   "source": [
    "model_version = 'v01'\n",
    "filter = 'name:{}'.format(model_version)\n",
    "versions = !(gcloud ai-platform versions list --model={model_name} --format='value(name)' --filter={filter})\n",
    "\n",
    "if not versions:\n",
    "    !gcloud ai-platform versions create {model_version} \\\n",
    "        --model={model_name} \\\n",
    "        --origin=$JOB_DIR \\\n",
    "        --runtime-version=1.15 \\\n",
    "        --framework=scikit-learn \\\n",
    "        --python-version=3.7\n",
    "else:\n",
    "    print(\"Model version: {} already exists.\".format(versions[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'serving_instances.json'\n",
    "\n",
    "with open(input_file, 'w') as f:\n",
    "    for index, row in X_validate.head().iterrows():\n",
    "        f.write(json.dumps(list(row.values)))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NaN, NaN, NaN, NaN, NaN, NaN, 0.945866, NaN, NaN, 0.494202588206387, NaN, NaN, NaN, 7.0, 7.0, 1.0, 0.00134783702916585, 41.71766, 61.90571, 0.67032, 202.69596, 306.98965, NaN, 306.98965, 202.69596, 0.01653, 0.00482, 0.0205689315250881, 120.0, 81.97552, NaN, 2.118, 1.14006, 1.32468, 0.48883, 3.78405, 0.0, 100000000.0, 29.893056975288, 6.8732723, 11.042327931, 48.539034072552, 1.161872929525, 0.0, 0.437723046051, 0.158397586631, 0.422029989697, 0.0, 0.377984030119, 0.193620639333, 0.900680499402, NaN, NaN, NaN, NaN, NaN, NaN, NaN, 8.997901, 1.412973, 9.294714]\n",
      "[NaN, NaN, NaN, NaN, NaN, NaN, 0.650987, NaN, NaN, 0.488844046545112, 97.91514, 92.06308, 90.143693670312, 6.0, 6.0, 0.0, 0.00116329542495008, 4.41833, 7.90815, NaN, 38.14325, 50.46973, 3.80748, 50.46973, 38.14325, 0.00884, 0.00268, 0.0114366673210033, 120.0, 90.44944, 1.84214970527458, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, 1.8643, NaN, 1.72677, 0.0, 18.24572, 21.3478, NaN, 10.131126, 1.07082, 10.086173]\n",
      "[NaN, NaN, NaN, NaN, NaN, NaN, 0.751585, NaN, NaN, 0.582030274185796, 95.42591, 90.86684, 86.710508958244, 5.0, 5.0, 0.0, 0.00136322667868409, 5.18517, 12.68627, NaN, 58.99556, 76.867, NaN, 76.867, 58.99556, 0.01075, 0.00242, 0.0103271398943388, 110.0, 92.25756, 1.50963905319924, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, 1.34558, NaN, 1.37724, 13.09166, 0.0, 21.40256, NaN, 13.607153, 1.086256, 10.722116]\n",
      "[NaN, NaN, NaN, NaN, NaN, NaN, 1.410484, NaN, NaN, 0.974377215842701, 98.34165, 94.67883, 93.108723622695, 7.0, 7.0, 0.0, 0.00246727347105747, 32.32495, 24.39341, 6.16265, 154.60899, 217.49, 11.74454, 217.49, 154.60899, 0.01371, 0.00965, 0.041180537181971, 140.0, 129.8138, 3.94486887288418, 1.69966, 1.12461, 16.77551, 0.49932, 2.03639, 0.65609, 100000000.0, 19.3787383474863, 7.5173746, 6.141675796, 54.193013746985, 0.882565939892, 0.308577441748, 0.685618270153, 0.0, 7.543840657158, 0.0, 0.526298490308, 0.279162578433, 1.020214045862, 2.0179, NaN, 0.2731, 9.14884, 22.51555, 14.30739, NaN, 12.138418, 792.200968, 11.335088]\n",
      "[NaN, NaN, NaN, NaN, NaN, NaN, 1.198178, NaN, NaN, 0.799789823464695, 98.59263, 87.64914, 86.415592298382, 5.0, 5.0, 0.0, 0.00214539007092198, 5.39631, 6.42917, NaN, 24.14402, 35.9695, 3.09541, 35.9695, 24.14402, 0.01856, 0.00847, 0.0361449896301859, 140.0, 93.76275, 2.72351656791207, NaN, NaN, NaN, NaN, NaN, NaN, 100000000.0, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, 0.69584, NaN, 0.49482, 0.0, 32.36431, 14.9683, NaN, 8.607376, 813.120053, 12.228583]\n"
     ]
    }
   ],
   "source": [
    "!cat $input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "[u'delta', u'gamma', u'delta', u'delta', u'delta']\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform predict \\\n",
    "--model $model_name \\\n",
    "--version $model_version \\\n",
    "--json-instances $input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/benazirsproject/trainer_image:latest'"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINER_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://benazirsproject-demo/data/testing/test_amyris.csv'"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESTING_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline/amyris_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline/amyris_pipeline.py\n",
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"KFP pipeline orchestrating BigQuery and Cloud AI Platform services.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from helper_components import evaluate_model\n",
    "from helper_components import retrieve_best_run\n",
    "from jinja2 import Template\n",
    "import kfp\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp.dsl.types import Dict\n",
    "from kfp.dsl.types import GCPProjectID\n",
    "from kfp.dsl.types import GCPRegion\n",
    "from kfp.dsl.types import GCSPath\n",
    "from kfp.dsl.types import String\n",
    "from kfp.gcp import use_gcp_secret\n",
    "\n",
    "# Defaults and environment settings\n",
    "BASE_IMAGE = os.getenv('BASE_IMAGE')\n",
    "TRAINER_IMAGE = os.getenv('TRAINER_IMAGE')\n",
    "RUNTIME_VERSION = os.getenv('RUNTIME_VERSION')\n",
    "PYTHON_VERSION = os.getenv('PYTHON_VERSION')\n",
    "COMPONENT_URL_SEARCH_PREFIX = os.getenv('COMPONENT_URL_SEARCH_PREFIX')\n",
    "USE_KFP_SA = os.getenv('USE_KFP_SA')\n",
    "TRAINING_FILE_PATH = 'gs://benazirsproject-demo/data/training/Anonymized_Fermentation_Data_final.xlsx'\n",
    "TESTING_FILE_PATH = 'gs://benazirsproject-demo/data/testing/test_amyris.csv'\n",
    "# VALIDATION_FILE_PATH = 'datasets/validation/data.csv'\n",
    "# TESTING_FILE_PATH = 'datasets/testing/data.csv'\n",
    "\n",
    "# Parameter defaults\n",
    "# SPLITS_DATASET_ID = 'splits'\n",
    "HYPERTUNE_SETTINGS = \"\"\"\n",
    "{\n",
    "    \"hyperparameters\":  {\n",
    "        \"goal\": \"MAXIMIZE\",\n",
    "        \"maxTrials\": 3,\n",
    "        \"maxParallelTrials\": 3,\n",
    "        \"hyperparameterMetricTag\": \"accuracy\",\n",
    "        \"enableTrialEarlyStopping\": True,\n",
    "        \"algorithm\": \"RANDOM_SEARCH\",\n",
    "        \"params\": [\n",
    "            {\n",
    "                \"parameterName\": \"n_estimators\",\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"minValue\": 10,\n",
    "                \"maxValue\": 200,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"max_leaf_nodes\",\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"minValue\": 10,\n",
    "                \"maxValue\": 500,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"max_depth\",\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"minValue\": 3,\n",
    "                \"maxValue\": 20,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"min_samples_split\",\n",
    "                \"type\": \"DISCRETE\",\n",
    "                \"discreteValues\": [2,5,10]\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"max_features\",\n",
    "                \"type\": \"DOUBLE\",\n",
    "                \"minValue\": 0.5,\n",
    "                \"maxValue\": 1.0,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"class_weight\",\n",
    "                \"type\": \"CATEGORICAL\",\n",
    "                \"categoricalValues\": [ \"balanced\", \"balanced_subsample\"]\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"bootstrap\",\n",
    "                \"type\": \"CATEGORICAL\",\n",
    "                \"categoricalValues\": [ \"TRUE\", \"FALSE\"]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# # Helper functions\n",
    "# def generate_sampling_query(source_table_name, num_lots, lots):\n",
    "#     \"\"\"Prepares the data sampling query.\"\"\"\n",
    "\n",
    "#     sampling_query_template = \"\"\"\n",
    "#          SELECT *\n",
    "#          FROM \n",
    "#              `{{ source_table }}` AS cover\n",
    "#          WHERE \n",
    "#          MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) IN ({{ lots }})\n",
    "#          \"\"\"\n",
    "#     query = Template(sampling_query_template).render(\n",
    "#         source_table=source_table_name, num_lots=num_lots, lots=str(lots)[1:-1])\n",
    "\n",
    "#     return query\n",
    "\n",
    "\n",
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "# bigquery_query_op = component_store.load_component('bigquery/query')\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')\n",
    "retrieve_best_run_op = func_to_container_op(\n",
    "    retrieve_best_run, base_image=BASE_IMAGE)\n",
    "evaluate_model_op = func_to_container_op(evaluate_model, base_image=BASE_IMAGE)\n",
    "\n",
    "\n",
    "@kfp.dsl.pipeline(\n",
    "    name='Amyris Classifier Training',\n",
    "    description='The pipeline training and deploying the Amyris classifierpipeline_yaml'\n",
    ")\n",
    "def amyris_train(project_id,\n",
    "                    region,\n",
    "                    gcs_root,\n",
    "                    evaluation_metric_name,\n",
    "                    evaluation_metric_threshold,\n",
    "                    model_id,\n",
    "                    version_id,\n",
    "                    replace_existing_version,\n",
    "                    hypertune_settings=HYPERTUNE_SETTINGS,\n",
    "                    dataset_location='US'):\n",
    "    \"\"\"Orchestrates training and deployment of an sklearn model.\"\"\"\n",
    "\n",
    "    # Create the training split\n",
    "#     query = generate_sampling_query(\n",
    "#         source_table_name=source_table_name, num_lots=10, lots=[1, 2, 3, 4])\n",
    "\n",
    "#     training_file_path = '{}/{}'.format(gcs_root, TRAINING_FILE_PATH)\n",
    "\n",
    "#     create_training_split = bigquery_query_op(\n",
    "#         query=query,\n",
    "#         project_id=project_id,\n",
    "#         dataset_id=dataset_id,\n",
    "#         table_id='',\n",
    "#         output_gcs_path=training_file_path,\n",
    "#         dataset_location=dataset_location)\n",
    "\n",
    "#     # Create the validation split\n",
    "#     query = generate_sampling_query(\n",
    "#         source_table_name=source_table_name, num_lots=10, lots=[8])\n",
    "\n",
    "#     validation_file_path = '{}/{}'.format(gcs_root, VALIDATION_FILE_PATH)\n",
    "\n",
    "#     create_validation_split = bigquery_query_op(\n",
    "#         query=query,\n",
    "#         project_id=project_id,\n",
    "#         dataset_id=dataset_id,\n",
    "#         table_id='',\n",
    "#         output_gcs_path=validation_file_path,\n",
    "#         dataset_location=dataset_location)\n",
    "\n",
    "    # Create the testing split\n",
    "#     query = generate_sampling_query(\n",
    "#         source_table_name=source_table_name, num_lots=10, lots=[9])\n",
    "\n",
    "#     testing_file_path = '{}/{}'.format(gcs_root, TESTING_FILE_PATH)\n",
    "\n",
    "#     create_testing_split = bigquery_query_op(\n",
    "#         query=query,\n",
    "#         project_id=project_id,\n",
    "#         dataset_id=dataset_id,\n",
    "#         table_id='',\n",
    "#         output_gcs_path=testing_file_path,\n",
    "#         dataset_location=dataset_location)\n",
    "\n",
    "    # Tune hyperparameters\n",
    "    tune_args = [\n",
    "        '--training_dataset_path',\n",
    "        TRAINING_FILE_PATH,\n",
    "         '--hptune', 'True'\n",
    "    ]\n",
    "\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir/hypertune',\n",
    "                                kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    hypertune = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAINER_IMAGE,\n",
    "        job_dir=job_dir,\n",
    "        args=tune_args,\n",
    "        training_input=hypertune_settings)\n",
    "\n",
    "    # Retrieve the best trial\n",
    "    get_best_trial = retrieve_best_run_op(\n",
    "            project_id, hypertune.outputs['job_id'])\n",
    "\n",
    "    # Train the model on a combined training and validation datasets\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir', kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    train_args = [\n",
    "        '--training_dataset_path',\n",
    "       TRAINING_FILE_PATH,\n",
    "        '--n_estimators',get_best_trial.outputs['n_estimators'], \n",
    "        '--max_leaf_nodes',get_best_trial.outputs['max_leaf_nodes'], \n",
    "        '--max_depth',get_best_trial.outputs['max_depth'],\n",
    "        '--min_samples_split',get_best_trial.outputs['min_samples_split'],\n",
    "        '--max_features',get_best_trial.outputs['max_features'],\n",
    "        '--class_weight',get_best_trial.outputs['class_weight'],\n",
    "        '--bootstrap',get_best_trial.outputs['bootstrap'],\n",
    "        '--hptune', 'False'\n",
    "    ]\n",
    "\n",
    "    train_model = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAINER_IMAGE,\n",
    "        job_dir=job_dir,\n",
    "        args=train_args)\n",
    "\n",
    "    # Evaluate the model on the testing split\n",
    "    eval_model = evaluate_model_op(\n",
    "        dataset_path=TESTING_FILE_PATH,\n",
    "        model_path=str(train_model.outputs['job_dir']),\n",
    "        metric_name=evaluation_metric_name)\n",
    "\n",
    "    # Deploy the model if the primary metric is better than threshold\n",
    "    with kfp.dsl.Condition(eval_model.outputs['metric_value'] > evaluation_metric_threshold):\n",
    "        deploy_model = mlengine_deploy_op(\n",
    "        model_uri=train_model.outputs['job_dir'],\n",
    "        project_id=project_id,\n",
    "        model_id=model_id,\n",
    "        version_id=version_id,\n",
    "        runtime_version=RUNTIME_VERSION,\n",
    "        python_version=PYTHON_VERSION,\n",
    "        replace_existing_version=replace_existing_version)\n",
    "\n",
    "    # Configure the pipeline to run using the service account defined\n",
    "      # in the user-gcp-sa k8s secret\n",
    "    if USE_KFP_SA == 'True':\n",
    "        kfp.dsl.get_pipeline_conf().add_op_transformer(\n",
    "              use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/benazirsproject/amyris_trainer_image2:latest'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINER_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline/helper_components.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline/helper_components.py\n",
    "\n",
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\"\"\"Helper components.\"\"\"\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "def retrieve_best_run(project_id: str, job_id: str) -> NamedTuple('Outputs', [('metric_value', float), ('n_estimators', int),\n",
    "                            ('max_leaf_nodes', int), ('max_depth', int), ('min_samples_split', int),\n",
    "                             ('max_features', float), ('class_weight', str), ('bootstrap', str)]):\n",
    "    \"\"\"Retrieves the parameters of the best Hypertune run.\"\"\"\n",
    "\n",
    "    from googleapiclient import discovery\n",
    "    from googleapiclient import errors\n",
    "    \n",
    "    ml = discovery.build('ml', 'v1')\n",
    "\n",
    "    job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n",
    "    request = ml.projects().jobs().get(name=job_name)\n",
    "\n",
    "    try:\n",
    "        response = request.execute()\n",
    "    except errors.HttpError as err:\n",
    "        print(err)\n",
    "    except:\n",
    "        print('Unexpected error')\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    best_trial = response['trainingOutput']['trials'][0]\n",
    "\n",
    "    metric_value = best_trial['finalMetric']['objectiveValue']\n",
    "\n",
    "    n_estimators = int(best_trial['hyperparameters']['n_estimators'])\n",
    "    max_leaf_nodes = int(best_trial['hyperparameters']['max_leaf_nodes'])\n",
    "    max_depth = int(best_trial['hyperparameters']['max_depth'])\n",
    "    min_samples_split = int(best_trial['hyperparameters']['min_samples_split'])\n",
    "    max_features = float(best_trial['hyperparameters']['max_features'])\n",
    "    class_weight = best_trial['hyperparameters']['class_weight']\n",
    "    bootstrap = best_trial['hyperparameters']['bootstrap']\n",
    "\n",
    "    return (metric_value, n_estimators, max_leaf_nodes, max_depth, min_samples_split, max_features, class_weight, bootstrap )\n",
    "\n",
    "\n",
    "def evaluate_model(dataset_path: str, model_path: str, metric_name: str) -> NamedTuple('Outputs', [('metric_name', str), ('metric_value', float),\n",
    "                            ('mlpipeline_metrics', 'Metrics')]):\n",
    "    \n",
    "    \"\"\"Evaluates a trained sklearn model.\"\"\"\n",
    "    import pickle\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "    df_test = pd.read_csv(dataset_path)\n",
    "\n",
    "    X_test = df_test.drop('Run_Performance', axis=1)\n",
    "    y_test = df_test['Run_Performance']\n",
    "\n",
    "    # Copy the model from GCS\n",
    "    model_filename = 'model.pkl'\n",
    "    gcs_model_filepath = '{}/{}'.format(model_path, model_filename)\n",
    "    print(gcs_model_filepath)\n",
    "    subprocess.check_call(['gsutil', 'cp', gcs_model_filepath, model_filename],\n",
    "                        stderr=sys.stdout)\n",
    "\n",
    "    with open(model_filename, 'rb') as model_file:\n",
    "        model = pickle.load(model_file)\n",
    "        \n",
    "    y_hat = model.predict(X_test)\n",
    "\n",
    "    if metric_name == 'accuracy':\n",
    "        metric_value = accuracy_score(y_test, y_hat)\n",
    "    elif metric_name == 'recall':\n",
    "        metric_value = recall_score(y_test, y_hat)\n",
    "    else:\n",
    "        metric_name = 'N/A'\n",
    "        metric_value = 0\n",
    "\n",
    "    # Export the metric\n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': metric_name,\n",
    "          'numberValue': float(metric_value)\n",
    "      }]\n",
    "    }\n",
    "\n",
    "    return (metric_name, metric_value, json.dumps(metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create an empty folder base_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./base_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./base_image/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='base_image'\n",
    "TAG='latest'\n",
    "BASE_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 1 file(s) totalling 122 bytes before compression.\n",
      "Uploading tarball of [base_image] to [gs://benazirsproject_cloudbuild/source/1599228597.5-862becd638ab48cba856e4d9138fadcf.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/benazirsproject/builds/10f0594a-da4c-44f2-b174-c0694df81a7a].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/10f0594a-da4c-44f2-b174-c0694df81a7a?project=981930454113].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"10f0594a-da4c-44f2-b174-c0694df81a7a\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://benazirsproject_cloudbuild/source/1599228597.5-862becd638ab48cba856e4d9138fadcf.tgz#1599228597934229\n",
      "Copying gs://benazirsproject_cloudbuild/source/1599228597.5-862becd638ab48cba856e4d9138fadcf.tgz#1599228597934229...\n",
      "/ [1 files][  228.0 B/  228.0 B]                                                \n",
      "Operation completed over 1 objects/228.0 B.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  2.048kB\n",
      "Step 1/2 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7c3167c320d: Pulling fs layer\n",
      "131f805ec7fd: Pulling fs layer\n",
      "322ed380e680: Pulling fs layer\n",
      "6ac240b13098: Pulling fs layer\n",
      "9ce3a9266402: Pulling fs layer\n",
      "72c706dfac1d: Pulling fs layer\n",
      "6383427606e5: Pulling fs layer\n",
      "3e8b21666cec: Pulling fs layer\n",
      "358bb5d659ed: Pulling fs layer\n",
      "8ade7556a8f1: Pulling fs layer\n",
      "b2ebb7e1223e: Pulling fs layer\n",
      "8d5d283ad922: Pulling fs layer\n",
      "14c0fd48a5f3: Pulling fs layer\n",
      "ceaad5dc04d2: Pulling fs layer\n",
      "c1074350f761: Pulling fs layer\n",
      "687ad0b9a318: Pulling fs layer\n",
      "d2365d2ee19a: Pulling fs layer\n",
      "5095b04f1d98: Pulling fs layer\n",
      "6ac240b13098: Waiting\n",
      "9ce3a9266402: Waiting\n",
      "72c706dfac1d: Waiting\n",
      "6383427606e5: Waiting\n",
      "3e8b21666cec: Waiting\n",
      "358bb5d659ed: Waiting\n",
      "8ade7556a8f1: Waiting\n",
      "b2ebb7e1223e: Waiting\n",
      "8d5d283ad922: Waiting\n",
      "14c0fd48a5f3: Waiting\n",
      "ceaad5dc04d2: Waiting\n",
      "c1074350f761: Waiting\n",
      "687ad0b9a318: Waiting\n",
      "d2365d2ee19a: Waiting\n",
      "5095b04f1d98: Waiting\n",
      "322ed380e680: Verifying Checksum\n",
      "322ed380e680: Download complete\n",
      "131f805ec7fd: Verifying Checksum\n",
      "131f805ec7fd: Download complete\n",
      "6ac240b13098: Verifying Checksum\n",
      "6ac240b13098: Download complete\n",
      "d7c3167c320d: Verifying Checksum\n",
      "d7c3167c320d: Download complete\n",
      "6383427606e5: Verifying Checksum\n",
      "6383427606e5: Download complete\n",
      "72c706dfac1d: Verifying Checksum\n",
      "72c706dfac1d: Download complete\n",
      "358bb5d659ed: Verifying Checksum\n",
      "358bb5d659ed: Download complete\n",
      "8ade7556a8f1: Verifying Checksum\n",
      "8ade7556a8f1: Download complete\n",
      "3e8b21666cec: Verifying Checksum\n",
      "3e8b21666cec: Download complete\n",
      "b2ebb7e1223e: Verifying Checksum\n",
      "b2ebb7e1223e: Download complete\n",
      "8d5d283ad922: Verifying Checksum\n",
      "8d5d283ad922: Download complete\n",
      "14c0fd48a5f3: Verifying Checksum\n",
      "14c0fd48a5f3: Download complete\n",
      "ceaad5dc04d2: Verifying Checksum\n",
      "ceaad5dc04d2: Download complete\n",
      "c1074350f761: Verifying Checksum\n",
      "c1074350f761: Download complete\n",
      "687ad0b9a318: Verifying Checksum\n",
      "687ad0b9a318: Download complete\n",
      "5095b04f1d98: Verifying Checksum\n",
      "5095b04f1d98: Download complete\n",
      "9ce3a9266402: Verifying Checksum\n",
      "9ce3a9266402: Download complete\n",
      "d7c3167c320d: Pull complete\n",
      "d2365d2ee19a: Verifying Checksum\n",
      "d2365d2ee19a: Download complete\n",
      "131f805ec7fd: Pull complete\n",
      "322ed380e680: Pull complete\n",
      "6ac240b13098: Pull complete\n",
      "9ce3a9266402: Pull complete\n",
      "72c706dfac1d: Pull complete\n",
      "6383427606e5: Pull complete\n",
      "3e8b21666cec: Pull complete\n",
      "358bb5d659ed: Pull complete\n",
      "8ade7556a8f1: Pull complete\n",
      "b2ebb7e1223e: Pull complete\n",
      "8d5d283ad922: Pull complete\n",
      "14c0fd48a5f3: Pull complete\n",
      "ceaad5dc04d2: Pull complete\n",
      "c1074350f761: Pull complete\n",
      "687ad0b9a318: Pull complete\n",
      "d2365d2ee19a: Pull complete\n",
      "5095b04f1d98: Pull complete\n",
      "Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d8706668f140\n",
      "Step 2/2 : RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5\n",
      " ---> Running in 3b275a6560fe\n",
      "Collecting fire\n",
      "  Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Collecting kfp==0.2.5\n",
      "  Downloading kfp-0.2.5.tar.gz (116 kB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Collecting urllib3<1.25,>=1.15\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.29.0)\n",
      "Collecting kubernetes<=10.0.0,>=8.0.0\n",
      "  Downloading kubernetes-10.0.0-py2.py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied, skipping upgrade: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.9.2)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.17.2)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting cloudpickle==1.1.1\n",
      "  Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting kfp-server-api<=0.1.40,>=0.1.18\n",
      "  Downloading kfp-server-api-0.1.40.tar.gz (38 kB)\n",
      "Collecting argo-models==2.2.1a\n",
      "  Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (3.2.0)\n",
      "Collecting tabulate==0.8.3\n",
      "  Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "Collecting click==7.0\n",
      "  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.10-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.9.tar.gz (30 kB)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (0.5.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (47.3.1.post20200616)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (0.57.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.2.5) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (0.2.7)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==0.2.5) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp==0.2.5) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.4.2->kfp==0.2.5) (2.20)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==0.2.5) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp==0.2.5) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.51.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (3.12.3)\n",
      "Building wheels for collected packages: fire, kfp, termcolor, kfp-server-api, argo-models, tabulate, strip-hints\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=f73ca10abfef5474e5318b41a1b5614e25d94241d5bb305c2635c621fee04b8f\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-0.2.5-py3-none-any.whl size=159978 sha256=917de0e81021c7382645fddc3aec10c82f3f20a74b8cc5aca0bdfdee5d5f8191\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/74/7e/0a882d654bdf82d039460ab5c6adf8724ae56e277de7c0eaea\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=0d35b14e8dcd03f148c8883bd5568fe8bce0fc3f21614b26f354407cbf1b8de5\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-0.1.40-py3-none-any.whl size=102468 sha256=e50291cc27a5644da04898ea55480391ef88316719f1832139b8f641b7a4fa62\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/e3/43/3972dea76ee89e35f090b313817089043f2609236cf560069d\n",
      "  Building wheel for argo-models (setup.py): started\n",
      "  Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "  Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57307 sha256=3a931b3f82f3e60a52210295e25664f065b4cb0a347001c93de87135e77a0c55\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/4b/fd/cdd013bd2ad1a7162ecfaf954e9f1bb605174a20e3c02016b7\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23378 sha256=ef5b1ec8cc32c219c568b8e186972842d095cacff6a146c160300cafceef3b39\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.9-py2.py3-none-any.whl size=20993 sha256=183afd812706e567f93f7549b93891bd1ca2ff045430a835675dd8a1fb97c633\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/b8/4e/a3ec111d2db63cec88121bd7c0ab1a123bce3b55dd19dda5c1\n",
      "Successfully built fire kfp termcolor kfp-server-api argo-models tabulate strip-hints\n",
      "\u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: jupyterlab-git 0.10.1 has requirement nbdime<2.0.0,>=1.1.0, but you'll have nbdime 2.0.0 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: distributed 2.19.0 has requirement cloudpickle>=1.3.0, but you'll have cloudpickle 1.1.1 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: termcolor, fire, scikit-learn, pandas, urllib3, kubernetes, requests-toolbelt, cloudpickle, kfp-server-api, argo-models, tabulate, click, Deprecated, strip-hints, kfp\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.1\n",
      "    Uninstalling scikit-learn-0.23.1:\n",
      "      Successfully uninstalled scikit-learn-0.23.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.9\n",
      "    Uninstalling urllib3-1.25.9:\n",
      "      Successfully uninstalled urllib3-1.25.9\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 11.0.0\n",
      "    Uninstalling kubernetes-11.0.0:\n",
      "      Successfully uninstalled kubernetes-11.0.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.4.1\n",
      "    Uninstalling cloudpickle-1.4.1:\n",
      "      Successfully uninstalled cloudpickle-1.4.1\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 7.1.2\n",
      "    Uninstalling click-7.1.2:\n",
      "      Successfully uninstalled click-7.1.2\n",
      "Successfully installed Deprecated-1.2.10 argo-models-2.2.1a0 click-7.0 cloudpickle-1.1.1 fire-0.3.1 kfp-0.2.5 kfp-server-api-0.1.40 kubernetes-10.0.0 pandas-0.24.2 requests-toolbelt-0.9.1 scikit-learn-0.20.4 strip-hints-0.1.9 tabulate-0.8.3 termcolor-1.1.0 urllib3-1.24.3\n",
      "Removing intermediate container 3b275a6560fe\n",
      " ---> 0fd3dd7362d5\n",
      "Successfully built 0fd3dd7362d5\n",
      "Successfully tagged gcr.io/benazirsproject/base_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/benazirsproject/base_image:latest\n",
      "The push refers to repository [gcr.io/benazirsproject/base_image]\n",
      "433c03b3f28c: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "89212ed9ad75: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "c51fe61c6231: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "4c4a5579b7a8: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "433c03b3f28c: Pushed\n",
      "latest: digest: sha256:b27ca3a77fa1f7021475bda28be4b6c591a7700af31cfa7e9f15ab5d63da3119 size: 4293\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                    IMAGES                                       STATUS\n",
      "10f0594a-da4c-44f2-b174-c0694df81a7a  2020-09-04T14:09:58+00:00  3M56S     gs://benazirsproject_cloudbuild/source/1599228597.5-862becd638ab48cba856e4d9138fadcf.tgz  gcr.io/benazirsproject/base_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $BASE_IMAGE base_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ENDPOINT = '19a5aed0f754a516-dot-us-central2.pipelines.googleusercontent.com'\n",
    "ARTIFACT_STORE_URI = 'gs://benazirsproject-demo'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compile pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: USE_KFP_SA=False\n",
      "env: BASE_IMAGE=gcr.io/benazirsproject/base_image:latest\n",
      "env: TRAINER_IMAGE=gcr.io/benazirsproject/amyris_trainer_image2:latest\n",
      "env: COMPONENT_URL_SEARCH_PREFIX=https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/\n",
      "env: RUNTIME_VERSION=1.15\n",
      "env: PYTHON_VERSION=3.7\n"
     ]
    }
   ],
   "source": [
    "USE_KFP_SA = False\n",
    "\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/'\n",
    "RUNTIME_VERSION = '1.15'\n",
    "PYTHON_VERSION = '3.7'\n",
    "\n",
    "%env USE_KFP_SA={USE_KFP_SA}\n",
    "%env BASE_IMAGE={BASE_IMAGE}\n",
    "%env TRAINER_IMAGE={TRAINER_IMAGE}\n",
    "%env COMPONENT_URL_SEARCH_PREFIX={COMPONENT_URL_SEARCH_PREFIX}\n",
    "%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "%env PYTHON_VERSION={PYTHON_VERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy helper components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dsl-compile --py pipeline/amyris_pipeline.py --output amyris_pipeline.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: argoproj.io/v1alpha1\n",
      "kind: Workflow\n",
      "metadata:\n",
      "  generateName: amyris-classifier-training-\n",
      "  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.0.0, pipelines.kubeflow.org/pipeline_compilation_time: '2020-09-04T04:19:37.732728',\n",
      "    pipelines.kubeflow.org/pipeline_spec: '{\"description\": \"The pipeline training\n",
      "      and deploying the Amyris classifierpipeline_yaml\", \"inputs\": [{\"name\": \"project_id\"},\n",
      "      {\"name\": \"region\"}, {\"name\": \"gcs_root\"}, {\"name\": \"evaluation_metric_name\"},\n",
      "      {\"name\": \"evaluation_metric_threshold\"}, {\"name\": \"model_id\"}, {\"name\": \"version_id\"},\n",
      "      {\"name\": \"replace_existing_version\"}, {\"default\": \"\\n{\\n    \\\"hyperparameters\\\":  {\\n        \\\"goal\\\":\n"
     ]
    }
   ],
   "source": [
    "!head amyris_pipeline.yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deploy pipeline package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 0906289c-a5dd-4292-ac8c-71c8925dac26 has been submitted\n",
      "\n",
      "Pipeline Details\n",
      "------------------\n",
      "ID           0906289c-a5dd-4292-ac8c-71c8925dac26\n",
      "Name         amyris_pipeline_RF3\n",
      "Description\n",
      "Uploaded at  2020-09-04T14:14:21+00:00\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| Parameter Name              | Default Value                                         |\n",
      "+=============================+=======================================================+\n",
      "| project_id                  |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| region                      |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| gcs_root                    |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| evaluation_metric_name      |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| evaluation_metric_threshold |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| model_id                    |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| version_id                  |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| replace_existing_version    |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| hypertune_settings          | {                                                     |\n",
      "|                             |     \"hyperparameters\":  {                             |\n",
      "|                             |         \"goal\": \"MAXIMIZE\",                           |\n",
      "|                             |         \"maxTrials\": 3,                               |\n",
      "|                             |         \"maxParallelTrials\": 3,                       |\n",
      "|                             |         \"hyperparameterMetricTag\": \"accuracy\",        |\n",
      "|                             |         \"enableTrialEarlyStopping\": True,             |\n",
      "|                             |         \"algorithm\": \"RANDOM_SEARCH\",                 |\n",
      "|                             |         \"params\": [                                   |\n",
      "|                             |             {                                         |\n",
      "|                             |                 \"parameterName\": \"n_estimators\",      |\n",
      "|                             |                 \"type\": \"INTEGER\",                    |\n",
      "|                             |                 \"minValue\": 10,                       |\n",
      "|                             |                 \"maxValue\": 200,                      |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "|                             |             },                                        |\n",
      "|                             |             {                                         |\n",
      "|                             |                 \"parameterName\": \"max_depth\",         |\n",
      "|                             |                 \"type\": \"INTEGER\",                    |\n",
      "|                             |                 \"minValue\": 3,                        |\n",
      "|                             |                 \"maxValue\": 20,                       |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "|                             |             },                                        |\n",
      "|                             |             {                                         |\n",
      "|                             |                 \"parameterName\": \"min_samples_split\", |\n",
      "|                             |                 \"type\": \"DISCRETE\",                   |\n",
      "|                             |                 \"discreteValues\": [2,5,10]            |\n",
      "|                             |             }                                         |\n",
      "|                             |         ]                                             |\n",
      "|                             |     }                                                 |\n",
      "|                             | }                                                     |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| dataset_location            | US                                                    |\n",
      "+-----------------------------+-------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME='amyris_pipeline_RF3'\n",
    "\n",
    "!kfp --endpoint $ENDPOINT pipeline upload \\\n",
    "-p $PIPELINE_NAME \\\n",
    "amyris_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| Pipeline ID                          | Name                                            | Uploaded at               |\n",
      "+======================================+=================================================+===========================+\n",
      "| 0906289c-a5dd-4292-ac8c-71c8925dac26 | amyris_pipeline_RF3                             | 2020-09-04T14:14:21+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 124bc222-1b03-405b-a46c-23f39436d02b | amyris_pipeline_RF2                             | 2020-09-04T13:21:07+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| c2171aa5-2680-4a96-b5de-516679f24c07 | amyris_pipeline_sept                            | 2020-09-02T22:26:04+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 4ffefa9f-c1aa-4b11-9b4b-ad70c223226e | covertype_continuous_training_self              | 2020-08-26T23:25:58+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 1b5be90d-6020-43fc-8126-b6d979da2e39 | [Tutorial] DSL - Control structures             | 2020-08-26T22:17:35+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| b162aff5-6673-41eb-8619-221ccfaaa71c | [Tutorial] Data passing in python components    | 2020-08-26T22:17:34+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 81d20eef-4bb7-4bed-8d87-569c56f824f5 | [Demo] TFX - Iris classification pipeline       | 2020-08-26T22:17:33+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 5707fa44-e6d1-403f-88fb-88ee299d2f33 | [Demo] TFX - Taxi tip prediction model trainer  | 2020-08-26T22:17:32+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 6f868f09-82bc-49b5-b44f-ed260eb653e0 | [Demo] XGBoost - Training with confusion matrix | 2020-08-26T22:17:31+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT pipeline list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID='0906289c-a5dd-4292-ac8c-71c8925dac26'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'amyrisRF'\n",
    "RUN_ID = 'Run_001'\n",
    "EVALUATION_METRIC = 'accuracy'\n",
    "EVALUATION_METRIC_THRESHOLD = '0.69'\n",
    "MODEL_ID = 'amyris_endtoendRF2'\n",
    "VERSION_ID = 'v01'\n",
    "REPLACE_EXISTING_VERSION = 'True'\n",
    "\n",
    "GCS_STAGING_PATH = '{}/staging'.format(ARTIFACT_STORE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run d54ea2d8-77c1-46f4-b893-0e6143809666 is submitted\n",
      "+--------------------------------------+---------+----------+---------------------------+\n",
      "| run id                               | name    | status   | created at                |\n",
      "+======================================+=========+==========+===========================+\n",
      "| d54ea2d8-77c1-46f4-b893-0e6143809666 | Run_001 |          | 2020-09-04T14:15:14+00:00 |\n",
      "+--------------------------------------+---------+----------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT run submit \\\n",
    "-e $EXPERIMENT_NAME \\\n",
    "-r $RUN_ID \\\n",
    "-p $PIPELINE_ID \\\n",
    "project_id=$PROJECT_ID \\\n",
    "gcs_root=$GCS_STAGING_PATH \\\n",
    "region=$REGION \\\n",
    "evaluation_metric_name=$EVALUATION_METRIC \\\n",
    "evaluation_metric_threshold=$EVALUATION_METRIC_THRESHOLD \\\n",
    "model_id=$MODEL_ID \\\n",
    "version_id=$VERSION_ID \\\n",
    "replace_existing_version=$REPLACE_EXISTING_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CI/CD PART "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENDPOINT = '19a5aed0f754a516-dot-us-central2.pipelines.googleusercontent.com'\n",
    "# PROJECT_ID = !(gcloud config get-value core/project)\n",
    "# PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy the kfp-cli folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "RUN pip install kfp==0.2.5\n",
      "ENTRYPOINT [\"/bin/bash\"]\n"
     ]
    }
   ],
   "source": [
    "!cat kfp-cli/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='kfp-cli'\n",
    "TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 206 bytes before compression.\n",
      "Uploading tarball of [kfp-cli] to [gs://benazirsproject_cloudbuild/source/1599315785.7-334d8b717fdf484982fb00bc9ee0ac40.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/benazirsproject/builds/c7999640-8c63-4b80-90eb-df23d2e27adb].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/c7999640-8c63-4b80-90eb-df23d2e27adb?project=981930454113].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"c7999640-8c63-4b80-90eb-df23d2e27adb\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://benazirsproject_cloudbuild/source/1599315785.7-334d8b717fdf484982fb00bc9ee0ac40.tgz#1599315786049287\n",
      "Copying gs://benazirsproject_cloudbuild/source/1599315785.7-334d8b717fdf484982fb00bc9ee0ac40.tgz#1599315786049287...\n",
      "/ [1 files][  284.0 B/  284.0 B]                                                \n",
      "Operation completed over 1 objects/284.0 B.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  3.584kB\n",
      "Step 1/3 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7c3167c320d: Pulling fs layer\n",
      "131f805ec7fd: Pulling fs layer\n",
      "322ed380e680: Pulling fs layer\n",
      "6ac240b13098: Pulling fs layer\n",
      "9ce3a9266402: Pulling fs layer\n",
      "72c706dfac1d: Pulling fs layer\n",
      "6383427606e5: Pulling fs layer\n",
      "3e8b21666cec: Pulling fs layer\n",
      "358bb5d659ed: Pulling fs layer\n",
      "8ade7556a8f1: Pulling fs layer\n",
      "b2ebb7e1223e: Pulling fs layer\n",
      "8d5d283ad922: Pulling fs layer\n",
      "14c0fd48a5f3: Pulling fs layer\n",
      "ceaad5dc04d2: Pulling fs layer\n",
      "c1074350f761: Pulling fs layer\n",
      "687ad0b9a318: Pulling fs layer\n",
      "d2365d2ee19a: Pulling fs layer\n",
      "5095b04f1d98: Pulling fs layer\n",
      "6ac240b13098: Waiting\n",
      "9ce3a9266402: Waiting\n",
      "72c706dfac1d: Waiting\n",
      "6383427606e5: Waiting\n",
      "3e8b21666cec: Waiting\n",
      "358bb5d659ed: Waiting\n",
      "8ade7556a8f1: Waiting\n",
      "b2ebb7e1223e: Waiting\n",
      "8d5d283ad922: Waiting\n",
      "14c0fd48a5f3: Waiting\n",
      "ceaad5dc04d2: Waiting\n",
      "c1074350f761: Waiting\n",
      "687ad0b9a318: Waiting\n",
      "d2365d2ee19a: Waiting\n",
      "5095b04f1d98: Waiting\n",
      "322ed380e680: Verifying Checksum\n",
      "322ed380e680: Download complete\n",
      "131f805ec7fd: Verifying Checksum\n",
      "131f805ec7fd: Download complete\n",
      "6ac240b13098: Verifying Checksum\n",
      "6ac240b13098: Download complete\n",
      "d7c3167c320d: Verifying Checksum\n",
      "d7c3167c320d: Download complete\n",
      "6383427606e5: Download complete\n",
      "72c706dfac1d: Verifying Checksum\n",
      "72c706dfac1d: Download complete\n",
      "358bb5d659ed: Verifying Checksum\n",
      "358bb5d659ed: Download complete\n",
      "3e8b21666cec: Verifying Checksum\n",
      "3e8b21666cec: Download complete\n",
      "8ade7556a8f1: Verifying Checksum\n",
      "8ade7556a8f1: Download complete\n",
      "b2ebb7e1223e: Verifying Checksum\n",
      "b2ebb7e1223e: Download complete\n",
      "8d5d283ad922: Verifying Checksum\n",
      "8d5d283ad922: Download complete\n",
      "14c0fd48a5f3: Verifying Checksum\n",
      "14c0fd48a5f3: Download complete\n",
      "ceaad5dc04d2: Verifying Checksum\n",
      "ceaad5dc04d2: Download complete\n",
      "c1074350f761: Verifying Checksum\n",
      "c1074350f761: Download complete\n",
      "687ad0b9a318: Verifying Checksum\n",
      "687ad0b9a318: Download complete\n",
      "9ce3a9266402: Verifying Checksum\n",
      "9ce3a9266402: Download complete\n",
      "5095b04f1d98: Verifying Checksum\n",
      "5095b04f1d98: Download complete\n",
      "d7c3167c320d: Pull complete\n",
      "131f805ec7fd: Pull complete\n",
      "322ed380e680: Pull complete\n",
      "6ac240b13098: Pull complete\n",
      "d2365d2ee19a: Verifying Checksum\n",
      "d2365d2ee19a: Download complete\n",
      "9ce3a9266402: Pull complete\n",
      "72c706dfac1d: Pull complete\n",
      "6383427606e5: Pull complete\n",
      "3e8b21666cec: Pull complete\n",
      "358bb5d659ed: Pull complete\n",
      "8ade7556a8f1: Pull complete\n",
      "b2ebb7e1223e: Pull complete\n",
      "8d5d283ad922: Pull complete\n",
      "14c0fd48a5f3: Pull complete\n",
      "ceaad5dc04d2: Pull complete\n",
      "c1074350f761: Pull complete\n",
      "687ad0b9a318: Pull complete\n",
      "d2365d2ee19a: Pull complete\n",
      "5095b04f1d98: Pull complete\n",
      "Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d8706668f140\n",
      "Step 2/3 : RUN pip install kfp==0.2.5\n",
      " ---> Running in 5e631698b5f0\n",
      "Collecting kfp==0.2.5\n",
      "  Downloading kfp-0.2.5.tar.gz (116 kB)\n",
      "Collecting urllib3<1.25,>=1.15\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.15.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.8.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (5.3.1)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.29.0)\n",
      "Collecting kubernetes<=10.0.0,>=8.0.0\n",
      "  Downloading kubernetes-10.0.0-py2.py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.7.1)\n",
      "Requirement already satisfied: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.9.2)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.17.2)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting cloudpickle==1.1.1\n",
      "  Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting kfp-server-api<=0.1.40,>=0.1.18\n",
      "  Downloading kfp-server-api-0.1.40.tar.gz (38 kB)\n",
      "Collecting argo-models==2.2.1a\n",
      "  Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (3.2.0)\n",
      "Collecting tabulate==0.8.3\n",
      "  Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "Collecting click==7.0\n",
      "  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.10-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.9.tar.gz (30 kB)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (0.5.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (1.3.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (0.57.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.2.0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (47.3.1.post20200616)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.24.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.2.5) (1.14.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.6)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (1.7.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (19.3.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==0.2.5) (1.11.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp==0.2.5) (0.34.2)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.16.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.10)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.4.2->kfp==0.2.5) (2.20)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==0.2.5) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp==0.2.5) (3.1.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (3.12.3)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (2020.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.51.0)\n",
      "Building wheels for collected packages: kfp, kfp-server-api, argo-models, tabulate, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-0.2.5-py3-none-any.whl size=159978 sha256=1e1d2a03981080b1d6b8c3108c308ebd11e39751451493f8be287864ac811713\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/74/7e/0a882d654bdf82d039460ab5c6adf8724ae56e277de7c0eaea\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-0.1.40-py3-none-any.whl size=102468 sha256=816af1313ed8c4cd870c9e1d5529e5ddc48db790ac2ac708d0fc06b9dca8b014\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/e3/43/3972dea76ee89e35f090b313817089043f2609236cf560069d\n",
      "  Building wheel for argo-models (setup.py): started\n",
      "  Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "  Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57307 sha256=98a7792cac33c57f4ebe15968927cc81121b08d0bd437c2c2dd1d56e2e20ee88\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/4b/fd/cdd013bd2ad1a7162ecfaf954e9f1bb605174a20e3c02016b7\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23378 sha256=7d274b822fd195296466ca799d7f363b620374fc87165993b74ace680a6adeb2\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.9-py2.py3-none-any.whl size=20993 sha256=09c193d6e4df65983efa6fdb391744dd90fc1ac20f4679753e18fceca6a9d7d8\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/b8/4e/a3ec111d2db63cec88121bd7c0ab1a123bce3b55dd19dda5c1\n",
      "Successfully built kfp kfp-server-api argo-models tabulate strip-hints\n",
      "\u001b[91mERROR: jupyterlab-git 0.10.1 has requirement nbdime<2.0.0,>=1.1.0, but you'll have nbdime 2.0.0 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: distributed 2.19.0 has requirement cloudpickle>=1.3.0, but you'll have cloudpickle 1.1.1 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: urllib3, kubernetes, requests-toolbelt, cloudpickle, kfp-server-api, argo-models, tabulate, click, Deprecated, strip-hints, kfp\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.9\n",
      "    Uninstalling urllib3-1.25.9:\n",
      "      Successfully uninstalled urllib3-1.25.9\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 11.0.0\n",
      "    Uninstalling kubernetes-11.0.0:\n",
      "      Successfully uninstalled kubernetes-11.0.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.4.1\n",
      "    Uninstalling cloudpickle-1.4.1:\n",
      "      Successfully uninstalled cloudpickle-1.4.1\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 7.1.2\n",
      "    Uninstalling click-7.1.2:\n",
      "      Successfully uninstalled click-7.1.2\n",
      "Successfully installed Deprecated-1.2.10 argo-models-2.2.1a0 click-7.0 cloudpickle-1.1.1 kfp-0.2.5 kfp-server-api-0.1.40 kubernetes-10.0.0 requests-toolbelt-0.9.1 strip-hints-0.1.9 tabulate-0.8.3 urllib3-1.24.3\n",
      "Removing intermediate container 5e631698b5f0\n",
      " ---> 7b5e398deb2e\n",
      "Step 3/3 : ENTRYPOINT [\"/bin/bash\"]\n",
      " ---> Running in 468cf065a2c4\n",
      "Removing intermediate container 468cf065a2c4\n",
      " ---> 257e71152d5c\n",
      "Successfully built 257e71152d5c\n",
      "Successfully tagged gcr.io/benazirsproject/kfp-cli:latest\n",
      "PUSH\n",
      "Pushing gcr.io/benazirsproject/kfp-cli:latest\n",
      "The push refers to repository [gcr.io/benazirsproject/kfp-cli]\n",
      "1a2cd3a7eb5f: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "c51fe61c6231: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "89212ed9ad75: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "4c4a5579b7a8: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "1a2cd3a7eb5f: Pushed\n",
      "latest: digest: sha256:7c1a8fe8bee9f5de508572ba24e10b9084ec91b9ba070f397b12048a7c283193 size: 4292\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                    IMAGES                                    STATUS\n",
      "c7999640-8c63-4b80-90eb-df23d2e27adb  2020-09-05T14:23:06+00:00  3M26S     gs://benazirsproject_cloudbuild/source/1599315785.7-334d8b717fdf484982fb00bc9ee0ac40.tgz  gcr.io/benazirsproject/kfp-cli (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag {IMAGE_URI} kfp-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSTITUTIONS=\"\"\"\n",
    "_ENDPOINT={},\\\n",
    "_TRAINER_IMAGE_NAME=trainer_image,\\\n",
    "_BASE_IMAGE_NAME=base_image,\\\n",
    "TAG_NAME=latest,\\\n",
    "_PIPELINE_FOLDER=.,\\\n",
    "_PIPELINE_DSL=amyris_pipeline.py,\\\n",
    "_PIPELINE_PACKAGE=amyris_pipeline.yaml,\\\n",
    "_PIPELINE_NAME=amyris_pipeline_RF7,\\\n",
    "_RUNTIME_VERSION=1.15,\\\n",
    "_PYTHON_VERSION=3.7,\\\n",
    "_USE_KFP_SA=True,\\\n",
    "_COMPONENT_URL_SEARCH_PREFIX=https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/\n",
    "\"\"\".format(ENDPOINT).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 31 file(s) totalling 25.2 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://benazirsproject_cloudbuild/source/1599413258.24-5a887cc0c30e4e4697f300c45116c76c.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/benazirsproject/builds/8a9238b6-1ff2-419d-9cee-34d3f3863f20].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/8a9238b6-1ff2-419d-9cee-34d3f3863f20?project=981930454113].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"8a9238b6-1ff2-419d-9cee-34d3f3863f20\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://benazirsproject_cloudbuild/source/1599413258.24-5a887cc0c30e4e4697f300c45116c76c.tgz#1599413260612117\n",
      "Copying gs://benazirsproject_cloudbuild/source/1599413258.24-5a887cc0c30e4e4697f300c45116c76c.tgz#1599413260612117...\n",
      "/ [1 files][  8.2 MiB/  8.2 MiB]                                                \n",
      "Operation completed over 1 objects/8.2 MiB.                                      \n",
      "BUILD\n",
      "Starting Step #0\n",
      "Step #0: Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #0: Sending build context to Docker daemon  11.26kB\n",
      "Step #0: Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "Step #0: latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "Step #0: d7c3167c320d: Pulling fs layer\n",
      "Step #0: 131f805ec7fd: Pulling fs layer\n",
      "Step #0: 322ed380e680: Pulling fs layer\n",
      "Step #0: 6ac240b13098: Pulling fs layer\n",
      "Step #0: 9ce3a9266402: Pulling fs layer\n",
      "Step #0: 72c706dfac1d: Pulling fs layer\n",
      "Step #0: 6383427606e5: Pulling fs layer\n",
      "Step #0: 3e8b21666cec: Pulling fs layer\n",
      "Step #0: 358bb5d659ed: Pulling fs layer\n",
      "Step #0: 8ade7556a8f1: Pulling fs layer\n",
      "Step #0: b2ebb7e1223e: Pulling fs layer\n",
      "Step #0: 8d5d283ad922: Pulling fs layer\n",
      "Step #0: 14c0fd48a5f3: Pulling fs layer\n",
      "Step #0: ceaad5dc04d2: Pulling fs layer\n",
      "Step #0: c1074350f761: Pulling fs layer\n",
      "Step #0: 687ad0b9a318: Pulling fs layer\n",
      "Step #0: d2365d2ee19a: Pulling fs layer\n",
      "Step #0: 5095b04f1d98: Pulling fs layer\n",
      "Step #0: 6ac240b13098: Waiting\n",
      "Step #0: 9ce3a9266402: Waiting\n",
      "Step #0: 72c706dfac1d: Waiting\n",
      "Step #0: 6383427606e5: Waiting\n",
      "Step #0: 3e8b21666cec: Waiting\n",
      "Step #0: 358bb5d659ed: Waiting\n",
      "Step #0: 8ade7556a8f1: Waiting\n",
      "Step #0: b2ebb7e1223e: Waiting\n",
      "Step #0: 8d5d283ad922: Waiting\n",
      "Step #0: 14c0fd48a5f3: Waiting\n",
      "Step #0: ceaad5dc04d2: Waiting\n",
      "Step #0: c1074350f761: Waiting\n",
      "Step #0: 687ad0b9a318: Waiting\n",
      "Step #0: d2365d2ee19a: Waiting\n",
      "Step #0: 5095b04f1d98: Waiting\n",
      "Step #0: 131f805ec7fd: Verifying Checksum\n",
      "Step #0: 131f805ec7fd: Download complete\n",
      "Step #0: 322ed380e680: Verifying Checksum\n",
      "Step #0: 322ed380e680: Download complete\n",
      "Step #0: 6ac240b13098: Download complete\n",
      "Step #0: d7c3167c320d: Verifying Checksum\n",
      "Step #0: d7c3167c320d: Download complete\n",
      "Step #0: 6383427606e5: Verifying Checksum\n",
      "Step #0: 6383427606e5: Download complete\n",
      "Step #0: 72c706dfac1d: Verifying Checksum\n",
      "Step #0: 72c706dfac1d: Download complete\n",
      "Step #0: 358bb5d659ed: Verifying Checksum\n",
      "Step #0: 358bb5d659ed: Download complete\n",
      "Step #0: 3e8b21666cec: Verifying Checksum\n",
      "Step #0: 3e8b21666cec: Download complete\n",
      "Step #0: 8ade7556a8f1: Verifying Checksum\n",
      "Step #0: 8ade7556a8f1: Download complete\n",
      "Step #0: b2ebb7e1223e: Verifying Checksum\n",
      "Step #0: b2ebb7e1223e: Download complete\n",
      "Step #0: 14c0fd48a5f3: Verifying Checksum\n",
      "Step #0: 14c0fd48a5f3: Download complete\n",
      "Step #0: 8d5d283ad922: Verifying Checksum\n",
      "Step #0: 8d5d283ad922: Download complete\n",
      "Step #0: c1074350f761: Verifying Checksum\n",
      "Step #0: c1074350f761: Download complete\n",
      "Step #0: ceaad5dc04d2: Verifying Checksum\n",
      "Step #0: ceaad5dc04d2: Download complete\n",
      "Step #0: 687ad0b9a318: Verifying Checksum\n",
      "Step #0: 687ad0b9a318: Download complete\n",
      "Step #0: 5095b04f1d98: Verifying Checksum\n",
      "Step #0: 5095b04f1d98: Download complete\n",
      "Step #0: 9ce3a9266402: Verifying Checksum\n",
      "Step #0: 9ce3a9266402: Download complete\n",
      "Step #0: d7c3167c320d: Pull complete\n",
      "Step #0: d2365d2ee19a: Verifying Checksum\n",
      "Step #0: d2365d2ee19a: Download complete\n",
      "Step #0: 131f805ec7fd: Pull complete\n",
      "Step #0: 322ed380e680: Pull complete\n",
      "Step #0: 6ac240b13098: Pull complete\n",
      "Step #0: 9ce3a9266402: Pull complete\n",
      "Step #0: 72c706dfac1d: Pull complete\n",
      "Step #0: 6383427606e5: Pull complete\n",
      "Step #0: 3e8b21666cec: Pull complete\n",
      "Step #0: 358bb5d659ed: Pull complete\n",
      "Step #0: 8ade7556a8f1: Pull complete\n",
      "Step #0: b2ebb7e1223e: Pull complete\n",
      "Step #0: 8d5d283ad922: Pull complete\n",
      "Step #0: 14c0fd48a5f3: Pull complete\n",
      "Step #0: ceaad5dc04d2: Pull complete\n",
      "Step #0: c1074350f761: Pull complete\n",
      "Step #0: 687ad0b9a318: Pull complete\n",
      "Step #0: d2365d2ee19a: Pull complete\n",
      "Step #0: 5095b04f1d98: Pull complete\n",
      "Step #0: Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Step #0: Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      "Step #0:  ---> d8706668f140\n",
      "Step #0: Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      "Step #0:  ---> Running in bf92bd6eae3a\n",
      "Step #0: Collecting fire\n",
      "Step #0:   Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Step #0: Collecting cloudml-hypertune\n",
      "Step #0:   Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Step #0: Collecting scikit-learn==0.20.4\n",
      "Step #0:   Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Step #0: Collecting pandas==0.24.2\n",
      "Step #0:   Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Step #0: Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Step #0: Collecting termcolor\n",
      "Step #0:   Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Step #0: Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Step #0: Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Step #0: Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Step #0: Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Step #0: Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "Step #0:   Building wheel for fire (setup.py): started\n",
      "Step #0:   Building wheel for fire (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=380ac2099c4fdf320a534a7dbbc1be6fec3b90f7f926647e54a6cd82af5f455d\n",
      "Step #0:   Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "Step #0:   Building wheel for cloudml-hypertune (setup.py): started\n",
      "Step #0:   Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3986 sha256=d973187d5283354b54539e82505e6506b605edc0fecab0d1a4fc7ea46b20ab33\n",
      "Step #0:   Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Step #0:   Building wheel for termcolor (setup.py): started\n",
      "Step #0:   Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=de781d5563230528c7cb6f40db40425fef34ed9d91f9a8d949079c1c57cd8088\n",
      "Step #0:   Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Step #0: Successfully built fire cloudml-hypertune termcolor\n",
      "Step #0: \u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "Step #0: \u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "Step #0: \u001b[0mInstalling collected packages: termcolor, fire, cloudml-hypertune, scikit-learn, pandas\n",
      "Step #0:   Attempting uninstall: scikit-learn\n",
      "Step #0:     Found existing installation: scikit-learn 0.23.1\n",
      "Step #0:     Uninstalling scikit-learn-0.23.1:\n",
      "Step #0:       Successfully uninstalled scikit-learn-0.23.1\n",
      "Step #0:   Attempting uninstall: pandas\n",
      "Step #0:     Found existing installation: pandas 1.0.5\n",
      "Step #0:     Uninstalling pandas-1.0.5:\n",
      "Step #0:       Successfully uninstalled pandas-1.0.5\n",
      "Step #0: Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.3.1 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "Step #0: Removing intermediate container bf92bd6eae3a\n",
      "Step #0:  ---> aefaf377189b\n",
      "Step #0: Step 3/5 : WORKDIR /app\n",
      "Step #0:  ---> Running in 5444bfa10fd5\n",
      "Step #0: Removing intermediate container 5444bfa10fd5\n",
      "Step #0:  ---> aaf416c4203f\n",
      "Step #0: Step 4/5 : COPY train.py .\n",
      "Step #0:  ---> 5da84df81a14\n",
      "Step #0: Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      "Step #0:  ---> Running in 5fddef56bc4e\n",
      "Step #0: Removing intermediate container 5fddef56bc4e\n",
      "Step #0:  ---> f7dc68f6c570\n",
      "Step #0: Successfully built f7dc68f6c570\n",
      "Step #0: Successfully tagged gcr.io/benazirsproject/trainer_image:latest\n",
      "Finished Step #0\n",
      "Starting Step #1\n",
      "Step #1: Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #1: Sending build context to Docker daemon  3.584kB\n",
      "Step #1: Step 1/2 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "Step #1:  ---> d8706668f140\n",
      "Step #1: Step 2/2 : RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5\n",
      "Step #1:  ---> Running in 03b162651abc\n",
      "Step #1: Collecting fire\n",
      "Step #1:   Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Step #1: Collecting scikit-learn==0.20.4\n",
      "Step #1:   Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Step #1: Collecting pandas==0.24.2\n",
      "Step #1:   Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Step #1: Collecting kfp==0.2.5\n",
      "Step #1:   Downloading kfp-0.2.5.tar.gz (116 kB)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Step #1: Collecting termcolor\n",
      "Step #1:   Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Step #1: Collecting urllib3<1.25,>=1.15\n",
      "Step #1:   Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2020.6.20)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (5.3.1)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.29.0)\n",
      "Step #1: Collecting kubernetes<=10.0.0,>=8.0.0\n",
      "Step #1:   Downloading kubernetes-10.0.0-py2.py3-none-any.whl (1.5 MB)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.7.1)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.9.2)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.17.2)\n",
      "Step #1: Collecting requests_toolbelt>=0.8.0\n",
      "Step #1:   Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Step #1: Collecting cloudpickle==1.1.1\n",
      "Step #1:   Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Step #1: Collecting kfp-server-api<=0.1.40,>=0.1.18\n",
      "Step #1:   Downloading kfp-server-api-0.1.40.tar.gz (38 kB)\n",
      "Step #1: Collecting argo-models==2.2.1a\n",
      "Step #1:   Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (3.2.0)\n",
      "Step #1: Collecting tabulate==0.8.3\n",
      "Step #1:   Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "Step #1: Collecting click==7.0\n",
      "Step #1:   Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "Step #1: Collecting Deprecated\n",
      "Step #1:   Downloading Deprecated-1.2.10-py2.py3-none-any.whl (8.7 kB)\n",
      "Step #1: Collecting strip-hints\n",
      "Step #1:   Downloading strip-hints-0.1.9.tar.gz (30 kB)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (1.3.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (0.5.1)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (47.3.1.post20200616)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.24.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.2.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (0.57.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.2.5) (1.14.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.1.1)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (0.2.7)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.6)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (1.7.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (19.3.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (0.16.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==0.2.5) (1.11.2)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp==0.2.5) (0.34.2)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.16.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.10)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.4)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.1)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.4.2->kfp==0.2.5) (2.20)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==0.2.5) (0.4.8)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp==0.2.5) (3.1.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (3.12.3)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.51.0)\n",
      "Step #1: Building wheels for collected packages: fire, kfp, termcolor, kfp-server-api, argo-models, tabulate, strip-hints\n",
      "Step #1:   Building wheel for fire (setup.py): started\n",
      "Step #1:   Building wheel for fire (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=756852731be7b94bcdc1da865ef35b728587232931e5cbcda87ca75f83c814db\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "Step #1:   Building wheel for kfp (setup.py): started\n",
      "Step #1:   Building wheel for kfp (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for kfp: filename=kfp-0.2.5-py3-none-any.whl size=159978 sha256=296f4d58f1788a3ab171dc5ea8595e14392f821563e08a99712660468d052c75\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/98/74/7e/0a882d654bdf82d039460ab5c6adf8724ae56e277de7c0eaea\n",
      "Step #1:   Building wheel for termcolor (setup.py): started\n",
      "Step #1:   Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=20f287317d7e209f877f28c5cddf4cf1b40a5391bc648bae16ca669ccc180975\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Step #1:   Building wheel for kfp-server-api (setup.py): started\n",
      "Step #1:   Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for kfp-server-api: filename=kfp_server_api-0.1.40-py3-none-any.whl size=102468 sha256=6d9fe520b43757a36373950316f492574721d416789e9680a66fb9e73c0fea1d\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/01/e3/43/3972dea76ee89e35f090b313817089043f2609236cf560069d\n",
      "Step #1:   Building wheel for argo-models (setup.py): started\n",
      "Step #1:   Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57307 sha256=6ad4b187d6bb573743cc63abbbecc7c16588b4bd8a783778df56773ae7261651\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/a9/4b/fd/cdd013bd2ad1a7162ecfaf954e9f1bb605174a20e3c02016b7\n",
      "Step #1:   Building wheel for tabulate (setup.py): started\n",
      "Step #1:   Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23378 sha256=0c8808ed2116e7abee5eaf48f671f348b1b26813799c89d8f34b2d5e3f89ee5c\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "Step #1:   Building wheel for strip-hints (setup.py): started\n",
      "Step #1:   Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for strip-hints: filename=strip_hints-0.1.9-py2.py3-none-any.whl size=20993 sha256=0d3b627441961196a18a3bb99175fe316b072b45c6069db1b7c7097b8e2c787e\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/2d/b8/4e/a3ec111d2db63cec88121bd7c0ab1a123bce3b55dd19dda5c1\n",
      "Step #1: Successfully built fire kfp termcolor kfp-server-api argo-models tabulate strip-hints\n",
      "Step #1: \u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "Step #1: \u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "Step #1: \u001b[0m\u001b[91mERROR: jupyterlab-git 0.10.1 has requirement nbdime<2.0.0,>=1.1.0, but you'll have nbdime 2.0.0 which is incompatible.\n",
      "Step #1: \u001b[0m\u001b[91mERROR: distributed 2.19.0 has requirement cloudpickle>=1.3.0, but you'll have cloudpickle 1.1.1 which is incompatible.\n",
      "Step #1: \u001b[0mInstalling collected packages: termcolor, fire, scikit-learn, pandas, urllib3, kubernetes, requests-toolbelt, cloudpickle, kfp-server-api, argo-models, tabulate, click, Deprecated, strip-hints, kfp\n",
      "Step #1:   Attempting uninstall: scikit-learn\n",
      "Step #1:     Found existing installation: scikit-learn 0.23.1\n",
      "Step #1:     Uninstalling scikit-learn-0.23.1:\n",
      "Step #1:       Successfully uninstalled scikit-learn-0.23.1\n",
      "Step #1:   Attempting uninstall: pandas\n",
      "Step #1:     Found existing installation: pandas 1.0.5\n",
      "Step #1:     Uninstalling pandas-1.0.5:\n",
      "Step #1:       Successfully uninstalled pandas-1.0.5\n",
      "Step #1:   Attempting uninstall: urllib3\n",
      "Step #1:     Found existing installation: urllib3 1.25.9\n",
      "Step #1:     Uninstalling urllib3-1.25.9:\n",
      "Step #1:       Successfully uninstalled urllib3-1.25.9\n",
      "Step #1:   Attempting uninstall: kubernetes\n",
      "Step #1:     Found existing installation: kubernetes 11.0.0\n",
      "Step #1:     Uninstalling kubernetes-11.0.0:\n",
      "Step #1:       Successfully uninstalled kubernetes-11.0.0\n",
      "Step #1:   Attempting uninstall: cloudpickle\n",
      "Step #1:     Found existing installation: cloudpickle 1.4.1\n",
      "Step #1:     Uninstalling cloudpickle-1.4.1:\n",
      "Step #1:       Successfully uninstalled cloudpickle-1.4.1\n",
      "Step #1:   Attempting uninstall: click\n",
      "Step #1:     Found existing installation: click 7.1.2\n",
      "Step #1:     Uninstalling click-7.1.2:\n",
      "Step #1:       Successfully uninstalled click-7.1.2\n",
      "Step #1: Successfully installed Deprecated-1.2.10 argo-models-2.2.1a0 click-7.0 cloudpickle-1.1.1 fire-0.3.1 kfp-0.2.5 kfp-server-api-0.1.40 kubernetes-10.0.0 pandas-0.24.2 requests-toolbelt-0.9.1 scikit-learn-0.20.4 strip-hints-0.1.9 tabulate-0.8.3 termcolor-1.1.0 urllib3-1.24.3\n",
      "Step #1: Removing intermediate container 03b162651abc\n",
      "Step #1:  ---> d321b0122a1b\n",
      "Step #1: Successfully built d321b0122a1b\n",
      "Step #1: Successfully tagged gcr.io/benazirsproject/base_image:latest\n",
      "Finished Step #1\n",
      "Starting Step #2\n",
      "Step #2: Pulling image: gcr.io/benazirsproject/kfp-cli\n",
      "Step #2: Using default tag: latest\n",
      "Step #2: latest: Pulling from benazirsproject/kfp-cli\n",
      "Step #2: d7c3167c320d: Already exists\n",
      "Step #2: 131f805ec7fd: Already exists\n",
      "Step #2: 322ed380e680: Already exists\n",
      "Step #2: 6ac240b13098: Already exists\n",
      "Step #2: 9ce3a9266402: Already exists\n",
      "Step #2: 72c706dfac1d: Already exists\n",
      "Step #2: 6383427606e5: Already exists\n",
      "Step #2: 3e8b21666cec: Already exists\n",
      "Step #2: 358bb5d659ed: Already exists\n",
      "Step #2: 8ade7556a8f1: Already exists\n",
      "Step #2: b2ebb7e1223e: Already exists\n",
      "Step #2: 8d5d283ad922: Already exists\n",
      "Step #2: 14c0fd48a5f3: Already exists\n",
      "Step #2: ceaad5dc04d2: Already exists\n",
      "Step #2: c1074350f761: Already exists\n",
      "Step #2: 687ad0b9a318: Already exists\n",
      "Step #2: d2365d2ee19a: Already exists\n",
      "Step #2: 5095b04f1d98: Already exists\n",
      "Step #2: 5bc0a4a57d67: Pulling fs layer\n",
      "Step #2: 5bc0a4a57d67: Verifying Checksum\n",
      "Step #2: 5bc0a4a57d67: Download complete\n",
      "Step #2: 5bc0a4a57d67: Pull complete\n",
      "Step #2: Digest: sha256:7c1a8fe8bee9f5de508572ba24e10b9084ec91b9ba070f397b12048a7c283193\n",
      "Step #2: Status: Downloaded newer image for gcr.io/benazirsproject/kfp-cli:latest\n",
      "Step #2: gcr.io/benazirsproject/kfp-cli:latest\n",
      "Finished Step #2\n",
      "Starting Step #3\n",
      "Step #3: Already have image (with digest): gcr.io/benazirsproject/kfp-cli\n",
      "Step #3: Pipeline 1657e059-76dd-4516-898b-35c1a0c25b4a has been submitted\n",
      "Step #3: \n",
      "Step #3: Pipeline Details\n",
      "Step #3: ------------------\n",
      "Step #3: ID           1657e059-76dd-4516-898b-35c1a0c25b4a\n",
      "Step #3: Name         amyris_pipeline_RF7_latest\n",
      "Step #3: Description\n",
      "Step #3: Uploaded at  2020-09-06T17:31:36+00:00\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | Parameter Name              | Default Value                                         |\n",
      "Step #3: +=============================+=======================================================+\n",
      "Step #3: | project_id                  |                                                       |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | region                      |                                                       |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | gcs_root                    |                                                       |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | evaluation_metric_name      |                                                       |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | evaluation_metric_threshold |                                                       |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | model_id                    |                                                       |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | version_id                  |                                                       |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | replace_existing_version    |                                                       |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | hypertune_settings          | {                                                     |\n",
      "Step #3: |                             |     \"hyperparameters\":  {                             |\n",
      "Step #3: |                             |         \"goal\": \"MAXIMIZE\",                           |\n",
      "Step #3: |                             |         \"maxTrials\": 3,                               |\n",
      "Step #3: |                             |         \"maxParallelTrials\": 3,                       |\n",
      "Step #3: |                             |         \"hyperparameterMetricTag\": \"accuracy\",        |\n",
      "Step #3: |                             |         \"enableTrialEarlyStopping\": True,             |\n",
      "Step #3: |                             |         \"algorithm\": \"RANDOM_SEARCH\",                 |\n",
      "Step #3: |                             |         \"params\": [                                   |\n",
      "Step #3: |                             |             {                                         |\n",
      "Step #3: |                             |                 \"parameterName\": \"n_estimators\",      |\n",
      "Step #3: |                             |                 \"type\": \"INTEGER\",                    |\n",
      "Step #3: |                             |                 \"minValue\": 10,                       |\n",
      "Step #3: |                             |                 \"maxValue\": 200,                      |\n",
      "Step #3: |                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "Step #3: |                             |             },                                        |\n",
      "Step #3: |                             |             {                                         |\n",
      "Step #3: |                             |                 \"parameterName\": \"max_leaf_nodes\",    |\n",
      "Step #3: |                             |                 \"type\": \"INTEGER\",                    |\n",
      "Step #3: |                             |                 \"minValue\": 10,                       |\n",
      "Step #3: |                             |                 \"maxValue\": 500,                      |\n",
      "Step #3: |                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "Step #3: |                             |             },                                        |\n",
      "Step #3: |                             |             {                                         |\n",
      "Step #3: |                             |                 \"parameterName\": \"max_depth\",         |\n",
      "Step #3: |                             |                 \"type\": \"INTEGER\",                    |\n",
      "Step #3: |                             |                 \"minValue\": 3,                        |\n",
      "Step #3: |                             |                 \"maxValue\": 20,                       |\n",
      "Step #3: |                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "Step #3: |                             |             },                                        |\n",
      "Step #3: |                             |             {                                         |\n",
      "Step #3: |                             |                 \"parameterName\": \"min_samples_split\", |\n",
      "Step #3: |                             |                 \"type\": \"DISCRETE\",                   |\n",
      "Step #3: |                             |                 \"discreteValues\": [2,5,10]            |\n",
      "Step #3: |                             |             }                                         |\n",
      "Step #3: |                             |         ]                                             |\n",
      "Step #3: |                             |     }                                                 |\n",
      "Step #3: |                             | }                                                     |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | dataset_location            | US                                                    |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Finished Step #3\n",
      "PUSH\n",
      "Pushing gcr.io/benazirsproject/trainer_image:latest\n",
      "The push refers to repository [gcr.io/benazirsproject/trainer_image]\n",
      "3cecb901bd3b: Preparing\n",
      "1e215500b4b6: Preparing\n",
      "7349f45a0355: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "222959643149: Waiting\n",
      "badaf1bc8335: Waiting\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "89212ed9ad75: Layer already exists\n",
      "c51fe61c6231: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "1e215500b4b6: Pushed\n",
      "3cecb901bd3b: Pushed\n",
      "4c4a5579b7a8: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "7349f45a0355: Pushed\n",
      "latest: digest: sha256:3209765e77a53d5e23abf27c7e9a54b782e71d49eaf3107e15d5b7feb6b1308f size: 4708\n",
      "Pushing gcr.io/benazirsproject/base_image:latest\n",
      "The push refers to repository [gcr.io/benazirsproject/base_image]\n",
      "16258cc67096: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "89212ed9ad75: Layer already exists\n",
      "c51fe61c6231: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "4c4a5579b7a8: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "16258cc67096: Pushed\n",
      "latest: digest: sha256:8f50cdf6a0647e67eb30fa014220adcd4101f5013f7489ceb65979d4a6f858a8 size: 4293\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                     IMAGES                                          STATUS\n",
      "8a9238b6-1ff2-419d-9cee-34d3f3863f20  2020-09-06T17:27:40+00:00  4M14S     gs://benazirsproject_cloudbuild/source/1599413258.24-5a887cc0c30e4e4697f300c45116c76c.tgz  gcr.io/benazirsproject/trainer_image (+3 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit . --config cloudbuild.yaml --substitutions {SUBSTITUTIONS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
