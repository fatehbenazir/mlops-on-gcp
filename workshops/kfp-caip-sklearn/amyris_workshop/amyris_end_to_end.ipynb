{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lab 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "import time \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upload data in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://benazirsproject-demo'\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'Anonymized_Fermentation_Data_final.xlsx')\n",
    "# VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://benazirsproject-demo/data/training/Anonymized_Fermentation_Data_final.xlsx'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(TRAINING_FILE_PATH,sheet_name='data')\n",
    "meta_data = pd.read_excel(TRAINING_FILE_PATH, sheet_name='meta data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## they need to upload data in bigquery - and then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Purpose</th>\n",
       "      <th>experiment</th>\n",
       "      <th>run</th>\n",
       "      <th>Project_Name</th>\n",
       "      <th>run_label</th>\n",
       "      <th>Strain</th>\n",
       "      <th>strain_key</th>\n",
       "      <th>Feedstock_Parent1</th>\n",
       "      <th>Start_Time</th>\n",
       "      <th>...</th>\n",
       "      <th>Cap_Oil_Em_End__percent</th>\n",
       "      <th>Cap_PCV_End__percent</th>\n",
       "      <th>Cap_Dead_Cell_Layer_End__percent</th>\n",
       "      <th>Zeex9ieJAlt_end__g_L</th>\n",
       "      <th>Zeex9ieJ_mAU_sec_end__area</th>\n",
       "      <th>Zeex9ieJ_Screening_end__g_L</th>\n",
       "      <th>Zeex9ieJ_end__g_L</th>\n",
       "      <th>Zeex9ieJ_end__g_L_121</th>\n",
       "      <th>Zeex9ieJ_end__g_L_122</th>\n",
       "      <th>interval_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>15</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>11408-15</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>9215897</td>\n",
       "      <td>m1098919</td>\n",
       "      <td>1/20/21 2:25 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>16.32145</td>\n",
       "      <td>21.04034</td>\n",
       "      <td>NULL</td>\n",
       "      <td>7.455144</td>\n",
       "      <td>1.3341</td>\n",
       "      <td>9.923562</td>\n",
       "      <td>0.606429</td>\n",
       "      <td>0.675183</td>\n",
       "      <td>0</td>\n",
       "      <td>Cumulative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>15</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>11408-15</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>9215897</td>\n",
       "      <td>m1098919</td>\n",
       "      <td>1/20/21 2:25 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>16.32145</td>\n",
       "      <td>21.04034</td>\n",
       "      <td>NULL</td>\n",
       "      <td>7.455144</td>\n",
       "      <td>1.3341</td>\n",
       "      <td>9.923562</td>\n",
       "      <td>0.606429</td>\n",
       "      <td>0.675183</td>\n",
       "      <td>0</td>\n",
       "      <td>Curated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>15</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>11408-15</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>9215897</td>\n",
       "      <td>m1098919</td>\n",
       "      <td>1/20/21 2:25 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>16.32145</td>\n",
       "      <td>21.04034</td>\n",
       "      <td>NULL</td>\n",
       "      <td>7.455144</td>\n",
       "      <td>1.3341</td>\n",
       "      <td>9.923562</td>\n",
       "      <td>0.606429</td>\n",
       "      <td>0.675183</td>\n",
       "      <td>0</td>\n",
       "      <td>Cumulative (Day 3-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>15</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>11408-15</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>9215897</td>\n",
       "      <td>m1098919</td>\n",
       "      <td>1/20/21 2:25 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>16.32145</td>\n",
       "      <td>21.04034</td>\n",
       "      <td>NULL</td>\n",
       "      <td>7.455144</td>\n",
       "      <td>1.3341</td>\n",
       "      <td>9.923562</td>\n",
       "      <td>0.606429</td>\n",
       "      <td>0.675183</td>\n",
       "      <td>0</td>\n",
       "      <td>Single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>16</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>11409-16</td>\n",
       "      <td>F9FD5EC4C1</td>\n",
       "      <td>9215897</td>\n",
       "      <td>m1098919</td>\n",
       "      <td>1/20/21 2:24 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>15.21805</td>\n",
       "      <td>20.34157</td>\n",
       "      <td>NULL</td>\n",
       "      <td>4.546885</td>\n",
       "      <td>2.179946</td>\n",
       "      <td>7.52725</td>\n",
       "      <td>0.342935</td>\n",
       "      <td>0.455659</td>\n",
       "      <td>1.684056</td>\n",
       "      <td>Cumulative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>16209-8</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8770794</td>\n",
       "      <td>m1252714</td>\n",
       "      <td>9/30/21 2:32 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>19.50613</td>\n",
       "      <td>22.02205</td>\n",
       "      <td>NULL</td>\n",
       "      <td>17.845346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>16209-8</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8770794</td>\n",
       "      <td>m1252714</td>\n",
       "      <td>9/30/21 2:32 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0144</td>\n",
       "      <td>18.78522</td>\n",
       "      <td>NULL</td>\n",
       "      <td>31.702724</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Cumulative (Day 3-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>16209-8</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8770794</td>\n",
       "      <td>m1252714</td>\n",
       "      <td>9/30/21 2:32 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>16.5879</td>\n",
       "      <td>18.74606</td>\n",
       "      <td>NULL</td>\n",
       "      <td>38.355477</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Cumulative (Day 3-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>16209-8</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8770794</td>\n",
       "      <td>m1252714</td>\n",
       "      <td>9/30/21 2:32 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0144</td>\n",
       "      <td>18.78522</td>\n",
       "      <td>NULL</td>\n",
       "      <td>31.702724</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>Zeex9ieJ</td>\n",
       "      <td>MF</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8</td>\n",
       "      <td>Zeex9ieJ for All</td>\n",
       "      <td>16209-8</td>\n",
       "      <td>B4EDDA67F0</td>\n",
       "      <td>8770794</td>\n",
       "      <td>m1252714</td>\n",
       "      <td>9/30/21 2:32 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>16.5879</td>\n",
       "      <td>18.74606</td>\n",
       "      <td>NULL</td>\n",
       "      <td>38.355477</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Single</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1568 rows × 124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Product Purpose  experiment  run      Project_Name run_label  \\\n",
       "0     Zeex9ieJ      MF  F9FD5EC4C1   15  Zeex9ieJ for All  11408-15   \n",
       "1     Zeex9ieJ      MF  F9FD5EC4C1   15  Zeex9ieJ for All  11408-15   \n",
       "2     Zeex9ieJ      MF  F9FD5EC4C1   15  Zeex9ieJ for All  11408-15   \n",
       "3     Zeex9ieJ      MF  F9FD5EC4C1   15  Zeex9ieJ for All  11408-15   \n",
       "4     Zeex9ieJ      MF  F9FD5EC4C1   16  Zeex9ieJ for All  11409-16   \n",
       "...        ...     ...         ...  ...               ...       ...   \n",
       "1563  Zeex9ieJ      MF  B4EDDA67F0    8  Zeex9ieJ for All   16209-8   \n",
       "1564  Zeex9ieJ      MF  B4EDDA67F0    8  Zeex9ieJ for All   16209-8   \n",
       "1565  Zeex9ieJ      MF  B4EDDA67F0    8  Zeex9ieJ for All   16209-8   \n",
       "1566  Zeex9ieJ      MF  B4EDDA67F0    8  Zeex9ieJ for All   16209-8   \n",
       "1567  Zeex9ieJ      MF  B4EDDA67F0    8  Zeex9ieJ for All   16209-8   \n",
       "\n",
       "          Strain  strain_key Feedstock_Parent1       Start_Time  ...  \\\n",
       "0     F9FD5EC4C1     9215897          m1098919  1/20/21 2:25 PM  ...   \n",
       "1     F9FD5EC4C1     9215897          m1098919  1/20/21 2:25 PM  ...   \n",
       "2     F9FD5EC4C1     9215897          m1098919  1/20/21 2:25 PM  ...   \n",
       "3     F9FD5EC4C1     9215897          m1098919  1/20/21 2:25 PM  ...   \n",
       "4     F9FD5EC4C1     9215897          m1098919  1/20/21 2:24 PM  ...   \n",
       "...          ...         ...               ...              ...  ...   \n",
       "1563  B4EDDA67F0     8770794          m1252714  9/30/21 2:32 PM  ...   \n",
       "1564  B4EDDA67F0     8770794          m1252714  9/30/21 2:32 PM  ...   \n",
       "1565  B4EDDA67F0     8770794          m1252714  9/30/21 2:32 PM  ...   \n",
       "1566  B4EDDA67F0     8770794          m1252714  9/30/21 2:32 PM  ...   \n",
       "1567  B4EDDA67F0     8770794          m1252714  9/30/21 2:32 PM  ...   \n",
       "\n",
       "      Cap_Oil_Em_End__percent  Cap_PCV_End__percent  \\\n",
       "0                    16.32145              21.04034   \n",
       "1                    16.32145              21.04034   \n",
       "2                    16.32145              21.04034   \n",
       "3                    16.32145              21.04034   \n",
       "4                    15.21805              20.34157   \n",
       "...                       ...                   ...   \n",
       "1563                 19.50613              22.02205   \n",
       "1564                  16.0144              18.78522   \n",
       "1565                  16.5879              18.74606   \n",
       "1566                  16.0144              18.78522   \n",
       "1567                  16.5879              18.74606   \n",
       "\n",
       "      Cap_Dead_Cell_Layer_End__percent Zeex9ieJAlt_end__g_L  \\\n",
       "0                                 NULL             7.455144   \n",
       "1                                 NULL             7.455144   \n",
       "2                                 NULL             7.455144   \n",
       "3                                 NULL             7.455144   \n",
       "4                                 NULL             4.546885   \n",
       "...                                ...                  ...   \n",
       "1563                              NULL            17.845346   \n",
       "1564                              NULL            31.702724   \n",
       "1565                              NULL            38.355477   \n",
       "1566                              NULL            31.702724   \n",
       "1567                              NULL            38.355477   \n",
       "\n",
       "     Zeex9ieJ_mAU_sec_end__area Zeex9ieJ_Screening_end__g_L Zeex9ieJ_end__g_L  \\\n",
       "0                        1.3341                    9.923562          0.606429   \n",
       "1                        1.3341                    9.923562          0.606429   \n",
       "2                        1.3341                    9.923562          0.606429   \n",
       "3                        1.3341                    9.923562          0.606429   \n",
       "4                      2.179946                     7.52725          0.342935   \n",
       "...                         ...                         ...               ...   \n",
       "1563                          0                           0                 0   \n",
       "1564                          0                           0                 0   \n",
       "1565                          0                           0                 0   \n",
       "1566                          0                           0                 0   \n",
       "1567                          0                           0                 0   \n",
       "\n",
       "     Zeex9ieJ_end__g_L_121 Zeex9ieJ_end__g_L_122        interval_type  \n",
       "0                 0.675183                     0           Cumulative  \n",
       "1                 0.675183                     0              Curated  \n",
       "2                 0.675183                     0  Cumulative (Day 3-)  \n",
       "3                 0.675183                     0               Single  \n",
       "4                 0.455659              1.684056           Cumulative  \n",
       "...                    ...                   ...                  ...  \n",
       "1563                     0                  NULL               Single  \n",
       "1564                     0                  NULL  Cumulative (Day 3-)  \n",
       "1565                     0                  NULL  Cumulative (Day 3-)  \n",
       "1566                     0                  NULL               Single  \n",
       "1567                     0                  NULL               Single  \n",
       "\n",
       "[1568 rows x 124 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "SELECT *\n",
    "FROM `amyris.amyris_fermentation_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r7286b0f3cf393f52_000001744bd1a546_1 ... (1s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table amyris.training \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `amyris.amyris_fermentation_data` AS training \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(training))), 10) IN (1, 2, 3, 4)' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contains 1568 samples and 124 variables\n",
      "There are 20 unique strains which are replicated or measured under different fermentation conditions.\n",
      "The variables are comprised of a variety of fermentation process (meta data), physiological and biochemical parameters (independent):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "independent    63\n",
       "metadata       53\n",
       "dependent       5\n",
       "category        3\n",
       "Name: variable type, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "include: ['Run_Execution' 'Run_Performance' 'Product_Produced__g'\n",
      " 'Titer_End__g_over_kg']\n"
     ]
    }
   ],
   "source": [
    "meta_data.head()\n",
    "\n",
    "print('The data contains {} samples and {} variables'.format(data.shape[0],data.shape[1]))\n",
    "print('There are {} unique strains which are replicated or measured under different fermentation conditions.'.format(data['Strain'].nunique()))\n",
    "print('The variables are comprised of a variety of fermentation process (meta data), physiological and biochemical parameters (independent):')\n",
    "display(meta_data['variable type'].value_counts())\n",
    "print('include: {}'.format(meta_data.query('target == 1').name.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for analysis\n",
    "#split out numeric from categorical varibles\n",
    "numeric_vars = ((data.dtypes == 'float64') | (data.dtypes == 'int64')) & (meta_data['variable type'] == 'independent').values\n",
    "numeric_x_data = data[data.columns[numeric_vars]]\n",
    "\n",
    "#things to try to predict\n",
    "y_data = data[['Run_Performance']]#data[data.columns[(meta_data['target'] == 1).values]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta data about variables\n",
    "meta_data = meta_data.set_index('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/pandas/core/generic.py:6786: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_data.Run_Performance.replace(('delta', 'gamma'), (1, 0), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run_Performance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Run_Performance\n",
       "0                1\n",
       "1                1"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = numeric_x_data[:1400]\n",
    "y_train = y_data[:1400]\n",
    "X_validation = numeric_x_data[1400:]\n",
    "y_validation = y_data[1400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute missing with median\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "\n",
    "#auto scale\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=3)\n",
    "pipe = Pipeline([('imputer',imputer),\n",
    "                 ('scaler', scaler),\n",
    "                 ('pca', pca),\n",
    "                 ('classifier', SGDClassifier(loss='log', tol=1e-3))\n",
    "                ])\n",
    "pipe.set_params(classifier__alpha=0.001, classifier__max_iter=200)\n",
    "pca_result = pipe.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8869047619047619\n"
     ]
    }
   ],
   "source": [
    "accuracy = pipe.score(X_validation, y_validation)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## steps to create an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, alpha, max_iter, hptune):\n",
    "    data = pd.read_excel(training_dataset_path,sheet_name='data')\n",
    "    meta_data = pd.read_excel(training_dataset_path, sheet_name='meta data')\n",
    "    \n",
    "    numeric_vars = ((data.dtypes == 'float64') | (data.dtypes == 'int64')) & (meta_data['variable type'] == 'independent').values\n",
    "    numeric_x_data = data[data.columns[numeric_vars]]\n",
    "\n",
    "    #things to try to predict\n",
    "    y_data = data[['Run_Performance']]\n",
    "    meta_data = meta_data.set_index('name')\n",
    "    y_data.Run_Performance.replace(('delta', 'gamma'), (1, 0), inplace=True)\n",
    "\n",
    "    X_train = numeric_x_data[:1400]\n",
    "    y_train = y_data[:1400]\n",
    "    X_validation = numeric_x_data[1400:]\n",
    "    y_validation = y_data[1400:]\n",
    "    \n",
    "    if not hptune:\n",
    "        X_train = pd.concat([X_train, X_validation])\n",
    "        y_train = pd.concat([y_train, y_validation])\n",
    "\n",
    "    #impute missing with median\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "\n",
    "    #auto scale\n",
    "    scaler = StandardScaler()\n",
    "    pca = PCA(n_components=3)\n",
    "    pipe = Pipeline([('imputer',imputer),\n",
    "                     ('scaler', scaler),\n",
    "                     ('pca', pca),\n",
    "                     ('classifier', SGDClassifier(loss='log', tol=1e-3))\n",
    "                    ])\n",
    "\n",
    "    \n",
    "\n",
    "    print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "\n",
    "    pipe.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "    pipe.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    if hptune:\n",
    "        accuracy = pipe.score(X_validation, y_validation)\n",
    "        print('Model accuracy: {}'.format(accuracy))\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "          hyperparameter_metric_tag='accuracy',\n",
    "          metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        model_filename = 'model.pkl'\n",
    "        with open(model_filename, 'wb') as model_file:\n",
    "            pickle.dump(pipe, model_file)\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "        subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## package script into a docker image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# buidl the docker image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='amyris_trainer_image'\n",
    "IMAGE_TAG='latest'\n",
    "TRAINER_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/benazirsproject/amyris_trainer_image:latest'"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINER_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 4 file(s) totalling 4.6 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://benazirsproject_cloudbuild/source/1599102700.08-554cb713b31b4a1683a6ad530f16f96a.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/benazirsproject/builds/e5278675-b531-4d53-ac36-f7d23ce0704b].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/e5278675-b531-4d53-ac36-f7d23ce0704b?project=981930454113].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"e5278675-b531-4d53-ac36-f7d23ce0704b\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://benazirsproject_cloudbuild/source/1599102700.08-554cb713b31b4a1683a6ad530f16f96a.tgz#1599102700514485\n",
      "Copying gs://benazirsproject_cloudbuild/source/1599102700.08-554cb713b31b4a1683a6ad530f16f96a.tgz#1599102700514485...\n",
      "/ [1 files][  2.0 KiB/  2.0 KiB]                                                \n",
      "Operation completed over 1 objects/2.0 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  9.728kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7c3167c320d: Pulling fs layer\n",
      "131f805ec7fd: Pulling fs layer\n",
      "322ed380e680: Pulling fs layer\n",
      "6ac240b13098: Pulling fs layer\n",
      "9ce3a9266402: Pulling fs layer\n",
      "72c706dfac1d: Pulling fs layer\n",
      "6383427606e5: Pulling fs layer\n",
      "3e8b21666cec: Pulling fs layer\n",
      "358bb5d659ed: Pulling fs layer\n",
      "8ade7556a8f1: Pulling fs layer\n",
      "b2ebb7e1223e: Pulling fs layer\n",
      "8d5d283ad922: Pulling fs layer\n",
      "14c0fd48a5f3: Pulling fs layer\n",
      "ceaad5dc04d2: Pulling fs layer\n",
      "c1074350f761: Pulling fs layer\n",
      "687ad0b9a318: Pulling fs layer\n",
      "d2365d2ee19a: Pulling fs layer\n",
      "5095b04f1d98: Pulling fs layer\n",
      "6ac240b13098: Waiting\n",
      "9ce3a9266402: Waiting\n",
      "72c706dfac1d: Waiting\n",
      "6383427606e5: Waiting\n",
      "3e8b21666cec: Waiting\n",
      "358bb5d659ed: Waiting\n",
      "8ade7556a8f1: Waiting\n",
      "b2ebb7e1223e: Waiting\n",
      "8d5d283ad922: Waiting\n",
      "14c0fd48a5f3: Waiting\n",
      "ceaad5dc04d2: Waiting\n",
      "c1074350f761: Waiting\n",
      "687ad0b9a318: Waiting\n",
      "d2365d2ee19a: Waiting\n",
      "5095b04f1d98: Waiting\n",
      "322ed380e680: Verifying Checksum\n",
      "322ed380e680: Download complete\n",
      "131f805ec7fd: Verifying Checksum\n",
      "131f805ec7fd: Download complete\n",
      "6ac240b13098: Verifying Checksum\n",
      "6ac240b13098: Download complete\n",
      "d7c3167c320d: Verifying Checksum\n",
      "d7c3167c320d: Download complete\n",
      "6383427606e5: Verifying Checksum\n",
      "6383427606e5: Download complete\n",
      "72c706dfac1d: Verifying Checksum\n",
      "72c706dfac1d: Download complete\n",
      "358bb5d659ed: Verifying Checksum\n",
      "358bb5d659ed: Download complete\n",
      "8ade7556a8f1: Verifying Checksum\n",
      "8ade7556a8f1: Download complete\n",
      "b2ebb7e1223e: Verifying Checksum\n",
      "b2ebb7e1223e: Download complete\n",
      "3e8b21666cec: Verifying Checksum\n",
      "3e8b21666cec: Download complete\n",
      "8d5d283ad922: Verifying Checksum\n",
      "8d5d283ad922: Download complete\n",
      "14c0fd48a5f3: Verifying Checksum\n",
      "14c0fd48a5f3: Download complete\n",
      "ceaad5dc04d2: Verifying Checksum\n",
      "ceaad5dc04d2: Download complete\n",
      "c1074350f761: Verifying Checksum\n",
      "c1074350f761: Download complete\n",
      "687ad0b9a318: Verifying Checksum\n",
      "687ad0b9a318: Download complete\n",
      "5095b04f1d98: Verifying Checksum\n",
      "5095b04f1d98: Download complete\n",
      "9ce3a9266402: Verifying Checksum\n",
      "9ce3a9266402: Download complete\n",
      "d7c3167c320d: Pull complete\n",
      "131f805ec7fd: Pull complete\n",
      "322ed380e680: Pull complete\n",
      "6ac240b13098: Pull complete\n",
      "d2365d2ee19a: Verifying Checksum\n",
      "d2365d2ee19a: Download complete\n",
      "9ce3a9266402: Pull complete\n",
      "72c706dfac1d: Pull complete\n",
      "6383427606e5: Pull complete\n",
      "3e8b21666cec: Pull complete\n",
      "358bb5d659ed: Pull complete\n",
      "8ade7556a8f1: Pull complete\n",
      "b2ebb7e1223e: Pull complete\n",
      "8d5d283ad922: Pull complete\n",
      "14c0fd48a5f3: Pull complete\n",
      "ceaad5dc04d2: Pull complete\n",
      "c1074350f761: Pull complete\n",
      "687ad0b9a318: Pull complete\n",
      "d2365d2ee19a: Pull complete\n",
      "5095b04f1d98: Pull complete\n",
      "Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d8706668f140\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in dda880936b8b\n",
      "Collecting fire\n",
      "  Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=97bfe6dfd5e767173db90aeec7e8590db975a867a93c6220da22f49aa8a1fafd\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3986 sha256=d2a6b4924e1c3439a44cc1707677041550939e626dde2087e89e95b2a72a1c63\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=55a3487f980d8796b8f39eef00b028b58abae1ba3df7a7a1a8c7ea961c09a1a8\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "\u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: termcolor, fire, cloudml-hypertune, scikit-learn, pandas\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.1\n",
      "    Uninstalling scikit-learn-0.23.1:\n",
      "      Successfully uninstalled scikit-learn-0.23.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.3.1 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "Removing intermediate container dda880936b8b\n",
      " ---> 672e0ee6f821\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 54f30bb68d39\n",
      "Removing intermediate container 54f30bb68d39\n",
      " ---> 0e960a4dd875\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> b2b440915f59\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 275a2c8eded5\n",
      "Removing intermediate container 275a2c8eded5\n",
      " ---> e86442af91c1\n",
      "Successfully built e86442af91c1\n",
      "Successfully tagged gcr.io/benazirsproject/amyris_trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/benazirsproject/amyris_trainer_image:latest\n",
      "The push refers to repository [gcr.io/benazirsproject/amyris_trainer_image]\n",
      "5c64b39dd315: Preparing\n",
      "50f06e117ea5: Preparing\n",
      "29c472153658: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "222959643149: Waiting\n",
      "badaf1bc8335: Waiting\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "c51fe61c6231: Layer already exists\n",
      "89212ed9ad75: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "5c64b39dd315: Pushed\n",
      "50f06e117ea5: Pushed\n",
      "4c4a5579b7a8: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "29c472153658: Pushed\n",
      "latest: digest: sha256:2cadb99341546a56d5d9363c58921947069c3c8d17763771398b47802a2b4016 size: 4708\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                     IMAGES                                                 STATUS\n",
      "e5278675-b531-4d53-ac36-f7d23ce0704b  2020-09-03T03:11:40+00:00  3M30S     gs://benazirsproject_cloudbuild/source/1599102700.08-554cb713b31b4a1683a6ad530f16f96a.tgz  gcr.io/benazirsproject/amyris_trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $TRAINER_IMAGE $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create hyperparameter file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/hptuning_config.yaml\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 4\n",
    "    maxParallelTrials: 4\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    enableTrialEarlyStopping: TRUE \n",
    "    params:\n",
    "    - parameterName: max_iter\n",
    "      type: DISCRETE\n",
    "      discreteValues: [\n",
    "          200,\n",
    "          500\n",
    "          ]\n",
    "    - parameterName: alpha\n",
    "      type: DOUBLE\n",
    "      minValue:  0.00001\n",
    "      maxValue:  0.001\n",
    "      scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start hyper parameter tuning job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20200902_203548] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20200902_203548\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20200902_203548\n",
      "jobId: JOB_20200902_203548\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$TRAINER_IMAGE \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "--config $TRAINING_APP_FOLDER/hptuning_config.yaml \\\n",
    "-- \\\n",
    "--training_dataset_path=$TRAINING_FILE_PATH \\\n",
    "--hptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobId': 'JOB_20200902_203548',\n",
       " 'trainingInput': {'args': ['--training_dataset_path=gs://benazirsproject-demo/data/training/Anonymized_Fermentation_Data_final.xlsx',\n",
       "   '--hptune'],\n",
       "  'hyperparameters': {'goal': 'MAXIMIZE',\n",
       "   'params': [{'parameterName': 'max_iter',\n",
       "     'type': 'DISCRETE',\n",
       "     'discreteValues': [200, 500]},\n",
       "    {'parameterName': 'alpha',\n",
       "     'minValue': 1e-05,\n",
       "     'maxValue': 0.001,\n",
       "     'type': 'DOUBLE',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'}],\n",
       "   'maxTrials': 4,\n",
       "   'maxParallelTrials': 4,\n",
       "   'hyperparameterMetricTag': 'accuracy',\n",
       "   'enableTrialEarlyStopping': True},\n",
       "  'region': 'us-central1',\n",
       "  'jobDir': 'gs://benazirsproject-demo/jobs/JOB_20200902_203548',\n",
       "  'masterConfig': {'imageUri': 'gcr.io/benazirsproject/trainer_image:latest'}},\n",
       " 'createTime': '2020-09-02T20:35:50Z',\n",
       " 'startTime': '2020-09-02T20:35:51Z',\n",
       " 'endTime': '2020-09-02T20:46:04Z',\n",
       " 'state': 'SUCCEEDED',\n",
       " 'trainingOutput': {'completedTrialCount': '4',\n",
       "  'trials': [{'trialId': '2',\n",
       "    'hyperparameters': {'max_iter': '200', 'alpha': '0.00028790154168083194'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.9523809523809523},\n",
       "    'startTime': '2020-09-02T20:36:28.436283106Z',\n",
       "    'endTime': '2020-09-02T20:45:00Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '4',\n",
       "    'hyperparameters': {'max_iter': '200', 'alpha': '0.00077353862604310716'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.8988095238095238},\n",
       "    'startTime': '2020-09-02T20:36:28.436531178Z',\n",
       "    'endTime': '2020-09-02T20:44:51Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '3',\n",
       "    'hyperparameters': {'max_iter': '500', 'alpha': '0.00052283590345628139'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.8273809523809523},\n",
       "    'startTime': '2020-09-02T20:36:28.436381075Z',\n",
       "    'endTime': '2020-09-02T20:44:59Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '1',\n",
       "    'hyperparameters': {'alpha': '0.000505', 'max_iter': '500'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.7976190476190477},\n",
       "    'startTime': '2020-09-02T20:36:28.436067211Z',\n",
       "    'endTime': '2020-09-02T20:45:20Z',\n",
       "    'state': 'SUCCEEDED'}],\n",
       "  'consumedMLUnits': 0.26,\n",
       "  'isHyperparameterTuningJob': True,\n",
       "  'hyperparameterMetricTag': 'accuracy'},\n",
       " 'etag': 'QL0q1KegXbI='}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = discovery.build('ml', 'v1')\n",
    "\n",
    "job_id = 'projects/{}/jobs/{}'.format(PROJECT_ID, JOB_NAME)\n",
    "request = ml.projects().jobs().get(name=job_id)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "except errors.HttpError as err:\n",
    "    print(err)\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "    \n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trialId': '2',\n",
       " 'hyperparameters': {'max_iter': '200', 'alpha': '0.00028790154168083194'},\n",
       " 'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.9523809523809523},\n",
       " 'startTime': '2020-09-02T20:36:28.436283106Z',\n",
       " 'endTime': '2020-09-02T20:45:00Z',\n",
       " 'state': 'SUCCEEDED'}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['trainingOutput']['trials'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = response['trainingOutput']['trials'][0]['hyperparameters']['alpha']\n",
    "max_iter = response['trainingOutput']['trials'][0]['hyperparameters']['max_iter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20200903_031958] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20200903_031958\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20200903_031958\n",
      "jobId: JOB_20200903_031958\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$TRAINER_IMAGE \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "-- \\\n",
    "--training_dataset_path=$TRAINING_FILE_PATH \\\n",
    "--alpha=$alpha \\\n",
    "--max_iter=$max_iter \\\n",
    "--nohptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Created ml engine model [projects/benazirsproject/models/amyris_endtoend2].\n"
     ]
    }
   ],
   "source": [
    "model_name = 'amyris_endtoend2'\n",
    "labels = \"task=classifier,domain=healthcare\"\n",
    "filter = 'name:{}'.format(model_name)\n",
    "# models = !(gcloud ai-platform models list --filter={filter} --format='value(name)')\n",
    "\n",
    "# if not models:\n",
    "!gcloud ai-platform models create  $model_name \\\n",
    "    --regions=$REGION \\\n",
    "    --labels=$labels\n",
    "# else:\n",
    "#     print(\"Model: {} already exists.\".format(models[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Creating version (this might take a few minutes)......done.                    \n"
     ]
    }
   ],
   "source": [
    "model_version = 'v01'\n",
    "filter = 'name:{}'.format(model_version)\n",
    "# versions = !(gcloud ai-platform versions list --model={model_name} --format='value(name)' --filter={filter})\n",
    "\n",
    "# if not versions:\n",
    "!gcloud ai-platform versions create {model_version} \\\n",
    "    --model={model_name} \\\n",
    "    --origin=$JOB_DIR \\\n",
    "    --runtime-version=1.15 \\\n",
    "    --framework=scikit-learn \\\n",
    "    --python-version=3.7\n",
    "# else:\n",
    "#     print(\"Model version: {} already exists.\".format(versions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'serving_instances.json'\n",
    "\n",
    "with open(input_file, 'w') as f:\n",
    "    for index, row in X_validation.head().iterrows():\n",
    "        f.write(json.dumps(list(row.values)))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.5899229207696, 0.0652723072295666, 0.0264099131499111, 0.004874694806509, 0.00644991314991109, 2.22779261068584, 1.683714, 1.56741742827044, 5.7118868998969, 1.18461774905092, NaN, NaN, NaN, 4.0, 4.0, 0.0, 0.00297938144329896, 10.42988, 9.67653, NaN, 50.63335, 70.73976, 4.6589, 70.73976, 50.63335, 0.01996, 0.01156, 0.0493312963547756, 140.0, 174.72618, 4.29832181463847, NaN, NaN, NaN, NaN, NaN, NaN, 100000000.0, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, 0.57698, NaN, 0.68327, 0.0, 27.39144, 17.67385, NaN, 10.127133, 948.005266, 14.459386]\n",
      "[30.5899229207696, 0.0652723072295666, 0.0264099131499111, 0.004874694806509, 0.00644991314991109, 2.22779261068584, 1.683714, 1.56741742827044, 5.7118868998969, 1.18461774905092, NaN, NaN, NaN, 4.0, 4.0, 0.0, 0.00297938144329896, 10.42988, 9.67653, NaN, 50.63335, 70.73976, 4.6589, 70.73976, 50.63335, 0.01996, 0.01156, 0.0493312963547756, 140.0, 174.72618, 4.29832181463847, NaN, NaN, NaN, NaN, NaN, NaN, 100000000.0, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, 0.57698, NaN, 0.68327, 0.0, 27.39144, 17.67385, NaN, 10.127133, 948.005266, 14.459386]\n",
      "[24.3418055857007, 0.0862510709820333, 0.0335858735590368, 0.00806291052704028, 0.0134458735590368, 2.79523185413041, 1.67618, 1.93924912202356, 7.01962430019471, 1.16288406936629, NaN, NaN, NaN, 5.0, 5.0, 0.0, 0.00296477495107632, 22.4006, 19.99258, NaN, 101.8071, 144.20028, 9.6257, 144.20028, 101.8071, 0.02014, 0.01212, 0.0517210477352838, 140.0, 170.87033, 4.18501851010493, NaN, NaN, NaN, NaN, NaN, NaN, 100000000.0, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, 1.20219, NaN, 0.82748, 1.12412, 26.96331, 17.75176, NaN, 16.481031, 1480.97433, 18.87715]\n",
      "[23.5735311933541, 0.0836891579825669, 0.0290332700428417, 0.00696997379720811, 0.0116232700428417, 2.70952450698211, 1.624785, 1.96223498890532, 6.93889679311666, 1.17666770248171, NaN, NaN, NaN, 6.0, 6.0, 0.0, 0.00292595541401273, 31.7194, 25.67859, NaN, 150.28669, 207.68468, 12.36331, 207.68468, 150.28669, 0.01741, 0.01176, 0.0501847789906714, 140.0, 163.5743, 4.61186574768228, NaN, NaN, NaN, NaN, NaN, NaN, 100000000.0, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, 2.14005, NaN, 0.83399, 4.64201, 24.65775, 15.42093, NaN, 20.911765, 1538.144191, 20.292776]\n",
      "[21.0394757922394, 0.089795307468246, 0.0293496214353293, 0.00726722495529052, 0.0132396214353293, 2.83176479367543, 1.554355, 2.00450511837497, 7.31412472057703, 1.10027236733448, NaN, NaN, NaN, 7.0, 7.0, 0.0, 0.00282106394411606, 39.86066, 30.64055, 5.82192, 179.88177, 256.2049, 14.75231, 256.2049, 179.88177, 0.01611, 0.01155, 0.0492886222229808, 140.0, 153.88535, 4.47282232892005, 1.67172, 1.00208, 16.16495, 0.37138, 3.78423, 0.0, 100000000.0, 20.3054359406126, 7.8690911, 6.548816031, 54.392174823106, 1.392244764655, 0.0, 0.515355047012, 0.0, 6.170821751753, 0.0, 0.398092906818, 0.176257908638, 0.851813746988, 1.93167, NaN, 0.19157, 28.9272, 0.76628, 14.95849, NaN, 24.388616, 1549.57557, 22.509606]\n"
     ]
    }
   ],
   "source": [
    "!cat $input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "[1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform predict \\\n",
    "--model $model_name \\\n",
    "--version $model_version \\\n",
    "--json-instances $input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/benazirsproject/trainer_image:latest'"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINER_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline/amyris_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline/amyris_pipeline.py\n",
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"KFP pipeline orchestrating BigQuery and Cloud AI Platform services.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from helper_components import evaluate_model\n",
    "from helper_components import retrieve_best_run\n",
    "from jinja2 import Template\n",
    "import kfp\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp.dsl.types import Dict\n",
    "from kfp.dsl.types import GCPProjectID\n",
    "from kfp.dsl.types import GCPRegion\n",
    "from kfp.dsl.types import GCSPath\n",
    "from kfp.dsl.types import String\n",
    "from kfp.gcp import use_gcp_secret\n",
    "\n",
    "# Defaults and environment settings\n",
    "BASE_IMAGE = os.getenv('BASE_IMAGE')\n",
    "TRAINER_IMAGE = os.getenv('TRAINER_IMAGE')\n",
    "RUNTIME_VERSION = os.getenv('RUNTIME_VERSION')\n",
    "PYTHON_VERSION = os.getenv('PYTHON_VERSION')\n",
    "COMPONENT_URL_SEARCH_PREFIX = os.getenv('COMPONENT_URL_SEARCH_PREFIX')\n",
    "USE_KFP_SA = os.getenv('USE_KFP_SA')\n",
    "TRAINING_FILE_PATH = 'gs://benazirsproject-demo/data/training/Anonymized_Fermentation_Data_final.xlsx'\n",
    "# VALIDATION_FILE_PATH = 'datasets/validation/data.csv'\n",
    "# TESTING_FILE_PATH = 'datasets/testing/data.csv'\n",
    "\n",
    "# Parameter defaults\n",
    "# SPLITS_DATASET_ID = 'splits'\n",
    "HYPERTUNE_SETTINGS = \"\"\"\n",
    "{\n",
    "    \"hyperparameters\":  {\n",
    "        \"goal\": \"MAXIMIZE\",\n",
    "        \"maxTrials\": 3,\n",
    "        \"maxParallelTrials\": 3,\n",
    "        \"hyperparameterMetricTag\": \"accuracy\",\n",
    "        \"enableTrialEarlyStopping\": True,\n",
    "        \"params\": [\n",
    "            {\n",
    "                \"parameterName\": \"max_iter\",\n",
    "                \"type\": \"DISCRETE\",\n",
    "                \"discreteValues\": [500, 1000]\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"alpha\",\n",
    "                \"type\": \"DOUBLE\",\n",
    "                \"minValue\": 0.0001,\n",
    "                \"maxValue\": 0.001,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# # Helper functions\n",
    "# def generate_sampling_query(source_table_name, num_lots, lots):\n",
    "#     \"\"\"Prepares the data sampling query.\"\"\"\n",
    "\n",
    "#     sampling_query_template = \"\"\"\n",
    "#          SELECT *\n",
    "#          FROM \n",
    "#              `{{ source_table }}` AS cover\n",
    "#          WHERE \n",
    "#          MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) IN ({{ lots }})\n",
    "#          \"\"\"\n",
    "#     query = Template(sampling_query_template).render(\n",
    "#         source_table=source_table_name, num_lots=num_lots, lots=str(lots)[1:-1])\n",
    "\n",
    "#     return query\n",
    "\n",
    "\n",
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "# bigquery_query_op = component_store.load_component('bigquery/query')\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')\n",
    "retrieve_best_run_op = func_to_container_op(\n",
    "    retrieve_best_run, base_image=BASE_IMAGE)\n",
    "evaluate_model_op = func_to_container_op(evaluate_model, base_image=BASE_IMAGE)\n",
    "\n",
    "\n",
    "@kfp.dsl.pipeline(\n",
    "    name='Amyris Classifier Training',\n",
    "    description='The pipeline training and deploying the Amyris classifierpipeline_yaml'\n",
    ")\n",
    "def amyris_train(project_id,\n",
    "                    region,\n",
    "                    gcs_root,\n",
    "                    evaluation_metric_name,\n",
    "                    evaluation_metric_threshold,\n",
    "                    model_id,\n",
    "                    version_id,\n",
    "                    replace_existing_version,\n",
    "                    hypertune_settings=HYPERTUNE_SETTINGS,\n",
    "                    dataset_location='US'):\n",
    "    \"\"\"Orchestrates training and deployment of an sklearn model.\"\"\"\n",
    "\n",
    "    # Create the training split\n",
    "#     query = generate_sampling_query(\n",
    "#         source_table_name=source_table_name, num_lots=10, lots=[1, 2, 3, 4])\n",
    "\n",
    "#     training_file_path = '{}/{}'.format(gcs_root, TRAINING_FILE_PATH)\n",
    "\n",
    "#     create_training_split = bigquery_query_op(\n",
    "#         query=query,\n",
    "#         project_id=project_id,\n",
    "#         dataset_id=dataset_id,\n",
    "#         table_id='',\n",
    "#         output_gcs_path=training_file_path,\n",
    "#         dataset_location=dataset_location)\n",
    "\n",
    "#     # Create the validation split\n",
    "#     query = generate_sampling_query(\n",
    "#         source_table_name=source_table_name, num_lots=10, lots=[8])\n",
    "\n",
    "#     validation_file_path = '{}/{}'.format(gcs_root, VALIDATION_FILE_PATH)\n",
    "\n",
    "#     create_validation_split = bigquery_query_op(\n",
    "#         query=query,\n",
    "#         project_id=project_id,\n",
    "#         dataset_id=dataset_id,\n",
    "#         table_id='',\n",
    "#         output_gcs_path=validation_file_path,\n",
    "#         dataset_location=dataset_location)\n",
    "\n",
    "    # Create the testing split\n",
    "#     query = generate_sampling_query(\n",
    "#         source_table_name=source_table_name, num_lots=10, lots=[9])\n",
    "\n",
    "#     testing_file_path = '{}/{}'.format(gcs_root, TESTING_FILE_PATH)\n",
    "\n",
    "#     create_testing_split = bigquery_query_op(\n",
    "#         query=query,\n",
    "#         project_id=project_id,\n",
    "#         dataset_id=dataset_id,\n",
    "#         table_id='',\n",
    "#         output_gcs_path=testing_file_path,\n",
    "#         dataset_location=dataset_location)\n",
    "\n",
    "    # Tune hyperparameters\n",
    "    tune_args = [\n",
    "        '--training_dataset_path',\n",
    "        TRAINING_FILE_PATH,\n",
    "         '--hptune', 'True'\n",
    "    ]\n",
    "\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir/hypertune',\n",
    "                                kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    hypertune = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri='gcr.io/benazirsproject/amyris_trainer_image:latest',\n",
    "        job_dir=job_dir,\n",
    "        args=tune_args,\n",
    "        training_input=hypertune_settings)\n",
    "\n",
    "    # Retrieve the best trial\n",
    "    get_best_trial = retrieve_best_run_op(\n",
    "            project_id, hypertune.outputs['job_id'])\n",
    "\n",
    "    # Train the model on a combined training and validation datasets\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir', kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    train_args = [\n",
    "        '--training_dataset_path',\n",
    "       TRAINING_FILE_PATH,\n",
    "         '--alpha',\n",
    "        get_best_trial.outputs['alpha'], '--max_iter',\n",
    "        get_best_trial.outputs['max_iter'], '--hptune', 'False'\n",
    "    ]\n",
    "\n",
    "    train_model = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri='gcr.io/benazirsproject/amyris_trainer_image:latest',\n",
    "        job_dir=job_dir,\n",
    "        args=train_args)\n",
    "\n",
    "    # Evaluate the model on the testing split\n",
    "    eval_model = evaluate_model_op(\n",
    "        dataset_path=TRAINING_FILE_PATH,\n",
    "        model_path=str(train_model.outputs['job_dir']),\n",
    "        metric_name=evaluation_metric_name)\n",
    "\n",
    "    # Deploy the model if the primary metric is better than threshold\n",
    "    with kfp.dsl.Condition(eval_model.outputs['metric_value'] > evaluation_metric_threshold):\n",
    "        deploy_model = mlengine_deploy_op(\n",
    "        model_uri=train_model.outputs['job_dir'],\n",
    "        project_id=project_id,\n",
    "        model_id=model_id,\n",
    "        version_id=version_id,\n",
    "        runtime_version=RUNTIME_VERSION,\n",
    "        python_version=PYTHON_VERSION,\n",
    "        replace_existing_version=replace_existing_version)\n",
    "\n",
    "    # Configure the pipeline to run using the service account defined\n",
    "      # in the user-gcp-sa k8s secret\n",
    "    if USE_KFP_SA == 'True':\n",
    "        kfp.dsl.get_pipeline_conf().add_op_transformer(\n",
    "              use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/benazirsproject/amyris_trainer_image:latest'"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINER_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline/helper_components.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline/helper_components.py\n",
    "\n",
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\"\"\"Helper components.\"\"\"\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "def retrieve_best_run(\n",
    "    project_id: str, job_id: str\n",
    ") -> NamedTuple('Outputs', [('metric_value', float), ('alpha', float),\n",
    "                            ('max_iter', int)]):\n",
    "  \"\"\"Retrieves the parameters of the best Hypertune run.\"\"\"\n",
    "\n",
    "  from googleapiclient import discovery\n",
    "  from googleapiclient import errors\n",
    "\n",
    "  ml = discovery.build('ml', 'v1')\n",
    "\n",
    "  job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n",
    "  request = ml.projects().jobs().get(name=job_name)\n",
    "\n",
    "  try:\n",
    "    response = request.execute()\n",
    "  except errors.HttpError as err:\n",
    "    print(err)\n",
    "  except:\n",
    "    print('Unexpected error')\n",
    "\n",
    "  print(response)\n",
    "\n",
    "  best_trial = response['trainingOutput']['trials'][0]\n",
    "\n",
    "  metric_value = best_trial['finalMetric']['objectiveValue']\n",
    "  alpha = float(best_trial['hyperparameters']['alpha'])\n",
    "  max_iter = int(best_trial['hyperparameters']['max_iter'])\n",
    "\n",
    "  return (metric_value, alpha, max_iter)\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    dataset_path: str, model_path: str, metric_name: str\n",
    ") -> NamedTuple('Outputs', [('metric_name', str), ('metric_value', float),\n",
    "                            ('mlpipeline_metrics', 'Metrics')]):\n",
    "  \"\"\"Evaluates a trained sklearn model.\"\"\"\n",
    "  #import joblib\n",
    "  import pickle\n",
    "  import json\n",
    "  import pandas as pd\n",
    "  import subprocess\n",
    "  import sys\n",
    "\n",
    "  from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "  df_test = pd.read_csv(dataset_path)\n",
    "\n",
    "  X_test = df_test.drop('Cover_Type', axis=1)\n",
    "  y_test = df_test['Cover_Type']\n",
    "\n",
    "  # Copy the model from GCS\n",
    "  model_filename = 'model.pkl'\n",
    "  gcs_model_filepath = '{}/{}'.format(model_path, model_filename)\n",
    "  print(gcs_model_filepath)\n",
    "  subprocess.check_call(['gsutil', 'cp', gcs_model_filepath, model_filename],\n",
    "                        stderr=sys.stdout)\n",
    "\n",
    "  with open(model_filename, 'rb') as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "  y_hat = model.predict(X_test)\n",
    "\n",
    "  if metric_name == 'accuracy':\n",
    "    metric_value = accuracy_score(y_test, y_hat)\n",
    "  elif metric_name == 'recall':\n",
    "    metric_value = recall_score(y_test, y_hat)\n",
    "  else:\n",
    "    metric_name = 'N/A'\n",
    "    metric_value = 0\n",
    "\n",
    "  # Export the metric\n",
    "  metrics = {\n",
    "      'metrics': [{\n",
    "          'name': metric_name,\n",
    "          'numberValue': float(metric_value)\n",
    "      }]\n",
    "  }\n",
    "\n",
    "  return (metric_name, metric_value, json.dumps(metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create an empty folder base_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./base_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./base_image/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='base_image'\n",
    "TAG='latest'\n",
    "BASE_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 1 file(s) totalling 122 bytes before compression.\n",
      "Uploading tarball of [base_image] to [gs://benazirsproject_cloudbuild/source/1599084515.31-1c7fb82a1b4d46b0af6be2e2e95ba3e9.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/benazirsproject/builds/fc39fdfd-4636-4d6e-b387-05acd1e075bb].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/fc39fdfd-4636-4d6e-b387-05acd1e075bb?project=981930454113].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"fc39fdfd-4636-4d6e-b387-05acd1e075bb\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://benazirsproject_cloudbuild/source/1599084515.31-1c7fb82a1b4d46b0af6be2e2e95ba3e9.tgz#1599084515781863\n",
      "Copying gs://benazirsproject_cloudbuild/source/1599084515.31-1c7fb82a1b4d46b0af6be2e2e95ba3e9.tgz#1599084515781863...\n",
      "/ [1 files][  227.0 B/  227.0 B]                                                \n",
      "Operation completed over 1 objects/227.0 B.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  2.048kB\n",
      "Step 1/2 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7c3167c320d: Pulling fs layer\n",
      "131f805ec7fd: Pulling fs layer\n",
      "322ed380e680: Pulling fs layer\n",
      "6ac240b13098: Pulling fs layer\n",
      "9ce3a9266402: Pulling fs layer\n",
      "72c706dfac1d: Pulling fs layer\n",
      "6383427606e5: Pulling fs layer\n",
      "3e8b21666cec: Pulling fs layer\n",
      "358bb5d659ed: Pulling fs layer\n",
      "8ade7556a8f1: Pulling fs layer\n",
      "b2ebb7e1223e: Pulling fs layer\n",
      "8d5d283ad922: Pulling fs layer\n",
      "14c0fd48a5f3: Pulling fs layer\n",
      "ceaad5dc04d2: Pulling fs layer\n",
      "c1074350f761: Pulling fs layer\n",
      "687ad0b9a318: Pulling fs layer\n",
      "d2365d2ee19a: Pulling fs layer\n",
      "5095b04f1d98: Pulling fs layer\n",
      "6ac240b13098: Waiting\n",
      "9ce3a9266402: Waiting\n",
      "72c706dfac1d: Waiting\n",
      "6383427606e5: Waiting\n",
      "3e8b21666cec: Waiting\n",
      "358bb5d659ed: Waiting\n",
      "8ade7556a8f1: Waiting\n",
      "b2ebb7e1223e: Waiting\n",
      "8d5d283ad922: Waiting\n",
      "ceaad5dc04d2: Waiting\n",
      "c1074350f761: Waiting\n",
      "687ad0b9a318: Waiting\n",
      "d2365d2ee19a: Waiting\n",
      "5095b04f1d98: Waiting\n",
      "14c0fd48a5f3: Waiting\n",
      "322ed380e680: Verifying Checksum\n",
      "322ed380e680: Download complete\n",
      "131f805ec7fd: Verifying Checksum\n",
      "131f805ec7fd: Download complete\n",
      "6ac240b13098: Verifying Checksum\n",
      "6ac240b13098: Download complete\n",
      "d7c3167c320d: Verifying Checksum\n",
      "d7c3167c320d: Download complete\n",
      "6383427606e5: Verifying Checksum\n",
      "6383427606e5: Download complete\n",
      "72c706dfac1d: Verifying Checksum\n",
      "72c706dfac1d: Download complete\n",
      "358bb5d659ed: Verifying Checksum\n",
      "358bb5d659ed: Download complete\n",
      "8ade7556a8f1: Verifying Checksum\n",
      "8ade7556a8f1: Download complete\n",
      "b2ebb7e1223e: Verifying Checksum\n",
      "b2ebb7e1223e: Download complete\n",
      "3e8b21666cec: Verifying Checksum\n",
      "3e8b21666cec: Download complete\n",
      "14c0fd48a5f3: Verifying Checksum\n",
      "14c0fd48a5f3: Download complete\n",
      "8d5d283ad922: Verifying Checksum\n",
      "8d5d283ad922: Download complete\n",
      "ceaad5dc04d2: Verifying Checksum\n",
      "ceaad5dc04d2: Download complete\n",
      "c1074350f761: Verifying Checksum\n",
      "c1074350f761: Download complete\n",
      "687ad0b9a318: Verifying Checksum\n",
      "687ad0b9a318: Download complete\n",
      "5095b04f1d98: Verifying Checksum\n",
      "5095b04f1d98: Download complete\n",
      "9ce3a9266402: Verifying Checksum\n",
      "9ce3a9266402: Download complete\n",
      "d7c3167c320d: Pull complete\n",
      "131f805ec7fd: Pull complete\n",
      "322ed380e680: Pull complete\n",
      "6ac240b13098: Pull complete\n",
      "d2365d2ee19a: Verifying Checksum\n",
      "d2365d2ee19a: Download complete\n",
      "9ce3a9266402: Pull complete\n",
      "72c706dfac1d: Pull complete\n",
      "6383427606e5: Pull complete\n",
      "3e8b21666cec: Pull complete\n",
      "358bb5d659ed: Pull complete\n",
      "8ade7556a8f1: Pull complete\n",
      "b2ebb7e1223e: Pull complete\n",
      "8d5d283ad922: Pull complete\n",
      "14c0fd48a5f3: Pull complete\n",
      "ceaad5dc04d2: Pull complete\n",
      "c1074350f761: Pull complete\n",
      "687ad0b9a318: Pull complete\n",
      "d2365d2ee19a: Pull complete\n",
      "5095b04f1d98: Pull complete\n",
      "Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d8706668f140\n",
      "Step 2/2 : RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5\n",
      " ---> Running in 1888834701ae\n",
      "Collecting fire\n",
      "  Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Collecting kfp==0.2.5\n",
      "  Downloading kfp-0.2.5.tar.gz (116 kB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Collecting urllib3<1.25,>=1.15\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.29.0)\n",
      "Collecting kubernetes<=10.0.0,>=8.0.0\n",
      "  Downloading kubernetes-10.0.0-py2.py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied, skipping upgrade: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.9.2)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.17.2)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting cloudpickle==1.1.1\n",
      "  Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting kfp-server-api<=0.1.40,>=0.1.18\n",
      "  Downloading kfp-server-api-0.1.40.tar.gz (38 kB)\n",
      "Collecting argo-models==2.2.1a\n",
      "  Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (3.2.0)\n",
      "Collecting tabulate==0.8.3\n",
      "  Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "Collecting click==7.0\n",
      "  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.10-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.9.tar.gz (30 kB)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (0.5.1)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (0.57.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (47.3.1.post20200616)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.2.5) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (0.2.7)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==0.2.5) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp==0.2.5) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.4.2->kfp==0.2.5) (2.20)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==0.2.5) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp==0.2.5) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (3.12.3)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.51.0)\n",
      "Building wheels for collected packages: fire, kfp, termcolor, kfp-server-api, argo-models, tabulate, strip-hints\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=a378bbe5ec17d96187dae603d7ca8536e40802bac4f9bd03bcc2cd2832b3c8c1\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-0.2.5-py3-none-any.whl size=159978 sha256=daaca2ea45677620e8e71f2989097e181ca26dffc9b4241327bf62b512cd57b0\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/74/7e/0a882d654bdf82d039460ab5c6adf8724ae56e277de7c0eaea\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=ab85d66a058caae6c1cc835bf2a7d404f9980a46568d67148c728542792ec305\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-0.1.40-py3-none-any.whl size=102468 sha256=406d41271f4de7493f4ec449f4d8b7bf4843e6e0eec7658c73d6e510cc1e7f82\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/e3/43/3972dea76ee89e35f090b313817089043f2609236cf560069d\n",
      "  Building wheel for argo-models (setup.py): started\n",
      "  Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "  Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57307 sha256=1e23f9a6ef90a397e138dd159068bf3c6e65c3a5c57ad8381f74e1505ec9d964\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/4b/fd/cdd013bd2ad1a7162ecfaf954e9f1bb605174a20e3c02016b7\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23378 sha256=22f69656a9628b4bc7551d4a07f3a72f2eff0a76f32dc54048a3b5550bc7d3a2\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.9-py2.py3-none-any.whl size=20993 sha256=8004654a9af00fcdd3e7b4e3fe7bb0a1e477c0aa5c5220f21756557dbbd69b53\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/b8/4e/a3ec111d2db63cec88121bd7c0ab1a123bce3b55dd19dda5c1\n",
      "Successfully built fire kfp termcolor kfp-server-api argo-models tabulate strip-hints\n",
      "\u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: jupyterlab-git 0.10.1 has requirement nbdime<2.0.0,>=1.1.0, but you'll have nbdime 2.0.0 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: distributed 2.19.0 has requirement cloudpickle>=1.3.0, but you'll have cloudpickle 1.1.1 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: termcolor, fire, scikit-learn, pandas, urllib3, kubernetes, requests-toolbelt, cloudpickle, kfp-server-api, argo-models, tabulate, click, Deprecated, strip-hints, kfp\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.1\n",
      "    Uninstalling scikit-learn-0.23.1:\n",
      "      Successfully uninstalled scikit-learn-0.23.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.9\n",
      "    Uninstalling urllib3-1.25.9:\n",
      "      Successfully uninstalled urllib3-1.25.9\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 11.0.0\n",
      "    Uninstalling kubernetes-11.0.0:\n",
      "      Successfully uninstalled kubernetes-11.0.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.4.1\n",
      "    Uninstalling cloudpickle-1.4.1:\n",
      "      Successfully uninstalled cloudpickle-1.4.1\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 7.1.2\n",
      "    Uninstalling click-7.1.2:\n",
      "      Successfully uninstalled click-7.1.2\n",
      "Successfully installed Deprecated-1.2.10 argo-models-2.2.1a0 click-7.0 cloudpickle-1.1.1 fire-0.3.1 kfp-0.2.5 kfp-server-api-0.1.40 kubernetes-10.0.0 pandas-0.24.2 requests-toolbelt-0.9.1 scikit-learn-0.20.4 strip-hints-0.1.9 tabulate-0.8.3 termcolor-1.1.0 urllib3-1.24.3\n",
      "Removing intermediate container 1888834701ae\n",
      " ---> 777712c2a8c5\n",
      "Successfully built 777712c2a8c5\n",
      "Successfully tagged gcr.io/benazirsproject/base_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/benazirsproject/base_image:latest\n",
      "The push refers to repository [gcr.io/benazirsproject/base_image]\n",
      "75a9b551ed0e: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "222959643149: Layer already exists\n",
      "c51fe61c6231: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "89212ed9ad75: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "4c4a5579b7a8: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "75a9b551ed0e: Pushed\n",
      "latest: digest: sha256:4ddda171af629937af25de7ce6282ddbeccde067ad3a14a28d4976f13ff39e69 size: 4293\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                     IMAGES                                       STATUS\n",
      "fc39fdfd-4636-4d6e-b387-05acd1e075bb  2020-09-02T22:08:35+00:00  3M46S     gs://benazirsproject_cloudbuild/source/1599084515.31-1c7fb82a1b4d46b0af6be2e2e95ba3e9.tgz  gcr.io/benazirsproject/base_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $BASE_IMAGE base_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ENDPOINT = '19a5aed0f754a516-dot-us-central2.pipelines.googleusercontent.com'\n",
    "ARTIFACT_STORE_URI = 'gs://benazirsproject-demo'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compile pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: USE_KFP_SA=False\n",
      "env: BASE_IMAGE=gcr.io/benazirsproject/base_image:latest\n",
      "env: TRAINER_IMAGE=gcr.io/benazirsproject/amyris_trainer_image:latest\n",
      "env: COMPONENT_URL_SEARCH_PREFIX=https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/\n",
      "env: RUNTIME_VERSION=1.15\n",
      "env: PYTHON_VERSION=3.7\n"
     ]
    }
   ],
   "source": [
    "USE_KFP_SA = False\n",
    "\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/'\n",
    "RUNTIME_VERSION = '1.15'\n",
    "PYTHON_VERSION = '3.7'\n",
    "\n",
    "%env USE_KFP_SA={USE_KFP_SA}\n",
    "%env BASE_IMAGE={BASE_IMAGE}\n",
    "%env TRAINER_IMAGE={TRAINER_IMAGE}\n",
    "%env COMPONENT_URL_SEARCH_PREFIX={COMPONENT_URL_SEARCH_PREFIX}\n",
    "%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "%env PYTHON_VERSION={PYTHON_VERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy helper components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dsl-compile --py pipeline/amyris_pipeline.py --output amyris_pipeline.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: argoproj.io/v1alpha1\n",
      "kind: Workflow\n",
      "metadata:\n",
      "  generateName: amyris-classifier-training-\n",
      "  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.0.0, pipelines.kubeflow.org/pipeline_compilation_time: '2020-09-03T03:27:50.570494',\n",
      "    pipelines.kubeflow.org/pipeline_spec: '{\"description\": \"The pipeline training\n",
      "      and deploying the Amyris classifierpipeline_yaml\", \"inputs\": [{\"name\": \"project_id\"},\n",
      "      {\"name\": \"region\"}, {\"name\": \"gcs_root\"}, {\"name\": \"evaluation_metric_name\"},\n",
      "      {\"name\": \"evaluation_metric_threshold\"}, {\"name\": \"model_id\"}, {\"name\": \"version_id\"},\n",
      "      {\"name\": \"replace_existing_version\"}, {\"default\": \"\\n{\\n    \\\"hyperparameters\\\":  {\\n        \\\"goal\\\":\n"
     ]
    }
   ],
   "source": [
    "!head amyris_pipeline.yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deploy pipeline package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline c67cf5a4-db2b-4aa3-bb04-c947aba35230 has been submitted\n",
      "\n",
      "Pipeline Details\n",
      "------------------\n",
      "ID           c67cf5a4-db2b-4aa3-bb04-c947aba35230\n",
      "Name         amyris_pipeline_september\n",
      "Description\n",
      "Uploaded at  2020-09-03T03:28:08+00:00\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| Parameter Name              | Default Value                                    |\n",
      "+=============================+==================================================+\n",
      "| project_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| region                      |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| gcs_root                    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| evaluation_metric_name      |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| evaluation_metric_threshold |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| model_id                    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| version_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| replace_existing_version    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| hypertune_settings          | {                                                |\n",
      "|                             |     \"hyperparameters\":  {                        |\n",
      "|                             |         \"goal\": \"MAXIMIZE\",                      |\n",
      "|                             |         \"maxTrials\": 6,                          |\n",
      "|                             |         \"maxParallelTrials\": 3,                  |\n",
      "|                             |         \"hyperparameterMetricTag\": \"accuracy\",   |\n",
      "|                             |         \"enableTrialEarlyStopping\": True,        |\n",
      "|                             |         \"params\": [                              |\n",
      "|                             |             {                                    |\n",
      "|                             |                 \"parameterName\": \"max_iter\",     |\n",
      "|                             |                 \"type\": \"DISCRETE\",              |\n",
      "|                             |                 \"discreteValues\": [500, 1000]    |\n",
      "|                             |             },                                   |\n",
      "|                             |             {                                    |\n",
      "|                             |                 \"parameterName\": \"alpha\",        |\n",
      "|                             |                 \"type\": \"DOUBLE\",                |\n",
      "|                             |                 \"minValue\": 0.0001,              |\n",
      "|                             |                 \"maxValue\": 0.001,               |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\" |\n",
      "|                             |             }                                    |\n",
      "|                             |         ]                                        |\n",
      "|                             |     }                                            |\n",
      "|                             | }                                                |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| dataset_location            | US                                               |\n",
      "+-----------------------------+--------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME='amyris_pipeline_september'\n",
    "\n",
    "!kfp --endpoint $ENDPOINT pipeline upload \\\n",
    "-p $PIPELINE_NAME \\\n",
    "amyris_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| Pipeline ID                          | Name                                            | Uploaded at               |\n",
      "+======================================+=================================================+===========================+\n",
      "| c67cf5a4-db2b-4aa3-bb04-c947aba35230 | amyris_pipeline_september                       | 2020-09-03T03:28:08+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| c2171aa5-2680-4a96-b5de-516679f24c07 | amyris_pipeline_sept                            | 2020-09-02T22:26:04+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 3367f9e4-6e1c-4ece-b517-a347c8d42690 | covertype_continuous_training_test              | 2020-08-27T16:36:33+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| b6b2b054-e524-4acf-b3fc-0e7e3ba271bd | amyris_pipeline                                 | 2020-08-26T23:27:54+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 4ffefa9f-c1aa-4b11-9b4b-ad70c223226e | covertype_continuous_training_self              | 2020-08-26T23:25:58+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 1b5be90d-6020-43fc-8126-b6d979da2e39 | [Tutorial] DSL - Control structures             | 2020-08-26T22:17:35+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| b162aff5-6673-41eb-8619-221ccfaaa71c | [Tutorial] Data passing in python components    | 2020-08-26T22:17:34+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 81d20eef-4bb7-4bed-8d87-569c56f824f5 | [Demo] TFX - Iris classification pipeline       | 2020-08-26T22:17:33+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 5707fa44-e6d1-403f-88fb-88ee299d2f33 | [Demo] TFX - Taxi tip prediction model trainer  | 2020-08-26T22:17:32+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 6f868f09-82bc-49b5-b44f-ed260eb653e0 | [Demo] XGBoost - Training with confusion matrix | 2020-08-26T22:17:31+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT pipeline list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID='c67cf5a4-db2b-4aa3-bb04-c947aba35230'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'amyris1'\n",
    "RUN_ID = 'Run_001'\n",
    "SOURCE_TABLE = 'covertype_dataset.covertype'\n",
    "DATASET_ID = 'splits'\n",
    "EVALUATION_METRIC = 'accuracy'\n",
    "EVALUATION_METRIC_THRESHOLD = '0.69'\n",
    "MODEL_ID = 'covertype_classifier'\n",
    "VERSION_ID = 'v01'\n",
    "REPLACE_EXISTING_VERSION = 'True'\n",
    "\n",
    "GCS_STAGING_PATH = '{}/staging'.format(ARTIFACT_STORE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating experiment amyris1.\n",
      "(400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Content-Length': '1451', 'Content-Type': 'text/html; charset=utf-8', 'Date': 'Thu, 03 Sep 2020 03:29:41 GMT', 'X-Content-Type-Options': 'nosniff', 'X-Frame-Options': 'SAMEORIGIN', 'X-Powered-By': 'Express', 'X-Xss-Protection': '0', 'Set-Cookie': 'S=cloud_datalab_tunnel=Ync7COYNtFPrG88i7p-vVy1MbOKKQCWGkU75ZcTGjWo; Path=/; Max-Age=3600'})\n",
      "HTTP response body: \n",
      "<!DOCTYPE html>\n",
      "<html lang=en>\n",
      "  <meta charset=utf-8>\n",
      "  <meta name=viewport content=\"initial-scale=1, minimum-scale=1, width=device-width\">\n",
      "  <title>Error 400 (Bad Request)!!1</title>\n",
      "  <style>\n",
      "    *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/logos/errorpage/error_logo-150x54.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/logos/errorpage/error_logo-150x54-2x.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/logos/errorpage/error_logo-150x54-2x.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/logos/errorpage/error_logo-150x54-2x.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}\n",
      "  </style>\n",
      "  <a href=//www.google.com/><span id=logo aria-label=Google></span></a>\n",
      "  <p><b>400.</b> <ins>That’s an error.</ins>\n",
      "  <p>  <ins>That’s all we know.</ins>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT run submit \\\n",
    "-e $EXPERIMENT_NAME \\\n",
    "-r $RUN_ID \\\n",
    "-p $PIPELINE_ID \\\n",
    "project_id=$PROJECT_ID \\\n",
    "gcs_root=$GCS_STAGING_PATH \\\n",
    "region=$REGION \\\n",
    "source_table_name=$SOURCE_TABLE \\\n",
    "dataset_id=$DATASET_ID \\\n",
    "evaluation_metric_name=$EVALUATION_METRIC \\\n",
    "evaluation_metric_threshold=$EVALUATION_METRIC_THRESHOLD \\\n",
    "model_id=$MODEL_ID \\\n",
    "version_id=$VERSION_ID \\\n",
    "replace_existing_version=$REPLACE_EXISTING_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
